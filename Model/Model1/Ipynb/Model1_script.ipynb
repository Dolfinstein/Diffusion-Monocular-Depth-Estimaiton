{"cells":[{"cell_type":"markdown","source":["# model 1 把所有normalize換成 batch normalize, 所有nonlinear 換成leaky relu, 在fpn最前面加入norm並且在所有相加的部分前加入norm\n"],"metadata":{"id":"O9KtahLh2F-8"}},{"cell_type":"code","source":["import torch\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","import random\n","from google.colab import files\n","import sys\n","import time\n","from torch.utils.data import random_split\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。\n","# from Model.functions.functions import *\n","# from Model.Data_Process.data_processing import *"],"metadata":{"id":"6jvoA5GQbsmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WRkUz1I6-JB"},"outputs":[],"source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_be0bSq7A88"},"outputs":[],"source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILHJ1zbU8LYb"},"outputs":[],"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 =nn.BatchNorm2d(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = nn.BatchNorm2d(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","\n","    def forward(self, x, temb):\n","        h = x\n","        h = self.norm1(h)    # normalize\n","\n","        h = self.Lrelu(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","\n","        h = h + self.temb_proj(self.Lrelu(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        h = self.norm2(h)\n","        h = self.Lrelu(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy0-VDEM8uIl"},"outputs":[],"source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = nn.BatchNorm2d(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qQRtwwWkXTJ"},"outputs":[],"source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpRkaCZE2FM6"},"outputs":[],"source":["class UpsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n","                                        int(out_channels),\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6dIsb3rx6CP"},"outputs":[],"source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        self.bn0 = nn.BatchNorm2d(resolutions[0])\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","        self.norm_seq = nn.ModuleList()\n","        for idx in range((len(self.resolutions) - 2) * 2 + 2):\n","\n","          self.norm_seq.append(nn.BatchNorm2d(self.target_channel))\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = self.bn0(x)\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.pool(self.Lrelu(self.ConvList[idx](h)))\n","          else:\n","            h = self.Lrelu(self.ConvList[idx](temp))\n","\n","          temp = h\n","          h = self.Lrelu(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        count = 0\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","            hold = self.convOut(hold + self.norm_seq[count](FPN_list[idx]))\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","          else:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","\n","            hold = hold + self.norm_seq[count](FPN_list[idx])\n","            count = count + 1\n","            hold = self.Upsampple[idx - 1](hold)\n","\n","        return hold\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esdKjIMr9wcF"},"outputs":[],"source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icPtSBzxAKCO"},"outputs":[],"source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fz0EX77czh3m"},"outputs":[],"source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        for idx in range(phase2):\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","\n","        for idx in range(len(self.phase1_model)):\n","          h = self.phase1_model[idx](h)\n","\n","        for idx in range(len(self.phase2_model)):\n","          h = self.phase2_model[idx](h)\n","\n","        return h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2Vzf5ZRC8wM"},"outputs":[],"source":["class depth_decode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        self.ch = config['model']['ch']\n","        # self.ch = 128\n","        self.resolution = config['data']['image_size']\n","        # self.resolution = 256\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        # self.targeted_size = 64\n","        count = 0\n","        tmp = self.targeted_size\n","        while True:\n","          if tmp == self.resolution:\n","            break\n","          count = count + 1\n","          tmp = tmp * 2\n","\n","        self.decode = nn.ModuleList()\n","        in_cha = self.ch\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        for idx in range(count):\n","          out_cha = in_cha / 4\n","          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n","          in_cha = out_cha\n","        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n","                                        1,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","\n","\n","\n","\n","\n","\n","\n","    def forward(self, pred):\n","        for idx in range(len(self.decode)):\n","          pred = self.decode[idx](pred)\n","        pred = self.final_conv(pred)\n","        pred = pred.squeeze(1)\n","\n","\n","\n","\n","\n","        return pred"]},{"cell_type":"code","source":["from Model.functions.functions import get_beta_schedule, get_index_from_list, get_timestep_embedding\n","class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod =      [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","            alphas_cumprod_prev = [1,   0.9, 0.9 * 0.8, 0.9 * 0.8 * 0.7]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0).to(device)\n","        self.alphas_cumprod_prev = torch.cat(\n","            [torch.ones(1).to(device), self.alphas_cumprod[:-1]], dim=0\n","        )\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n","\n","    def backward(self, model, image, weight_path, skip, eta = 0.1):\n","        checkpoint = torch.load(weight_path, map_location = torch.device(device))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        with torch.inference_mode():\n","\n","\n","            # seq = range(0, timesteps, skip)   # 這是原版\n","            # seq_next = [-1] + list(seq[:-1])\n","\n","            seq = range(1, timesteps, skip)\n","            seq_next = [0] + list(seq[:-1])\n","\n","\n","\n","            # seq =      [1, 2, 3]\n","            # seq_next = [0, 1, 2]\n","            x0_preds = []\n","            depth = torch.randn([image.shape[0], image.shape[-1], image.shape[-1]]).to(torch.float32)\n","            print(depth.dtype)\n","            xs = [depth]\n","            n = image.shape[0]\n","\n","\n","\n","            for i, j in zip(reversed(seq), reversed(seq_next)):\n","                t = (torch.ones(n) * i).to(image.device)\n","                next_t = (torch.ones(n) * j).to(image.device)\n","\n","                at = self.alphas_cumprod.gather(-1, t.to(torch.int64))\n","\n","                at_next = self.alphas_cumprod.gather(-1, next_t.to(torch.int64))\n","\n","                xt = xs[-1].to(device) # x_t\n","                x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\n","\n","                x0_preds.append(x0_t.to(device))\n","                et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\n","                c1 = (\n","                    eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\n","                                                                                                # can make the sampling process of x_{t - 1} become identical\n","                                                                                                # as ddpm\n","                )\n","                c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","                xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\n","                xs.append(xt_next.to(device))\n","            return xs, x0_preds\n","\n"],"metadata":{"id":"sb2qOrGHgPgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW2SNSqY0qys"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","        depth_enc_channels = config['model']['depth_enc_channels']\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","# nonlinear\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = nn.BatchNorm2d(block_in)\n","\n","        self.depth_decode = depth_decode(config)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","    def forward(self, image, depth, t, sampling = False):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = self.Lrelu(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","\n","        img_enc = self.fpn(image)\n","        depth = depth.to(torch.float32)\n","        # if sampling == False:\n","        depth = self.depth_encode(depth)\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","        if sampling == False:\n","            noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","            noisy_map = noisy_map.to(torch.float32)\n","            noise = noise.to(torch.float32)\n","        else:\n","            noisy_map = depth\n","\n","\n","\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","        # return backbone_input\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        # hs = [self.conv_in(image)]\n","\n","        hs = [self.conv_in(backbone_input)]\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = self.Lrelu(h)\n","        h = self.depth_decode(h)\n","        return h\n"]},{"cell_type":"code","source":[],"metadata":{"id":"wW03IDPvb7Xr"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}