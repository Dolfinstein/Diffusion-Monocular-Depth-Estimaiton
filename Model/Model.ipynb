{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL6RdTKY4KNI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import math\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import yaml\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deb(param, str):\n",
        "  print(str + \" = {}\".format(param))"
      ],
      "metadata": {
        "id": "sfQ1zEzuzZL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T3gP7QULTMt"
      },
      "outputs": [],
      "source": [
        "def load_config(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(model):\n",
        "  sum = 0\n",
        "  for param in model.parameters():\n",
        "    sum = sum + param.numel()\n",
        "  return sum"
      ],
      "metadata": {
        "id": "fedtFZjG6Ucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H-DIrmZLVw4"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/config.yml'\n",
        "config = load_config(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9i9pX5s6Gzx"
      },
      "outputs": [],
      "source": [
        "target_size = (config['data']['image_size'], config['data']['image_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvBywcbd6ed_"
      },
      "outputs": [],
      "source": [
        "def image_folder_to_tensor(folder_path):\n",
        "\n",
        "    images_list = []\n",
        "\n",
        "\n",
        "    files = sorted(os.listdir(folder_path))\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.resize(img, target_size, interpolation = cv2.INTER_LANCZOS4)\n",
        "        images_list.append(img)\n",
        "\n",
        "    img2 = np.stack(images_list, axis=0)\n",
        "    tensor = torch.tensor(img2)\n",
        "    tensor = tensor.to(torch.float16)\n",
        "    tensor = tensor / 255.0\n",
        "    tensor = tensor * 2.0\n",
        "    tensor = tensor - 1.0\n",
        "    tensor = tensor.permute(0, 3, 1, 2)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC3-WzjWHV1P"
      },
      "outputs": [],
      "source": [
        "def image_tensor_to_numpy(tensor):\n",
        "\n",
        "    tensor = tensor.permute(0, 2, 3, 1)\n",
        "    output = tensor.numpy()\n",
        "    output = output + 1.0\n",
        "    output = output / 2.0\n",
        "    output = output * 255.0\n",
        "    output = output.astype(np.uint8)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpK8t4bbYtHL",
        "outputId": "d71ef6d5-2088-4305-89be-6243334588ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Image_Sample\"\n",
        "img = image_folder_to_tensor(folder_path)\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "p6tNy_NqY22R",
        "outputId": "f5c8fe02-720d-48f1-b720-d2faa25e4a68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n\\n    mini = torch.min(tensor[tensor != 0])\\n    tensor = (tensor - mini) / (tensor.max() - mini)\\n    tensor = torch.where(tensor < 0, -1, tensor)\\n\\n\\n\\n\\n\\n\\n    return tensor\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# this is min max normalize\n",
        "'''\n",
        "def depth_folder_to_tensor(folder_path):\n",
        "\n",
        "    images_list = []\n",
        "\n",
        "\n",
        "\n",
        "    files = sorted(os.listdir(folder_path))\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        depth = np.array(Image.open(file_path), dtype=np.int16)\n",
        "        assert(np.max(depth) > 255)\n",
        "        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n",
        "        depth = depth.astype(np.float16) / 256.0\n",
        "        images_list.append(depth)\n",
        "\n",
        "\n",
        "\n",
        "    img2 = np.stack(images_list, axis=0)\n",
        "    tensor = torch.tensor(img2)\n",
        "\n",
        "    mini = torch.min(tensor[tensor != 0])\n",
        "    tensor = (tensor - mini) / (tensor.max() - mini)\n",
        "    tensor = torch.where(tensor < 0, -1, tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return tensor\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is z-score normalization\n",
        "\n",
        "def depth_folder_to_tensor(folder_path):\n",
        "\n",
        "    images_list = []\n",
        "\n",
        "\n",
        "\n",
        "    files = sorted(os.listdir(folder_path))\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        depth = np.array(Image.open(file_path), dtype=np.int16)\n",
        "        assert(np.max(depth) > 255)\n",
        "        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n",
        "        depth = depth.astype(np.float16) / 256.0\n",
        "        images_list.append(depth)\n",
        "\n",
        "\n",
        "\n",
        "    img2 = np.stack(images_list, axis=0)\n",
        "    tensor = torch.tensor(img2)\n",
        "    nonzero_mask = tensor != 0\n",
        "    zero_mask = tensor == 0\n",
        "    mean = tensor[nonzero_mask].mean()\n",
        "    std = tensor[nonzero_mask].std()\n",
        "    result = (tensor[nonzero_mask] - mean) / std\n",
        "    tensor[nonzero_mask] = result\n",
        "    tensor[zero_mask] = -1\n",
        "\n",
        "\n",
        "\n",
        "    return tensor, mean, std"
      ],
      "metadata": {
        "id": "TepPXyX-YZ9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL6JEZ3Q87-a"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Depth_Sample\"\n",
        "dep, depth_mean, depth_std = depth_folder_to_tensor(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThuEp1EvP1pE",
        "outputId": "950b0323-df8e-45c9-dc51-ac457d83b746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img, depth):\n",
        "        self.img = img\n",
        "        self.depth = depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "custom_dataset = CustomDataset(img, dep)\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "trainloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG_LEJfXP6iX"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/groundtruth_depth/2011_09_26_drive_0002_sync_groundtruth_depth_0000000005_image_02.png\"\n",
        "def depth_read(filename):\n",
        "\n",
        "    depth_png = np.array(Image.open(filename), dtype=int)\n",
        "\n",
        "    assert(np.max(depth_png) > 255)\n",
        "\n",
        "    depth = depth_png.astype(np.float32) / 256.\n",
        "    depth[depth_png == 0] = -1.\n",
        "    return depth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbfNMqSeP5pZ"
      },
      "outputs": [],
      "source": [
        "def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n",
        "    def sigmoid(x):\n",
        "        return 1 / (np.exp(-x) + 1)\n",
        "\n",
        "    if beta_schedule == \"quad\":\n",
        "        betas = (\n",
        "            np.linspace(\n",
        "                beta_start ** 0.5,\n",
        "                beta_end ** 0.5,\n",
        "                num_diffusion_timesteps,\n",
        "                dtype=np.float64,\n",
        "            )\n",
        "            ** 2\n",
        "        )\n",
        "    elif beta_schedule == \"linear\":\n",
        "        betas = np.linspace(\n",
        "            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
        "        )\n",
        "    elif beta_schedule == \"const\":\n",
        "        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
        "    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
        "        betas = 1.0 / np.linspace(\n",
        "            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n",
        "        )\n",
        "    elif beta_schedule == \"sigmoid\":\n",
        "        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n",
        "        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "    else:\n",
        "        raise NotImplementedError(beta_schedule)\n",
        "    assert betas.shape == (num_diffusion_timesteps,)\n",
        "    return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfltklszFw2M",
        "outputId": "1cd35c7b-037c-4ad0-ba01-b092b901ce92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "beta = torch.tensor(get_beta_schedule(beta_schedule, beta_start = beta_start, beta_end = beta_end, num_diffusion_timesteps = num_diffusion_timesteps))\n",
        "beta = beta.to(device)\n",
        "print(beta.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM6MyJPrGPUT"
      },
      "outputs": [],
      "source": [
        "def compute_alpha(beta, t): # t給tensor 一維的\n",
        "    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n",
        "    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n",
        "    return a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiqjRlDrHpok"
      },
      "outputs": [],
      "source": [
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMYCE8V6JK22"
      },
      "outputs": [],
      "source": [
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr4mMbzN6tXr"
      },
      "outputs": [],
      "source": [
        "def nonlinearity(x):\n",
        "\n",
        "    return x*torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WRkUz1I6-JB"
      },
      "outputs": [],
      "source": [
        "class Upsample(nn.Module): # this\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x, scale_factor=2.0, mode=\"nearest\") # double the size\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_be0bSq7A88"
      },
      "outputs": [],
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILHJ1zbU8LYb"
      },
      "outputs": [],
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.temb_channels = temb_channels\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        self.temb_proj = torch.nn.Linear(temb_channels,\n",
        "                                         out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout) # param為機率\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n",
        "                                                     out_channels,\n",
        "                                                     kernel_size=3,\n",
        "                                                     stride=1,\n",
        "                                                     padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n",
        "                                                    out_channels,\n",
        "                                                    kernel_size=1,\n",
        "                                                    stride=1,\n",
        "                                                    padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)    # normalize\n",
        "\n",
        "        h = nonlinearity(h)  # sigmoid\n",
        "        h = self.conv1(h)    # channel become out_channel\n",
        "\n",
        "        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy0-VDEM8uIl"
      },
      "outputs": [],
      "source": [
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b, c, h, w = q.shape # (batch, channel, height, width)\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n",
        "                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n",
        "        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n",
        "        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n",
        "        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b, c, h*w)\n",
        "        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
        "        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = torch.bmm(v, w_)\n",
        "        h_ = h_.reshape(b, c, h, w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownsampleFPN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n",
        "                                        out_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2qQRtwwWkXTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleFPN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n",
        "                                        int(out_channels),\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x, scale_factor=2.0, mode=\"nearest\") # double the size\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "QpRkaCZE2FM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        resolutions = config['model']['FPN_conv_res'].copy()\n",
        "\n",
        "        # resolutions = [64, 128, 256, 512]\n",
        "        self.resolutions = resolutions.copy()\n",
        "\n",
        "        # self.target_channel = int(resolutions[0] / 2)\n",
        "        self.target_channel = config['model']['FPN_target_C']\n",
        "\n",
        "        resolutions.insert(0, 3)\n",
        "        # self.resolutions = resolutions # which is list\n",
        "        self.ConvList = nn.ModuleList()\n",
        "        self.tuneChannels = nn.ModuleList()\n",
        "        self.Upsampple = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for idx in range(len(resolutions) - 1):\n",
        "          self.ConvList.append(DownsampleFPN(resolutions[idx],\n",
        "                                          resolutions[idx + 1],\n",
        "                                          True))\n",
        "\n",
        "          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n",
        "                                                   self.target_channel,\n",
        "                                                   kernel_size = 3,\n",
        "                                                   stride = 1,\n",
        "                                                   padding = 1))\n",
        "          if idx != len(resolutions) - 2:\n",
        "            self.Upsampple.append(Upsample(self.target_channel, True))\n",
        "\n",
        "        self.convOut = torch.nn.Conv2d(self.target_channel,\n",
        "                                      self.target_channel,\n",
        "                                      kernel_size = 1,\n",
        "                                      stride = 1,\n",
        "                                      padding = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        FPN_list = []\n",
        "\n",
        "        for idx in range(len(self.resolutions)):\n",
        "\n",
        "\n",
        "          if idx == 0:\n",
        "            h = self.pool(nonlinearity(self.ConvList[idx](h)))\n",
        "          else:\n",
        "            h = nonlinearity(self.ConvList[idx](temp))\n",
        "\n",
        "          temp = h\n",
        "          h = nonlinearity(self.tuneChannels[idx](h))\n",
        "          FPN_list.append(h)\n",
        "\n",
        "        for idx in reversed(range(len(self.resolutions))):\n",
        "          if idx == 0:\n",
        "            hold = self.convOut(hold + FPN_list[idx])\n",
        "            break\n",
        "          if idx == len(self.resolutions) - 1:\n",
        "            hold = self.Upsampple[idx - 1](FPN_list[idx])\n",
        "          else:\n",
        "            hold = hold + FPN_list[idx]\n",
        "            hold = self.Upsampple[idx - 1](hold)\n",
        "\n",
        "        return hold\n"
      ],
      "metadata": {
        "id": "H6dIsb3rx6CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class depth_phase1_block(nn.Module):\n",
        "  def __init__(self, in_cha, out_cha):\n",
        "    super().__init__()\n",
        "    self.conv = DownsampleFPN(in_cha, out_cha, True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn = nn.BatchNorm2d(out_cha)\n",
        "\n",
        "  def forward(self, depth):\n",
        "    return self.relu(self.bn(self.conv(depth)))"
      ],
      "metadata": {
        "id": "esdKjIMr9wcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class depth_phase2_block(nn.Module):\n",
        "  def __init__(self, in_cha, out_cha):\n",
        "    super().__init__()\n",
        "    self.conv = torch.nn.Conv2d(in_cha,\n",
        "                                out_cha,\n",
        "                                kernel_size = 3,\n",
        "                                stride = 1,\n",
        "                                padding = 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn = nn.BatchNorm2d(out_cha)\n",
        "\n",
        "  def forward(self, depth):\n",
        "    return self.relu(self.bn(self.conv(depth)))"
      ],
      "metadata": {
        "id": "icPtSBzxAKCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class depth_encode(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config.copy()\n",
        "        resolution = config['data']['image_size']\n",
        "        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n",
        "        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n",
        "        # targeted_size = 64\n",
        "        # enc_channels = [4, 16, 64, 256]\n",
        "        enc_channels = self.enc_channels.copy()\n",
        "        enc_channels.insert(0, 1)\n",
        "\n",
        "        phase1 = 0\n",
        "        while True:\n",
        "          phase1 = phase1 + 1\n",
        "          resolution = resolution / 2\n",
        "          if resolution == self.targeted_size:\n",
        "            break\n",
        "        phase2 = len(self.enc_channels) - phase1\n",
        "        self.phase1_model = nn.ModuleList()\n",
        "        self.phase2_model = nn.ModuleList()\n",
        "\n",
        "        for idx in range(phase1):\n",
        "          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n",
        "                                                      enc_channels[idx + 1]))\n",
        "        for idx in range(phase2):\n",
        "          print(enc_channels[phase1 + idx])\n",
        "          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n",
        "                                                      enc_channels[phase1 + idx + 1]))\n",
        "\n",
        "    def forward(self, depth):\n",
        "        h = depth.unsqueeze(1)\n",
        "\n",
        "        for idx in range(len(self.phase1_model)):\n",
        "          h = self.phase1_model[idx](h)\n",
        "\n",
        "        for idx in range(len(self.phase2_model)):\n",
        "          h = self.phase2_model[idx](h)\n",
        "\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "Fz0EX77czh3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class depth_decode(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config.copy()\n",
        "        self.ch = config['model']['ch']\n",
        "        # self.ch = 128\n",
        "        self.resolution = config['data']['image_size']\n",
        "        # self.resolution = 256\n",
        "        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n",
        "        # self.targeted_size = 64\n",
        "        count = 0\n",
        "        tmp = self.targeted_size\n",
        "        while True:\n",
        "          if tmp == self.resolution:\n",
        "            break\n",
        "          count = count + 1\n",
        "          tmp = tmp * 2\n",
        "\n",
        "        self.decode = nn.ModuleList()\n",
        "        in_cha = self.ch\n",
        "        self.relu = nn.ReLU()\n",
        "        for idx in range(count):\n",
        "          out_cha = in_cha / 4\n",
        "          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n",
        "          in_cha = out_cha\n",
        "        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n",
        "                                        1,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, pred):\n",
        "        for idx in range(len(self.decode)):\n",
        "          pred = self.decode[idx](pred)\n",
        "        pred = self.final_conv(pred)\n",
        "        pred = pred.squeeze(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "N2Vzf5ZRC8wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_from_list(values, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    \"\"\"\n",
        "    pick the values from vals\n",
        "    according to the indices stored in `t`\n",
        "    \"\"\"\n",
        "    result = values.gather(-1, t.cpu())\n",
        "    \"\"\"\n",
        "    if\n",
        "    x_shape = (5, 3, 64, 64)\n",
        "        -> len(x_shape) = 4\n",
        "        -> len(x_shape) - 1 = 3\n",
        "\n",
        "    and thus we reshape `out` to dims\n",
        "    (batch_size, 1, 1, 1)\n",
        "\n",
        "    \"\"\"\n",
        "    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ],
      "metadata": {
        "id": "xq591w1qfVBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel:\n",
        "    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n",
        "        self.start_schedule = start_schedule\n",
        "        self.end_schedule = end_schedule\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        \"\"\"\n",
        "        if\n",
        "            betas = [0.1, 0.2, 0.3, ...]\n",
        "        then\n",
        "            alphas = [0.9, 0.8, 0.7, ...]\n",
        "            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n",
        "        self.betas = torch.tensor(betas)\n",
        "\n",
        "\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "\n",
        "    def forward(self, x_0, t, device):\n",
        "        \"\"\"\n",
        "        x_0: (B, C, H, W)\n",
        "        t: (B,)\n",
        "        \"\"\"\n",
        "\n",
        "        noise = torch.randn_like(x_0)\n",
        "        print(\"alphas_cumprod.sqrt().dtype = {}\".format(self.alphas_cumprod.sqrt().dtype) )\n",
        "        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n",
        "\n",
        "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n",
        "\n",
        "        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n",
        "        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n",
        "\n",
        "        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n"
      ],
      "metadata": {
        "id": "AeTTDyc0fZOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n",
        "        num_res_blocks = config['model']['num_res_blocks']\n",
        "        attn_resolutions = config['model']['attn_resolutions']\n",
        "        dropout = config['model']['dropout']\n",
        "        in_channels = config['model']['in_channels']\n",
        "        resolution = config['data']['image_size']\n",
        "        resamp_with_conv = config['model']['resamp_with_conv']\n",
        "        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n",
        "        depth_enc_channels = config['model']['depth_enc_channels']\n",
        "        if config['model']['type'] == 'bayesian':\n",
        "            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n",
        "        self.fpn = FPN(config)\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # timestep embedding\n",
        "        self.temb = nn.Module()\n",
        "        self.temb.dense = nn.ModuleList([\n",
        "            torch.nn.Linear(self.ch,\n",
        "                            self.temb_ch),\n",
        "            torch.nn.Linear(self.temb_ch,\n",
        "                            self.temb_ch),\n",
        "        ])\n",
        "\n",
        "        '''\n",
        "        # timestep embedding for diffusion---vvv\n",
        "        self.temb.diff1 = nn.Linear(1, 1)\n",
        "        self.temb.diff2 = nn.Linear(1, 1)\n",
        "        # timestep embedding for diffusion---^^^\n",
        "        '''\n",
        "\n",
        "\n",
        "        self.depth_encode = depth_encode(config)\n",
        "\n",
        "        # diffusion process ---vvv\n",
        "        self.beta_schedule = config['diffusion']['beta_schedule']\n",
        "        self.start_schedule = config['diffusion']['beta_start']\n",
        "        self.end_schedule = config['diffusion']['beta_end']\n",
        "        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n",
        "        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n",
        "        # diffusion process ---^^^\n",
        "\n",
        "\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        block_in = None\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = AttnBlock(block_in)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(AttnBlock(block_in))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "\n",
        "        self.depth_decode = depth_decode(config)\n",
        "\n",
        "    def forward(self, image, depth, t):\n",
        "        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = get_timestep_embedding(t, self.ch).to(device)\n",
        "        temb = self.temb.dense[0](temb)\n",
        "        temb = nonlinearity(temb)\n",
        "        temb = self.temb.dense[1](temb)\n",
        "\n",
        "\n",
        "\n",
        "        img_enc = self.fpn(image)\n",
        "        depth = self.depth_encode(depth)\n",
        "\n",
        "\n",
        "        # depth = depth.unsqueeze(1)\n",
        "        # return img_enc, depth\n",
        "\n",
        "        # diffusion process ---vvv\n",
        "\n",
        "        noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n",
        "        noisy_map = noisy_map.to(torch.float32)\n",
        "        noise = noise.to(torch.float32)\n",
        "\n",
        "\n",
        "        # diffusion process ---^^^\n",
        "\n",
        "\n",
        "        # concat img_enc and noisy_map\n",
        "        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n",
        "        # return backbone_input\n",
        "\n",
        "\n",
        "\n",
        "        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n",
        "        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n",
        "        # hs = [self.conv_in(image)]\n",
        "\n",
        "        hs = [self.conv_in(backbone_input)]\n",
        "\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.depth_decode(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "BW2SNSqY0qys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(config)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ParlMz0UnMrq",
        "outputId": "248c8c74-a121-4257-ef93-95f7947a416b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "64\n",
            "128\n",
            "32.0\n",
            "here\n",
            "32.0\n",
            "8.0\n",
            "here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 256\n",
        "NO_EPOCHS = 100\n",
        "PRINT_FREQUENCY = 1\n",
        "LR = 0.001\n",
        "VERBOSE = False\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "8VKW8xA20JkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用DataLoader加载数据集\n",
        "batch_size = 2\n",
        "trainloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "validloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "6ZDEnqO21HxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first epoch\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "for epoch in range(NO_EPOCHS):\n",
        "    mean_epoch_loss = []\n",
        "    mean_epoch_loss_val = []\n",
        "    for batch in trainloader:\n",
        "        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n",
        "\n",
        "        input_img = batch['img'].to(torch.float32).to(device)\n",
        "        target_depth = batch['depth'].to(torch.float32).to(device)\n",
        "\n",
        "        pred_depth = model(input_img, target_depth, t)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "        mean_epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for batch in validloader:\n",
        "        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n",
        "        input_img = batch['img'].to(torch.float32).to(device)\n",
        "        target_depth = batch['depth'].to(torch.float32).to(device)\n",
        "\n",
        "        pred_depth = model(input_img, target_depth, t)\n",
        "\n",
        "        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "        mean_epoch_loss_val.append(val_loss.item())\n",
        "\n",
        "    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\n",
        "        checkpoint = {\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n",
        "          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n",
        "          'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'model_checkpoint_epoch_{}.pth'.format(epoch))\n",
        "        source_path = 'model_checkpoint_epoch_{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/weight_save'\n",
        "\n",
        "\n",
        "        # save them to the google drive\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "\n",
        "\n",
        "        #-----以下是存loss的---vvv\n",
        "        checkpoint = {\n",
        "          'epoch': epoch,\n",
        "          'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'model_checkpoint_loss_epoch_{}.pth'.format(epoch))\n",
        "        source_path = 'model_checkpoint_loss_epoch_{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/loss_save'\n",
        "\n",
        "\n",
        "        # save them to the google drive\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        #-----以下是存loss的---^^^\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")"
      ],
      "metadata": {
        "id": "E67RH3azzzGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continue training\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "initial = 210\n",
        "NO_EPOCHS = 200 # 要多做幾個epochs\n",
        "load_path = '/content/drive/MyDrive/Colab Notebooks/dtransposed_diffusion_model/weight_save/model_checkpoint_epoch_{}.pth'.format(initial) # 位置要改\n",
        "checkpoint = torch.load(load_path)\n",
        "\n",
        "start = checkpoint['epoch'] + 1\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "for epoch in range(start , start + NO_EPOCHS + 1):\n",
        "    mean_epoch_loss = []\n",
        "    mean_epoch_loss_val = []\n",
        "    for batch in trainloader:\n",
        "        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n",
        "\n",
        "        input_img = batch['img'].to(torch.float32).to(device)\n",
        "        target_depth = batch['depth'].to(torch.float32).to(device)\n",
        "\n",
        "        pred_depth = model(input_img, target_depth, t)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "        mean_epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for batch in validloader:\n",
        "        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n",
        "        input_img = batch['img'].to(torch.float32).to(device)\n",
        "        target_depth = batch['depth'].to(torch.float32).to(device)\n",
        "\n",
        "        pred_depth = model(input_img, target_depth, t)\n",
        "\n",
        "        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n",
        "        mean_epoch_loss_val.append(val_loss.item())\n",
        "\n",
        "    if epoch % PRINT_FREQUENCY == 0 or epoch == start + NO_EPOCHS:\n",
        "        checkpoint = {\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n",
        "          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n",
        "          'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'model_checkpoint_epoch_{}.pth'.format(epoch))\n",
        "        source_path = 'model_checkpoint_epoch_{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/weight_save'\n",
        "\n",
        "\n",
        "        # save them to the google drive\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "        #-----以下是存loss的---vvv\n",
        "        checkpoint = {\n",
        "          'epoch': epoch,\n",
        "          'valid_loss' : np.mean(mean_epoch_loss_val),\n",
        "          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, 'model_checkpoint_loss_epoch_{}.pth'.format(epoch))\n",
        "        source_path = 'model_checkpoint_loss_epoch_{}.pth'.format(epoch)\n",
        "        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/loss_save'\n",
        "\n",
        "\n",
        "        # save them to the google drive\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        #-----以下是存loss的---^^^\n",
        "\n",
        "\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")"
      ],
      "metadata": {
        "id": "hw122Fd47aPB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}