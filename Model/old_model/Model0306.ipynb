{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7MyEk7vhmJf0//vX71Z4n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4LyamW0ULByJ"},"outputs":[],"source":["import torch\n","import cv2\n","import os\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","source":["def load_config(file_path):\n","    with open(file_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config\n"],"metadata":{"id":"w_eiqEcaLHXL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_params(model):\n","\n","  sum = 0\n","  for param in model.parameters():\n","    sum = sum + param.numel()\n","  return sum"],"metadata":{"id":"uQ9yOWy_LKKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/config.yml'\n","config = load_config(file_path)"],"metadata":{"id":"sndWKcn_LNLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_size = (config['data']['image_size'], config['data']['image_size'])"],"metadata":{"id":"ueJSxJGiLRrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def image_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","\n","        img = cv2.imread(file_path)\n","        img = cv2.resize(img, target_size, interpolation = cv2.INTER_LANCZOS4)\n","        images_list.append(img)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","\n","    tensor = tensor.to(torch.float16)\n","\n","\n","    tensor = tensor / 255.0\n","    tensor = tensor * 2.0\n","    tensor = tensor - 1.0\n","    tensor = tensor.permute(0, 3, 1, 2)\n","\n","\n","\n","    return tensor\n"],"metadata":{"id":"_ZhcpYDNLToq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def image_tensor_to_numpy(tensor):\n","\n","    tensor = tensor.permute(0, 2, 3, 1)\n","    output = tensor.numpy()\n","    output = output + 1.0\n","    output = output / 2.0\n","    output = output * 255.0\n","    output = output.astype(np.uint8)\n","\n","\n","\n","    return output"],"metadata":{"id":"cjj7OL1CLXQ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Image_Sample\"\n","img = image_folder_to_tensor(folder_path)"],"metadata":{"id":"8vNlE4FwLZ7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this is z-score normalization\n","\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","    nonzero_mask = tensor != 0\n","    zero_mask = tensor == 0\n","    mean = tensor[nonzero_mask].mean()\n","    std = tensor[nonzero_mask].std()\n","    result = (tensor[nonzero_mask] - mean) / std\n","    tensor[nonzero_mask] = result\n","    tensor[zero_mask] = -1\n","\n","\n","\n","    return tensor, mean, std"],"metadata":{"id":"0b6qGiPsLajN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Depth_Sample\"\n","dep, depth_mean, depth_std = depth_folder_to_tensor(folder_path)"],"metadata":{"id":"n3LLpQqBLgRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, img, depth):\n","        self.img = img\n","        self.depth = depth\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","        return sample\n","\n","\n","\n","\n"],"metadata":{"id":"cGnDFyjVLidb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 创建数据集实例\n","custom_dataset = CustomDataset(img, dep)\n","\n","# 使用DataLoader加载数据集\n","batch_size = 32\n","batch_size = 8\n","batch_size = 2\n","data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n","count = 0\n","# 迭代DataLoader获取批次数据\n","for batch in data_loader:\n","    count = count + 1\n","    input_img = batch['img']\n","    target_depth = batch['depth']\n","    break\n","input_img.shape"],"metadata":{"id":"sxamTATxL4kz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def depth_read(filename):\n","    # loads depth map D from png file\n","    # and returns it as a numpy array,\n","    # for details see readme.txt\n","\n","    depth_png = np.array(Image.open(filename), dtype=int)\n","    # make sure we have a proper 16bit depth map here.. not 8bit!\n","    assert(np.max(depth_png) > 255)\n","\n","    depth = depth_png.astype(np.float32) / 256.\n","    depth[depth_png == 0] = -1.\n","    return depth"],"metadata":{"id":"zU30CLDHL5Ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","    def sigmoid(x):\n","        return 1 / (np.exp(-x) + 1)\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas"],"metadata":{"id":"nEgbW64jMSTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["beta = torch.tensor(get_beta_schedule(beta_schedule, beta_start = beta_start, beta_end = beta_end, num_diffusion_timesteps = num_diffusion_timesteps))\n","beta = beta.to(device)"],"metadata":{"id":"80eJ0gJnMXcP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a"],"metadata":{"id":"kzn672ZRMYx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_timestep_embedding(timesteps, embedding_dim):\n","\n","    assert len(timesteps.shape) == 1\n","\n","    half_dim = embedding_dim // 2\n","    emb = math.log(10000) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","    emb = emb.to(device=timesteps.device)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:  # zero pad\n","        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n","    return emb"],"metadata":{"id":"CN7tOb3wwmgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Normalize(in_channels):\n","    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"],"metadata":{"id":"ssvpXPdewm3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def nonlinearity(x):\n","\n","    return x*torch.sigmoid(x)"],"metadata":{"id":"iuN4FFviwtAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x"],"metadata":{"id":"ZvTrCLaLwvRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"],"metadata":{"id":"BJorWlREwxRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 = Normalize(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = Normalize(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","    def forward(self, x, temb):\n","        # print(\"in = {}\".format(self.in_channels))\n","        # print(\"out = {}\".format(self.out_channels))\n","        # print(\"x.shape = {}\".format(x.shape))\n","        # print(\"temb.shape = {}\".format(temb.shape))\n","        # print(\"temb_channel = {}\".format(self.temb_channels))\n","        h = x\n","        h = self.norm1(h)    # normalize\n","        h = nonlinearity(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","        # print(\"h.shape = {}\".format(h.shape))\n","        # print(\"self.temb_proj(nonlinearity(temb)).shape = {}\".format(self.temb_proj(nonlinearity(temb)).shape))\n","        # print(\"temb_proj(nonlinearity(temb))[:, :, None, None].shape = {}\".format(self.temb_proj(nonlinearity(temb))[:, :, None, None].shape))\n","        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        # temb是什麼 是用來做positional embedding的嗎????????\n","\n","        h = self.norm2(h)\n","        h = nonlinearity(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"],"metadata":{"id":"8rEpksZ_w1I2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = Normalize(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"],"metadata":{"id":"APxWVLOIw3Im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"],"metadata":{"id":"RksT_icCw4-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = x\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.pool(nonlinearity(self.ConvList[idx](h)))\n","          else:\n","            h = nonlinearity(self.ConvList[idx](temp))\n","\n","          temp = h\n","          h = nonlinearity(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        for i in range(len(FPN_list)):\n","          print(\"i = {}\".format(i))\n","          print((\"FPN_list[i].shape = {}\".format(FPN_list[i].shape)))\n","\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            hold = self.convOut(hold + FPN_list[idx])\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","          else:\n","            hold = hold + FPN_list[idx]\n","            hold = self.Upsampple[idx - 1](hold)\n","          print(\"idx = {}\".format(idx))\n","          print(\"hold.shape = {}\".format(hold.shape))\n","        return hold"],"metadata":{"id":"TDtr2BmLw7cM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    self.relu = nn.ReLU()\n","    self.bn = nn.BatchNorm2d(out_cha)\n","\n","  def forward(self, depth):\n","    return self.relu(self.bn(self.conv(depth)))"],"metadata":{"id":"hPILqlXFw9N_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    self.relu = nn.ReLU()\n","    self.bn = nn.BatchNorm2d(out_cha)\n","\n","  def forward(self, depth):\n","    return self.relu(self.bn(self.conv(depth)))"],"metadata":{"id":"MSXSsXBow-ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        # print(enc_channels)\n","        for idx in range(phase2):\n","          print(enc_channels[phase1 + idx])\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","        # print(len(self.phase2_model))\n","\n","\n","\n","\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","\n","        for idx in range(len(self.phase1_model)):\n","          # print(idx)\n","          h = self.phase1_model[idx](h)\n","        for idx in range(len(self.phase2_model)):\n","          # print(idx)\n","          h = self.phase2_model[idx](h)\n","\n","\n","\n","\n","        return h\n"],"metadata":{"id":"dQyTwY0yxAfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_index_from_list(values, t, x_shape):\n","    batch_size = t.shape[0]\n","    \"\"\"\n","    pick the values from vals\n","    according to the indices stored in `t`\n","    \"\"\"\n","    result = values.gather(-1, t.cpu())\n","    \"\"\"\n","    if\n","    x_shape = (5, 3, 64, 64)\n","        -> len(x_shape) = 4\n","        -> len(x_shape) - 1 = 3\n","\n","    and thus we reshape `out` to dims\n","    (batch_size, 1, 1, 1)\n","\n","    \"\"\"\n","    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"],"metadata":{"id":"2Lk7Ix2UxDbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","        noise = torch.randn_like(x_0)\n","        print(\"alphas_cumprod.sqrt().dtype = {}\".format(self.alphas_cumprod.sqrt().dtype) )\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar"],"metadata":{"id":"kFgyYuyOxFab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(in_channels,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = Normalize(block_in)\n","        self.conv_out = torch.nn.Conv2d(block_in,\n","                                        out_ch,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, image, depth, t):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = nonlinearity(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","\n","        img_enc = self.fpn(image)\n","        depth = self.depth_encode(depth)\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","\n","        noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        noisy_map2 = torch.cat([noisy_map, img_enc], dim = 1)\n","        return noisy_map2, noisy_map, noise, img_enc\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        hs = [self.conv_in(image)]\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = nonlinearity(h)\n","        h = self.conv_out(h)\n","        return h"],"metadata":{"id":"MGpawavOxJLi"},"execution_count":null,"outputs":[]}]}