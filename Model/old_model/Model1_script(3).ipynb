{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TL6RdTKY4KNI","executionInfo":{"status":"ok","timestamp":1710440370292,"user_tz":-480,"elapsed":6412,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["# model 1 把所有normalize換成 batch normalize, 所有nonlinear 換成leaky relu, 在fpn最前面加入norm並且在所有相加的部分前加入norm\n","import torch\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","import random\n","from google.colab import files\n","import sys\n","import time\n","from torch.utils.data import random_split\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sfQ1zEzuzZL2","executionInfo":{"status":"ok","timestamp":1710440370297,"user_tz":-480,"elapsed":17,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def deb(param, str):\n","  print(str + \" = {}\".format(param))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5T3gP7QULTMt","executionInfo":{"status":"ok","timestamp":1710440370297,"user_tz":-480,"elapsed":16,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def load_config(file_path):\n","    with open(file_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config"]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/depth_analysis.pth' # 讀取depth的統計數字\n","\n","check = torch.load(path)\n","total_sum = check['total_sum']\n","DEPTH_NONZERO = check['total_nonzero']\n","DEPTH_MEAN = check['total_mean']\n","DEPTH_STD = check['total_std']\n","del check"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"E9m-sHCRz5Dg","executionInfo":{"status":"error","timestamp":1710440370298,"user_tz":-480,"elapsed":17,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"96d9b82d-443f-48e2-d925-34ab14a8d300"},"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/depth_analysis.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d56275ebd755>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/depth_analysis.pth'\u001b[0m \u001b[0;31m# 讀取depth的統計數字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDEPTH_NONZERO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_nonzero'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/depth_analysis.pth'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fedtFZjG6Ucb","executionInfo":{"status":"aborted","timestamp":1710440370298,"user_tz":-480,"elapsed":15,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def count_params(model):\n","  sum = 0\n","  for param in model.parameters():\n","    sum = sum + param.numel()\n","  return sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H-DIrmZLVw4","executionInfo":{"status":"aborted","timestamp":1710440370298,"user_tz":-480,"elapsed":14,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["file_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config.yml'\n","config = load_config(file_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"n9i9pX5s6Gzx","executionInfo":{"status":"error","timestamp":1710440370936,"user_tz":-480,"elapsed":6,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"colab":{"base_uri":"https://localhost:8080/","height":147},"outputId":"d47d7f79-0b1b-49bd-e945-1b7d4c34efc5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'config' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-820a0c24e6a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"]}],"source":["target_size = (config['data']['image_size'], config['data']['image_size'])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zzwmWbrdbkyo","executionInfo":{"status":"ok","timestamp":1710440371932,"user_tz":-480,"elapsed":6,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"342954b2-e4b7-4b51-e719-b406e051d2ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef find_next_file(first, second):\\n  first_list = sorted(os.listdir(first))\\n  second_list = sorted(os.listdir(second))\\n  length = len(first_list)\\n  while True:\\n    rand_int = random.randint(0, length - 1)\\n    target = first_list[rand_int]\\n    if target in second_list:\\n      return first + '/' + target, second + '/' + target\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["'''\n","def find_next_file(first, second):\n","  first_list = sorted(os.listdir(first))\n","  second_list = sorted(os.listdir(second))\n","  length = len(first_list)\n","  while True:\n","    rand_int = random.randint(0, length - 1)\n","    target = first_list[rand_int]\n","    if target in second_list:\n","      return first + '/' + target, second + '/' + target\n","'''"]},{"cell_type":"code","source":["'''\n","def compute_depth_mean(path):\n","  file_list = sorted(os.listdir(path))\n","  total_sum = 0\n","\n","  total_nonzero = 0\n","  # count = 0\n","  for name in file_list:\n","    file_path = path + '/' + name\n","    check = torch.load(file_path)\n","\n","    target = torch.tensor(check['depth_list']).to(torch.float64)\n","    print(target.dtype)\n","    total_sum += torch.sum(target)\n","\n","    total_nonzero += torch.nonzero(target).size(0)\n","    # count += 1\n","    # if count == 2:\n","    #   break\n","  total_mean = total_sum / total_nonzero\n","  return total_sum, total_nonzero, total_mean\n","\n","\n","'''"],"metadata":{"id":"oK1GUGywv-7X","executionInfo":{"status":"ok","timestamp":1710440372412,"user_tz":-480,"elapsed":6,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"44bcb0b3-0870-4d3b-826d-8d497e0e4802"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef compute_depth_mean(path):\\n  file_list = sorted(os.listdir(path))\\n  total_sum = 0\\n\\n  total_nonzero = 0\\n  # count = 0\\n  for name in file_list:\\n    file_path = path + '/' + name\\n    check = torch.load(file_path)\\n\\n    target = torch.tensor(check['depth_list']).to(torch.float64)\\n    print(target.dtype)\\n    total_sum += torch.sum(target)\\n\\n    total_nonzero += torch.nonzero(target).size(0)\\n    # count += 1\\n    # if count == 2:\\n    #   break\\n  total_mean = total_sum / total_nonzero\\n  return total_sum, total_nonzero, total_mean\\n\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["'''\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","total_sum, total_nonzero, total_mean = compute_depth_mean(path)\n","print(total_sum, total_nonzero, total_mean)\n","'''"],"metadata":{"id":"Wo_UMEDwy-uE","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1710440372412,"user_tz":-480,"elapsed":5,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"4f84b65e-7a60-4720-966e-24e58434e4e0"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\\ntotal_sum, total_nonzero, total_mean = compute_depth_mean(path)\\nprint(total_sum, total_nonzero, total_mean)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["'''\n","def compute_depth_std(path, total_mean, total_nonzero):\n","  file_list = sorted(os.listdir(path))\n","\n","\n","  total_sum = 0\n","  # count = 0\n","  for name in file_list:\n","    file_path = path + '/' + name\n","    check = torch.load(file_path)\n","\n","    target = torch.tensor(check['depth_list']).to(torch.float64)\n","    non_zero_mask = target != 0\n","\n","    target = target[non_zero_mask]\n","    target = target - total_mean\n","    target = target ** 2\n","    target = target / (total_nonzero - 1)\n","    total_sum += torch.sum(target)\n","\n","\n","    # count += 1\n","    # if count == 2:\n","    #   break\n","\n","  return total_sum\n","'''\n","\n"],"metadata":{"id":"Yd05l7k_40OY","executionInfo":{"status":"ok","timestamp":1710425169086,"user_tz":-480,"elapsed":79,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"6d68ca8f-4364-44d4-831e-d2a7596420dc"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef compute_depth_std(path, total_mean, total_nonzero):\\n  file_list = sorted(os.listdir(path))\\n\\n\\n  total_sum = 0\\n  # count = 0\\n  for name in file_list:\\n    file_path = path + '/' + name\\n    check = torch.load(file_path)\\n\\n    target = torch.tensor(check['depth_list']).to(torch.float64)\\n    non_zero_mask = target != 0\\n\\n    target = target[non_zero_mask]\\n    target = target - total_mean\\n    target = target ** 2\\n    target = target / (total_nonzero - 1)\\n    total_sum += torch.sum(target)\\n\\n\\n    # count += 1\\n    # if count == 2:\\n    #   break\\n\\n  return total_sum\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["'''\n","std_square = compute_depth_std(path, total_mean, total_nonzero)\n","print(std_square)\n","'''"],"metadata":{"id":"ffZtBw7m-VCw","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1710425169086,"user_tz":-480,"elapsed":77,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"df8b3b79-eb54-4ca3-a04b-307b5d0cd547"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nstd_square = compute_depth_std(path, total_mean, total_nonzero)\\nprint(std_square)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["'''\n","checkpoint = {\n","          'total_sum': total_sum,\n","          'total_nonzero': total_nonzero, # model.state_dict()是存下param的的值和形狀\n","          'total_mean': total_mean, # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'total_std' : total_std\n","        }\n","\n","torch.save(checkpoint, 'depth_analysis2.pth')\n","source_path = 'depth_analysis2.pth'\n","destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model'\n","shutil.copy(source_path, destination_path)\n","'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"yA8gEQSWBoWl","executionInfo":{"status":"ok","timestamp":1710425169086,"user_tz":-480,"elapsed":76,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"d3ea2758-f4af-4038-9ef2-94259a1c6f6b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ncheckpoint = {\\n          'total_sum': total_sum,\\n          'total_nonzero': total_nonzero, # model.state_dict()是存下param的的值和形狀\\n          'total_mean': total_mean, # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\\n          'total_std' : total_std\\n        }\\n\\ntorch.save(checkpoint, 'depth_analysis2.pth')\\nsource_path = 'depth_analysis2.pth'\\ndestination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model'\\nshutil.copy(source_path, destination_path)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"QNeKReI2rjYC","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":76,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"aabd38c9-698b-41fe-e2c4-686ad30ba01a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef find_next_file(first, second):\\n  first_list = sorted(os.listdir(first))\\n  second_list = sorted(os.listdir(second))\\n  length = len(first_list)\\n  while True:\\n    rand_int = random.randint(0, length - 1)\\n    target = first_list[rand_int]\\n    if target in second_list:\\n      return first + '/' + target, second + '/' + target\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["'''\n","def find_next_file(first, second):\n","  first_list = sorted(os.listdir(first))\n","  second_list = sorted(os.listdir(second))\n","  length = len(first_list)\n","  while True:\n","    rand_int = random.randint(0, length - 1)\n","    target = first_list[rand_int]\n","    if target in second_list:\n","      return first + '/' + target, second + '/' + target\n","'''"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"V91AbNDUKCF5","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":75,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"972c6d67-84ec-4b8d-cf50-fdc4256197b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef create_dataset_save(first_path, second_path, epoch_size = 5000): # first is depth, second is image\\n  depth_list = []\\n  image_list = []\\n  depth_name_list = []\\n  image_name_list = []\\n  count = 0\\n  file_count = 8  #改\\n\\n  first_path_list1 = sorted(os.listdir(first_path))\\n  second_path_list1 = sorted(os.listdir(second_path))\\n  idx1_start = 109   #改\\n\\n  for idx1 in range(idx1_start, len(first_path_list1)):\\n\\n    first_seq1 = first_path_list1[idx1]\\n    # if first_seq1 == '2011_09_28_drive_0090_sync': #改\\n    #   continue\\n    first_path2 = first_path + '/' + first_seq1 + '/proj_depth/groundtruth'\\n    second_path2 = second_path + '/' + first_seq1\\n    first_path_list2 = sorted(os.listdir(first_path2))\\n\\n    if idx1 == idx1_start:\\n      idx2_start = 1  # 改\\n    else:\\n      idx2_start = 0\\n    for idx2 in range(idx2_start, len(first_path_list2)): # image02, image03\\n      first_seq2 = first_path_list2[idx2]\\n      first_path3 = first_path2 + '/' + first_seq2\\n      second_path3 = second_path2 + '/' + first_seq2 + '/data'\\n      first_path_list3 = sorted(os.listdir(first_path3))\\n      if idx1 == idx1_start and idx2 == idx2_start:\\n        idx3_start = 58 #改成+1\\n      else:\\n        idx3_start = 0\\n      for idx3 in range(idx3_start, len(first_path_list3)):\\n        first_seq3 = first_path_list3[idx3]\\n        first_path4 = first_path3 + '/' + first_seq3\\n        second_path4 = second_path3 + '/' + first_seq3\\n        print(count)\\n        print(first_path4)\\n        print(second_path4)\\n        if first_path4[-5] == ')':\\n          continue\\n        depth = np.array(Image.open(first_path4), dtype=np.int16)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n\\n\\n        image = cv2.imread(second_path4)\\n\\n\\n        image = cv2.resize(image, target_size, interpolation = cv2.INTER_LANCZOS4)\\n        depth_list.append(depth)\\n        depth_name_list.append(first_path4)\\n        image_list.append(image)\\n        image_name_list.append(second_path4)\\n        count = count + 1\\n\\n\\n        if count == epoch_size:\\n          count = 0\\n          checkpoint = {\\n              'depth_list': depth_list,\\n              'image_list' : image_list,\\n          }\\n          checkpoint2 = {\\n              'depth_name' : depth_name_list,\\n              'image_name' : image_name_list\\n          }\\n          name = 'dataset_{}.pth'.format(file_count)\\n          name2 = 'name_list_{}.pth'.format(file_count)\\n          file_count = file_count + 1\\n          torch.save(checkpoint, name)\\n          torch.save(checkpoint2, name2)\\n          source_path = name\\n          source_path2 = name2\\n          destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\\n          destination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\n\\n          shutil.copy(source_path, destination_path)\\n          shutil.copy(source_path2, destination_path2)\\n\\n          depth_name_list = []\\n          image_name_list = []\\n          depth_list = []\\n          image_list = []\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["'''\n","def create_dataset_save(first_path, second_path, epoch_size = 5000): # first is depth, second is image\n","  depth_list = []\n","  image_list = []\n","  depth_name_list = []\n","  image_name_list = []\n","  count = 0\n","  file_count = 8  #改\n","\n","  first_path_list1 = sorted(os.listdir(first_path))\n","  second_path_list1 = sorted(os.listdir(second_path))\n","  idx1_start = 109   #改\n","\n","  for idx1 in range(idx1_start, len(first_path_list1)):\n","\n","    first_seq1 = first_path_list1[idx1]\n","    # if first_seq1 == '2011_09_28_drive_0090_sync': #改\n","    #   continue\n","    first_path2 = first_path + '/' + first_seq1 + '/proj_depth/groundtruth'\n","    second_path2 = second_path + '/' + first_seq1\n","    first_path_list2 = sorted(os.listdir(first_path2))\n","\n","    if idx1 == idx1_start:\n","      idx2_start = 1  # 改\n","    else:\n","      idx2_start = 0\n","    for idx2 in range(idx2_start, len(first_path_list2)): # image02, image03\n","      first_seq2 = first_path_list2[idx2]\n","      first_path3 = first_path2 + '/' + first_seq2\n","      second_path3 = second_path2 + '/' + first_seq2 + '/data'\n","      first_path_list3 = sorted(os.listdir(first_path3))\n","      if idx1 == idx1_start and idx2 == idx2_start:\n","        idx3_start = 58 #改成+1\n","      else:\n","        idx3_start = 0\n","      for idx3 in range(idx3_start, len(first_path_list3)):\n","        first_seq3 = first_path_list3[idx3]\n","        first_path4 = first_path3 + '/' + first_seq3\n","        second_path4 = second_path3 + '/' + first_seq3\n","        print(count)\n","        print(first_path4)\n","        print(second_path4)\n","        if first_path4[-5] == ')':\n","          continue\n","        depth = np.array(Image.open(first_path4), dtype=np.int16)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","\n","\n","        image = cv2.imread(second_path4)\n","\n","\n","        image = cv2.resize(image, target_size, interpolation = cv2.INTER_LANCZOS4)\n","        depth_list.append(depth)\n","        depth_name_list.append(first_path4)\n","        image_list.append(image)\n","        image_name_list.append(second_path4)\n","        count = count + 1\n","\n","\n","        if count == epoch_size:\n","          count = 0\n","          checkpoint = {\n","              'depth_list': depth_list,\n","              'image_list' : image_list,\n","          }\n","          checkpoint2 = {\n","              'depth_name' : depth_name_list,\n","              'image_name' : image_name_list\n","          }\n","          name = 'dataset_{}.pth'.format(file_count)\n","          name2 = 'name_list_{}.pth'.format(file_count)\n","          file_count = file_count + 1\n","          torch.save(checkpoint, name)\n","          torch.save(checkpoint2, name2)\n","          source_path = name\n","          source_path2 = name2\n","          destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\n","          destination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","\n","          shutil.copy(source_path, destination_path)\n","          shutil.copy(source_path2, destination_path2)\n","\n","          depth_name_list = []\n","          image_name_list = []\n","          depth_list = []\n","          image_list = []\n","\n","'''"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"yj7P7vU4eRNg","outputId":"f16b70b6-2462-4dff-de32-cdaf04ac372a","executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":74,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfirst_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\\nsecond_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Image'\\ncreate_dataset_save(first_path, second_path, epoch_size = 5000) # first is depth, second is image\\n\\nfrom google.colab import runtime\\nruntime.unassign()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["'''\n","first_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\n","second_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Image'\n","create_dataset_save(first_path, second_path, epoch_size = 5000) # first is depth, second is image\n","\n","from google.colab import runtime\n","runtime.unassign()\n","'''"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"OrfRJnibPUX8","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":74,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"43b6c101-826d-4e78-a174-9e63df48815b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nimage_name_list = checkpoint['depth_name']\\ndepth_name_list = checkpoint['depth_name']\\nprint(depth_name_list[4999])\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["'''\n","image_name_list = checkpoint['depth_name']\n","depth_name_list = checkpoint['depth_name']\n","print(depth_name_list[4999])\n","'''"]},{"cell_type":"code","source":["'''\n","# for finding the location where it breaked\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == '2011_09_28_drive_0184_sync':\n","    print(idx)\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == 'image_03':\n","    print(idx)\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth/image_03'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == '0000000062.png':\n","    print(idx)\n","'''"],"metadata":{"id":"S2eyANJ8GiqA","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":73,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"f9429a0a-1847-4fcf-f175-672d3b591b6c"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# for finding the location where it breaked\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == '2011_09_28_drive_0184_sync':\\n    print(idx)\\n\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == 'image_03':\\n    print(idx)\\n\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth/image_03'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == '0000000062.png':\\n    print(idx)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"41yd1a4oz7ih","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1710425169087,"user_tz":-480,"elapsed":72,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"eda178c9-1001-4a83-de55-08f984ea4fe6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef create_dataset(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\\n\\n  name_list = sorted(os.listdir(path))\\n\\n  if data_per_epoch % amount_from_file != 0:\\n    print(\"error, data_per_epoch can\\'t divide amount_from_file !!!!!\")\\n    sys.exit(1)\\n\\n  file_idx = data_per_epoch // amount_from_file\\n\\n  output_depth = []\\n  output_image = []\\n  length = len(name_list)\\n  for idx in range(file_idx):\\n    rand_int = random.randint(0, length - 1)\\n    tmp_file = name_list[rand_int]\\n    name = path + \\'/\\' + tmp_file\\n    checkpoint = torch.load(name)\\n    depth_list = checkpoint[\\'depth_list\\']\\n    image_list = checkpoint[\\'image_list\\']\\n    length_inside = len(depth_list)\\n    random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n\\n    for jdx in range(amount_from_file):\\n\\n      output_depth.append(depth_list[random_number[jdx]])\\n      output_image.append(image_list[random_number[jdx]])\\n  return output_image, output_depth\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["'''\n","def create_dataset(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\n","\n","  name_list = sorted(os.listdir(path))\n","\n","  if data_per_epoch % amount_from_file != 0:\n","    print(\"error, data_per_epoch can't divide amount_from_file !!!!!\")\n","    sys.exit(1)\n","\n","  file_idx = data_per_epoch // amount_from_file\n","\n","  output_depth = []\n","  output_image = []\n","  length = len(name_list)\n","  for idx in range(file_idx):\n","    rand_int = random.randint(0, length - 1)\n","    tmp_file = name_list[rand_int]\n","    name = path + '/' + tmp_file\n","    checkpoint = torch.load(name)\n","    depth_list = checkpoint['depth_list']\n","    image_list = checkpoint['image_list']\n","    length_inside = len(depth_list)\n","    random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","\n","    for jdx in range(amount_from_file):\n","\n","      output_depth.append(depth_list[random_number[jdx]])\n","      output_image.append(image_list[random_number[jdx]])\n","  return output_image, output_depth\n","\n","'''"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"7a20iYQxBFvk","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":72,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"f95c10da-7060-462c-9223-aff9f215ea2c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef create_dataset_for_debug(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\\n  name_list = sorted(os.listdir(path))\\n  file_idx = data_per_epoch // amount_from_file\\n  output_depth_path = []\\n  output_image_path = []\\n  output_depth = []\\n  output_image = []\\n  sym = 0\\n  if data_per_epoch % amount_from_file  != 0:\\n    file_idx = file_idx + 1\\n    sym = 1\\n  length = len(name_list)\\n  for idx in range(file_idx):\\n    rand_int = random.randint(0, length - 1)\\n    tmp_file = name_list[rand_int]\\n    name = path + '/' + tmp_file\\n    checkpoint = torch.load(name)\\n    depth_list = checkpoint['depth_list']\\n    image_list = checkpoint['image_list']\\n    length_inside = len(depth_list)\\n    if sym == 1:\\n      if idx == file_idx - 1:\\n\\n        random_number = torch.randint(0, length_inside, ((data_per_epoch % amount_from_file), ))\\n      else:\\n\\n        random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n\\n\\n    else:\\n      random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n    for jdx in range(amount_from_file):\\n\\n      output_depth.append(depth_list[random_number[jdx]])\\n      output_image.append(image_list[random_number[jdx]])\\n  return output_image, output_depth\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["'''\n","def create_dataset_for_debug(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\n","  name_list = sorted(os.listdir(path))\n","  file_idx = data_per_epoch // amount_from_file\n","  output_depth_path = []\n","  output_image_path = []\n","  output_depth = []\n","  output_image = []\n","  sym = 0\n","  if data_per_epoch % amount_from_file  != 0:\n","    file_idx = file_idx + 1\n","    sym = 1\n","  length = len(name_list)\n","  for idx in range(file_idx):\n","    rand_int = random.randint(0, length - 1)\n","    tmp_file = name_list[rand_int]\n","    name = path + '/' + tmp_file\n","    checkpoint = torch.load(name)\n","    depth_list = checkpoint['depth_list']\n","    image_list = checkpoint['image_list']\n","    length_inside = len(depth_list)\n","    if sym == 1:\n","      if idx == file_idx - 1:\n","\n","        random_number = torch.randint(0, length_inside, ((data_per_epoch % amount_from_file), ))\n","      else:\n","\n","        random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","\n","\n","    else:\n","      random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","    for jdx in range(amount_from_file):\n","\n","      output_depth.append(depth_list[random_number[jdx]])\n","      output_image.append(image_list[random_number[jdx]])\n","  return output_image, output_depth\n","\n","'''"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"MxKsLDplcOiV","executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":71,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def create_dataset_large_epoch(random_list, now, data_path, name_path): #data_per_epoch / amount_from_file要是整數\n","\n","  data_list = sorted(os.listdir(data_path))\n","  name_list = sorted(os.listdir(name_path))\n","\n","  # file_idx = data_per_epoch // amount_from_file\n","  output_depth_path = []\n","  output_image_path = []\n","  output_depth = []\n","  output_image = []\n","\n","  now_number = random_list[now]\n","  data_path = data_path + '/' + data_list[now_number]\n","  name_path = name_path + '/' + name_list[now_number]\n","  data_checkpoint = torch.load(data_path)\n","  name_checkpoint = torch.load(name_path)\n","  output_image_path = name_checkpoint['image_name']\n","  output_depth_path = name_checkpoint['depth_name']\n","  output_depth = data_checkpoint['depth_list']\n","  output_image = data_checkpoint['image_list']\n","  return output_image_path, output_depth_path, output_depth, output_image"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"LL_iQzCkcRfA","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":71,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"089454f1-f86c-48b0-fc3f-4a86fac36f77"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\\nname_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\nrandom_list = []\\n\\ncheck_length = len(sorted(os.listdir(data_path)))\\nfor idx in range(1, check_length + 1):\\n  random_list.append(idx)\\nrandom.shuffle(random_list)\\nnow = 2 # 實際上now也要從0一路走到最大值\\n\\n\\noutput_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}],"source":["'''\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","random_list = []\n","\n","check_length = len(sorted(os.listdir(data_path)))\n","for idx in range(1, check_length + 1):\n","  random_list.append(idx)\n","random.shuffle(random_list)\n","now = 2 # 實際上now也要從0一路走到最大值\n","\n","\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","'''"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"cl4Zkgx8cV7_","executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":70,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img, depth):\n","        self.img = img\n","        self.depth = depth\n","\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","        return sample\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"mcupty-acbv2","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":69,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"d0843198-35a9-4016-b80c-9e34be8f29df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ncustom_dataset = CustomDataset(output_image, output_depth)\\ntrain_size = int(0.9 * len(custom_dataset))\\nval_size = len(custom_dataset) - train_size\\ntrain_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\\n\\nbatch_size = 128\\n\\n\\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nvalidloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}],"source":["'''\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(0.9 * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","\n","batch_size = 128\n","\n","\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","'''"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"t0u7iwvychi6","executionInfo":{"status":"ok","timestamp":1710425169088,"user_tz":-480,"elapsed":68,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def image_loader_to_tensor(tensor):\n","  tensor = tensor.to(torch.float32)\n","  tensor = tensor / 255.0\n","  tensor = tensor * 2.0\n","  tensor = tensor - 1.0\n","  tensor = tensor.permute(0, 3, 1, 2)\n","  return tensor"]},{"cell_type":"code","source":["def depth_loader_to_tensor(tensor, DEPTH_MEAN, DEPTH_STD):\n","\n","\n","\n","  nonzero_mask = tensor != 0\n","  zero_mask = tensor == 0\n","  # mean = tensor[nonzero_mask].mean()\n","  # std = tensor[nonzero_mask].std()\n","  result = (tensor[nonzero_mask] - DEPTH_MEAN) / DEPTH_STD\n","  tensor[nonzero_mask] = result\n","  tensor[zero_mask] = -1\n","\n","\n","\n","  return tensor"],"metadata":{"id":"LGHsElDov5jL","executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":69,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["'''\n","for batch in trainloader:\n","  print(batch['img'].shape)\n","  print(batch['img'].dtype)\n","  print(torch.max(batch['img']))\n","  print(torch.min(batch['img']))\n","  print(torch.mean(batch['img'].to(torch.float32)))\n","  print(batch['depth'].shape)\n","  print(batch['depth'].dtype)\n","  print(torch.max(batch['depth']))\n","  print(torch.min(batch['depth']))\n","  print(torch.mean(batch['depth']))\n","  break\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"wu-O30R51a_p","executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":69,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"6c98e73e-9cab-474f-9c4f-7cb50cc82533"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor batch in trainloader:\\n  print(batch['img'].shape)\\n  print(batch['img'].dtype)\\n  print(torch.max(batch['img']))\\n  print(torch.min(batch['img']))\\n  print(torch.mean(batch['img'].to(torch.float32)))\\n  print(batch['depth'].shape)\\n  print(batch['depth'].dtype)\\n  print(torch.max(batch['depth']))\\n  print(torch.min(batch['depth']))\\n  print(torch.mean(batch['depth']))\\n  break\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# for batch in trainloader:\n","\n","#   batch['img'] = image_loader_to_tensor(batch['img'])\n","#   batch['depth'] = depth_loader_to_tensor(batch['depth'], DEPTH_MEAN, DEPTH_STD)\n","#   break\n","# nonzero = batch['depth'] != -1\n","# print(batch['img'].shape)\n","# print(batch['img'].dtype)\n","# print(torch.max(batch['img']))\n","# print(torch.min(batch['img']))\n","# print(torch.mean(batch['img']))\n","# print(batch['depth'].shape)\n","# print(batch['depth'].dtype)\n","# print(torch.max(batch['depth'][nonzero]))\n","# print(torch.min(batch['depth'][nonzero]))\n","# print(torch.mean(batch['depth'][nonzero]))\n","# print(torch.std(batch['depth'][nonzero]))"],"metadata":{"id":"BynQ0xH2yIDq","executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":68,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kyRWoXkp2zys","executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"id":"mC3-WzjWHV1P","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"57af059c-05f2-4135-ce00-953fe1c0bfbd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef image_tensor_to_numpy(tensor):\\n\\n    tensor = tensor.permute(0, 2, 3, 1)\\n    output = tensor.numpy()\\n    output = output + 1.0\\n    output = output / 2.0\\n    output = output * 255.0\\n    output = output.astype(np.uint8)\\n\\n    return output\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["'''\n","def image_tensor_to_numpy(tensor):\n","\n","    tensor = tensor.permute(0, 2, 3, 1)\n","    output = tensor.numpy()\n","    output = output + 1.0\n","    output = output / 2.0\n","    output = output * 255.0\n","    output = output.astype(np.uint8)\n","\n","    return output\n","'''"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"p6tNy_NqY22R","outputId":"6b0d25cb-86c6-44c7-999e-35d73ba1b92c","executionInfo":{"status":"ok","timestamp":1710425169089,"user_tz":-480,"elapsed":66,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n\\n    mini = torch.min(tensor[tensor != 0])\\n    tensor = (tensor - mini) / (tensor.max() - mini)\\n    tensor = torch.where(tensor < 0, -1, tensor)\\n\\n\\n\\n\\n\\n\\n    return tensor\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["# this is min max normalize\n","'''\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","\n","    mini = torch.min(tensor[tensor != 0])\n","    tensor = (tensor - mini) / (tensor.max() - mini)\n","    tensor = torch.where(tensor < 0, -1, tensor)\n","\n","\n","\n","\n","\n","\n","    return tensor\n","'''"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"TepPXyX-YZ9P","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1710425169090,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"7d548e48-4f5d-4374-ad9e-0e8847f0f727"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# this is z-score normalization\\n\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n    nonzero_mask = tensor != 0\\n    zero_mask = tensor == 0\\n    mean = tensor[nonzero_mask].mean()\\n    std = tensor[nonzero_mask].std()\\n    result = (tensor[nonzero_mask] - mean) / std\\n    tensor[nonzero_mask] = result\\n    tensor[zero_mask] = -1\\n\\n\\n\\n    return tensor, mean, std\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}],"source":["'''\n","# this is z-score normalization\n","\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","    nonzero_mask = tensor != 0\n","    zero_mask = tensor == 0\n","    mean = tensor[nonzero_mask].mean()\n","    std = tensor[nonzero_mask].std()\n","    result = (tensor[nonzero_mask] - mean) / std\n","    tensor[nonzero_mask] = result\n","    tensor[zero_mask] = -1\n","\n","\n","\n","    return tensor, mean, std\n","'''"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"EbfNMqSeP5pZ","executionInfo":{"status":"ok","timestamp":1710425169091,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","    def sigmoid(x):\n","        return 1 / (np.exp(-x) + 1)\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"AM6MyJPrGPUT","executionInfo":{"status":"ok","timestamp":1710425169091,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"kiqjRlDrHpok","executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def get_timestep_embedding(timesteps, embedding_dim):\n","\n","    assert len(timesteps.shape) == 1\n","\n","    half_dim = embedding_dim // 2\n","    emb = math.log(10000) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","    emb = emb.to(device=timesteps.device)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:  # zero pad\n","        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n","    return emb"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"JMYCE8V6JK22","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":67,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"73706fd4-4f95-4342-d4d9-ed50d57042f9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef Normalize(in_channels):\\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}],"source":["'''\n","def Normalize(in_channels):\n","    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n","'''"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"VwmQi1zV77Q8","executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":66,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["# for the batch normalization\n","def Normalize(input, channels, momentum = 0.1, epsilon = 1e-5):\n","  bn = nn.BatchNorm2d(channels, momentum = momentum, eps = epsilon)\n","  return bn(input)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"cr4mMbzN6tXr","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":66,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"d886ddaa-afc3-41ab-9407-fd6c5b54f32e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef nonlinearity(x):\\n\\n    return x*torch.sigmoid(x)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["'''\n","def nonlinearity(x):\n","\n","    return x*torch.sigmoid(x)\n","'''"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"jqFsXjkX_8D2","executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def nonlinearity(input, negative_slope = 0.1, inplace = False):\n","  return torch.nn.functional.leaky_relu(input_tensor, negative_slope = negative_slope, inplace = inplace)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"_WRkUz1I6-JB","executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"V_be0bSq7A88","executionInfo":{"status":"ok","timestamp":1710425169092,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"ILHJ1zbU8LYb","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 =nn.BatchNorm2d(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = nn.BatchNorm2d(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","    def forward(self, x, temb):\n","        h = x\n","        h = self.norm1(h)    # normalize\n","\n","        h = self.Lrelu(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","\n","        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        h = self.norm2(h)\n","        h = self.Lrelu(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Zy0-VDEM8uIl","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = nn.BatchNorm2d(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"2qQRtwwWkXTJ","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"QpRkaCZE2FM6","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class UpsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n","                                        int(out_channels),\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"H6dIsb3rx6CP","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":65,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        self.bn0 = nn.BatchNorm2d(self.resolutions[0])\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","        self.norm_seq = nn.ModuleList()\n","        for idx in range(len(self.resolutions) - 1):\n","          self.norm_seq.append(nn.BatchNorm2d(self.target_channel))\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = x\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.pool(self.Lrelu(self.ConvList[idx](h)))\n","          else:\n","            h = self.Lrelu(self.ConvList[idx](temp))\n","\n","          temp = h\n","          h = self.Lrelu(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        count = 0\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","            hold = self.convOut(hold + self.norm_seq[count](FPN_list[idx]))\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","          else:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","            hold = hold + self.norm_seq[count](FPN_list[idx])\n","            count = count + 1\n","            hold = self.Upsampple[idx - 1](hold)\n","\n","        return hold\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"esdKjIMr9wcF","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.relu(self.bn(self.conv(depth)))"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"icPtSBzxAKCO","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"Fz0EX77czh3m","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        for idx in range(phase2):\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","\n","        for idx in range(len(self.phase1_model)):\n","          h = self.phase1_model[idx](h)\n","\n","        for idx in range(len(self.phase2_model)):\n","          h = self.phase2_model[idx](h)\n","\n","        return h\n"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"N2Vzf5ZRC8wM","executionInfo":{"status":"ok","timestamp":1710425169093,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class depth_decode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        self.ch = config['model']['ch']\n","        # self.ch = 128\n","        self.resolution = config['data']['image_size']\n","        # self.resolution = 256\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        # self.targeted_size = 64\n","        count = 0\n","        tmp = self.targeted_size\n","        while True:\n","          if tmp == self.resolution:\n","            break\n","          count = count + 1\n","          tmp = tmp * 2\n","\n","        self.decode = nn.ModuleList()\n","        in_cha = self.ch\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        for idx in range(count):\n","          out_cha = in_cha / 4\n","          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n","          in_cha = out_cha\n","        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n","                                        1,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","\n","\n","\n","\n","\n","\n","\n","    def forward(self, pred):\n","        for idx in range(len(self.decode)):\n","          pred = self.decode[idx](pred)\n","        pred = self.final_conv(pred)\n","        pred = pred.squeeze(1)\n","\n","\n","\n","\n","\n","        return pred"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"xq591w1qfVBe","executionInfo":{"status":"ok","timestamp":1710425169094,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["def get_index_from_list(values, t, x_shape):\n","    batch_size = t.shape[0]\n","    \"\"\"\n","    pick the values from vals\n","    according to the indices stored in `t`\n","    \"\"\"\n","    result = values.gather(-1, t.cpu())\n","    \"\"\"\n","    if\n","    x_shape = (5, 3, 64, 64)\n","        -> len(x_shape) = 4\n","        -> len(x_shape) - 1 = 3\n","\n","    and thus we reshape `out` to dims\n","    (batch_size, 1, 1, 1)\n","\n","    \"\"\"\n","    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"AeTTDyc0fZOI","executionInfo":{"status":"ok","timestamp":1710425169094,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"BW2SNSqY0qys","executionInfo":{"status":"ok","timestamp":1710425169094,"user_tz":-480,"elapsed":64,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","        depth_enc_channels = config['model']['depth_enc_channels']\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","# nonlinear\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = nn.BatchNorm2d(block_in)\n","\n","        self.depth_decode = depth_decode(config)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","    def forward(self, image, depth, t):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = self.Lrelu(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","\n","        img_enc = self.fpn(image)\n","        depth = self.depth_encode(depth)\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","\n","        noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","        noisy_map = noisy_map.to(torch.float32)\n","        noise = noise.to(torch.float32)\n","\n","\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","        # return backbone_input\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        # hs = [self.conv_in(image)]\n","\n","        hs = [self.conv_in(backbone_input)]\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = self.Lrelu(h)\n","        h = self.depth_decode(h)\n","        return h"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"ParlMz0UnMrq","executionInfo":{"status":"ok","timestamp":1710425170053,"user_tz":-480,"elapsed":1022,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["model = Model(config)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"PtMRQmVtaAUR","outputId":"750169e8-962c-44ca-b52c-d897098e359c","executionInfo":{"status":"ok","timestamp":1710425170053,"user_tz":-480,"elapsed":20,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# analysis for model weights\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(113)\\ncheckpoint = torch.load(path, map_location = device)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}],"source":["'''\n","# analysis for model weights\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(113)\n","checkpoint = torch.load(path, map_location = device)\n","'''"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QVuuuiQoa9v0","outputId":"380b405f-e6f1-4c42-b989-6fd28b5f6d91","executionInfo":{"status":"ok","timestamp":1710425170053,"user_tz":-480,"elapsed":18,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# analysis for model weights\\nweights = checkpoint['model_state_dict']\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}],"source":["'''\n","# analysis for model weights\n","weights = checkpoint['model_state_dict']\n","'''\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"oXnDKCSAbSfl","outputId":"84874d50-86be-4734-afab-67a351975757","executionInfo":{"status":"ok","timestamp":1710425170053,"user_tz":-480,"elapsed":15,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# analysis for model weights\\nprint(\"number of keys : {}\".format(len(weights)))\\nfor key, value in weights.items():\\n    print(key, \"\\t\", value.shape, \"min = {}\".format(torch.min(value)))\\n    print(\"------------\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["'''\n","# analysis for model weights\n","print(\"number of keys : {}\".format(len(weights)))\n","for key, value in weights.items():\n","    print(key, \"\\t\", value.shape, \"min = {}\".format(torch.min(value)))\n","    print(\"------------\")\n","'''"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"8VKW8xA20JkQ","executionInfo":{"status":"ok","timestamp":1710425170054,"user_tz":-480,"elapsed":9,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["# BATCH_SIZE = 256\n","\n","NO_LARGE_EPOCHS = 10\n","save_frequency = 5\n","LR = 0.001\n","VERBOSE = False\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","batch_size = 128\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"]},{"cell_type":"code","source":["# 還有寫繼續train在大epoch還有哪些部分沒train完畢\n","# first epoch\n","epoch = 0\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","for large_epoch in range(1, NO_LARGE_EPOCHS + 1):\n","\n","  random_list = []\n","\n","  for idx in range(1, check_length + 1):\n","    random_list.append(idx)\n","\n","  random.shuffle(random_list)\n","\n","  for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","    epoch += 1\n","    output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","    custom_dataset = CustomDataset(output_image, output_depth)\n","    train_size = int(0.9 * len(custom_dataset))\n","    val_size = len(custom_dataset) - train_size\n","    train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # for name, param in model.named_parameters():\n","        #   if name not in epoch_gradient:\n","        #     epoch_gradient[name] = param.grad.clone()\n","        #   else:\n","        #     epoch_gradient[name] += param.grad\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","      checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","        'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'now' : now\n","        # 'gradients' : epoch_gradient\n","      }\n","\n","      torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","      source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","      destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","      # save them to the google drive\n","      shutil.copy(source_path, destination_path)\n","\n","    #---計算時間---vvv\n","    end_time = time.time()\n","    exe_time = end_time - start_time\n","    hours, remainder = divmod(exe_time, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","    #---計算時間---^^^\n","\n","    #-----以下是存loss的---vvv\n","    checkpoint = {\n","      'large_epoch' : large_epoch,\n","      'epoch': epoch,\n","      'valid_loss' : np.mean(mean_epoch_loss_val),\n","      'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","      'time' : exe_time\n","    }\n","\n","    torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","    source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","    destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","    # save them to the google drive\n","    shutil.copy(source_path, destination_path)\n","    #-----以下是存loss的---^^^\n","\n","    print('---')\n","    print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","    print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JXPioJu24vDX","executionInfo":{"status":"error","timestamp":1710425218189,"user_tz":-480,"elapsed":48144,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"d83a3b22-4a6c-4370-b057-b9cd3e667c3f"},"execution_count":59,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 3, 3, 3], expected input[128, 256, 257, 4] to have 3 channels, but got 256 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-1c2671283e77>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtarget_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpred_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-f8ed6594d94b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, depth, t)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mimg_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-f73d47aff722>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-1bdd2768a607>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 此動作相當於在每個圖片的channel的右邊下面pad 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[128, 256, 257, 4] to have 3 channels, but got 256 channels instead"]}]},{"cell_type":"code","source":["large_epoch =\n","epoch =\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save1/weight_{}_{}.pth'.format(large_epoch, epoch)\n","data_path = ''\n","check_length = len(sorted(os.listdir(data_path)))\n","checkpoint = torch.load(load_path)\n","if (checkpoint['now'].item() == check_length - 1):\n","  large_epoch_start = large_epoch + 1\n","  del large_epoch\n","\n","  for large_epoch in range(large_epoch_start, NO_LARGE_EPOCHS + 1):\n","\n","  random_list = []\n","\n","  for idx in range(1, check_length + 1):\n","    random_list.append(idx)\n","\n","  random.shuffle(random_list)\n","\n","  for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","    epoch += 1\n","    output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","    custom_dataset = CustomDataset(output_image, output_depth)\n","    train_size = int(0.9 * len(custom_dataset))\n","    val_size = len(custom_dataset) - train_size\n","    train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # for name, param in model.named_parameters():\n","        #   if name not in epoch_gradient:\n","        #     epoch_gradient[name] = param.grad.clone()\n","        #   else:\n","        #     epoch_gradient[name] += param.grad\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","      checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","        'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'now' : now,\n","        'random_list' : random_list\n","        # 'gradients' : epoch_gradient\n","      }\n","\n","      torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","      source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","      destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","      # save them to the google drive\n","      shutil.copy(source_path, destination_path)\n","\n","    #---計算時間---vvv\n","    end_time = time.time()\n","    exe_time = end_time - start_time\n","    hours, remainder = divmod(exe_time, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","    #---計算時間---^^^\n","\n","    #-----以下是存loss的---vvv\n","    checkpoint = {\n","      'large_epoch' : large_epoch,\n","      'epoch': epoch,\n","      'valid_loss' : np.mean(mean_epoch_loss_val),\n","      'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","      'time' : exe_time\n","    }\n","\n","    torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","    source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","    destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","    # save them to the google drive\n","    shutil.copy(source_path, destination_path)\n","    #-----以下是存loss的---^^^\n","\n","    print('---')\n","    print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","    print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","else:\n","  sym = 0\n","  if sym == 0:\n","    sym = 1\n","    now_start = checkpoint['now'].item() + 1\n","    random_list = checkpoint['random_list']\n","  else:\n","    large_epoch += 1\n","    now_start = 0\n","    for idx in range(1, check_length + 1):\n","      random_list.append(idx)\n","\n","    random.shuffle(random_list)\n","\n","\n","  for now in range(now_start, check_length):\n","    epoch += 1\n","    output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","    custom_dataset = CustomDataset(output_image, output_depth)\n","    train_size = int(0.9 * len(custom_dataset))\n","    val_size = len(custom_dataset) - train_size\n","    train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # for name, param in model.named_parameters():\n","        #   if name not in epoch_gradient:\n","        #     epoch_gradient[name] = param.grad.clone()\n","        #   else:\n","        #     epoch_gradient[name] += param.grad\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","      checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","        'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'now' : now,\n","        'random_list' : random_list\n","        # 'gradients' : epoch_gradient\n","      }\n","\n","      torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","      source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","      destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","      # save them to the google drive\n","      shutil.copy(source_path, destination_path)\n","\n","    #---計算時間---vvv\n","    end_time = time.time()\n","    exe_time = end_time - start_time\n","    hours, remainder = divmod(exe_time, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","    #---計算時間---^^^\n","\n","    #-----以下是存loss的---vvv\n","    checkpoint = {\n","      'large_epoch' : large_epoch,\n","      'epoch': epoch,\n","      'valid_loss' : np.mean(mean_epoch_loss_val),\n","      'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","      'time' : exe_time\n","    }\n","\n","    torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","    source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","    destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","    # save them to the google drive\n","    shutil.copy(source_path, destination_path)\n","    #-----以下是存loss的---^^^\n","\n","    print('---')\n","    print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","    print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","\n"],"metadata":{"id":"8pE2-xpLDCXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = torch.tensor([1]).item()\n","test == 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCo3iTI1D3WA","executionInfo":{"status":"ok","timestamp":1710421463106,"user_tz":-480,"elapsed":284,"user":{"displayName":"Archer914","userId":"03017676274708570535"}},"outputId":"34fd606e-6dca-4b78-e5d1-50c79e7a0ad5"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["# first epoch\n","'''\n","\n","\n","\n","for epoch in range(1, NO_EPOCHS + 1, 1):\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","'''"],"metadata":{"id":"s7nsCtRP6Vu9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZDEnqO21HxI"},"outputs":[],"source":["\n","\n","\n","# train_size = int(0.9 * len(custom_dataset))\n","# val_size = len(custom_dataset) - train_size\n","\n","\n","# train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","\n","# # 使用DataLoader加载数据集\n","# batch_size = 32\n","# trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","# validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvNvuYEqONOK"},"outputs":[],"source":["# print(input_img.shape)\n","# print(target_depth.shape)\n","# print(t.shape)\n","# count = 0\n","# for batch in trainloader:\n","#   count = count + 1\n","# print(count)\n","# print(32 * 31)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"EkbS3lIMvitZ","outputId":"186fcd65-ec61-411b-e0c7-da6f6fd2a530"},"outputs":[{"ename":"IndentationError","evalue":"unindent does not match any outer indentation level (<tokenize>, line 94)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    if epoch % PRINT_FREQUENCY == 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"]}],"source":["def training(start_epoch, steps, load_path, model, PRINT_FREQUENCY,  optimizer, save_frequency, trainloader, validloader, datasets_path, batch_size, config, device, weights_save_path, loss_save_path, gradient_save_path = None, save_gradient = False): # load path is where the already existed weights save\n","  initial = start_epoch\n","\n","  NO_EPOCHS = steps # 要多做幾個epochs\n","  # load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","  checkpoint = torch.load(load_path)\n","\n","  start = checkpoint['epoch'] + 1\n","  # model_state_dict = checkpoint['model_state_dict']\n","\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  for epoch in range(start , start + NO_EPOCHS + 1):\n","      start_time = time.time()\n","      epoch_gradient = {}\n","      mean_epoch_loss = []\n","      mean_epoch_loss_val = []\n","      for batch in trainloader:\n","          t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","          input_img = batch['img'].to(torch.float32).to(device)\n","          target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","          pred_depth = model(input_img, target_depth, t)\n","\n","          optimizer.zero_grad()\n","          loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","          mean_epoch_loss.append(loss.item())\n","          loss.backward()\n","\n","          if save_gradient :\n","            for name, param in model.named_parameters():\n","              if name not in epoch_gradient:\n","                epoch_gradient[name] = param.grad.clone()\n","              else:\n","                epoch_gradient[name] += param.grad\n","\n","\n","          optimizer.step()\n","\n","      with torch.inference_mode():\n","        for batch in validloader:\n","          t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","          input_img = batch['img'].to(torch.float32).to(device)\n","          target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","          pred_depth = model(input_img, target_depth, t)\n","\n","          val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","          mean_epoch_loss_val.append(val_loss.item())\n","\n","      if epoch % save_frequency == 0 or epoch == start + NO_EPOCHS:\n","          checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","            'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","          }\n","\n","          torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","          source_path = 'weight_{}.pth'.format(epoch)\n","          destination_path = weights_save_path\n","\n","\n","          # save them to the google drive\n","          shutil.copy(source_path, destination_path)\n","          #---計算時間---vvv\n","          end_time = time.time()\n","          exe_time = end_time - start_time\n","          hours, remainder = divmod(execution_time, 3600)\n","          minutes, seconds = divmod(remainder, 60)\n","          #---計算時間---^^^\n","          #-----以下是存loss的---vvv\n","          checkpoint = {\n","            'epoch': epoch,\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","            'time' : exe_time\n","          }\n","\n","          torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","          source_path = 'loss_{}.pth'.format(epoch)\n","          destination_path = loss_save_path\n","          #-----存gradient---vvv\n","          if save_gradient:\n","            checkpoint = {\n","            'gradients' : epoch_gradient\n","          }\n","          torch.save(checkpoint, 'gradient_{}.pth'.format(epoch))\n","          source_path = 'gradient_{}.pth'.format(epoch)\n","          destination_path = gradient_save_path\n","          shutil.copy(source_path, destination_path)\n","\n","          #-----存gradient---^^^\n","          # save them to the google drive\n","          shutil.copy(source_path, destination_path)\n","          #-----以下是存loss的---^^^\n","\n","        if epoch % PRINT_FREQUENCY == 0:\n","          print('---')\n","          print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","          print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"E67RH3azzzGn","outputId":"f055dba6-cdc1-4914-8deb-c8f9b3ee6aa9"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport shutil\\nfrom google.colab import files\\n\\nfor epoch in range(1, NO_EPOCHS + 1, 1):\\n    mean_epoch_loss = []\\n    mean_epoch_loss_val = []\\n    epoch_gradient = {}\\n    for batch in trainloader:\\n        t = torch.randint(0, config[\\'diffusion\\'][\\'num_diffusion_timesteps\\'], (batch_size,)).long().to(device)\\n\\n        input_img = batch[\\'img\\'].to(torch.float32).to(device)\\n        target_depth = batch[\\'depth\\'].to(torch.float32).to(device)\\n\\n        pred_depth = model(input_img, target_depth, t)\\n\\n        optimizer.zero_grad()\\n        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\\n        mean_epoch_loss.append(loss.item())\\n        loss.backward()\\n\\n\\n        for name, param in model.named_parameters():\\n          if name not in epoch_gradient:\\n            epoch_gradient[name] = param.grad.clone()\\n          else:\\n            epoch_gradient[name] += param.grad\\n        optimizer.step()\\n\\n    with torch.inference_mode():\\n      for batch in validloader:\\n        t = torch.randint(0, config[\\'diffusion\\'][\\'num_diffusion_timesteps\\'], (batch_size,)).long().to(device)\\n        input_img = batch[\\'img\\'].to(torch.float32).to(device)\\n        target_depth = batch[\\'depth\\'].to(torch.float32).to(device)\\n\\n        pred_depth = model(input_img, target_depth, t)\\n\\n        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\\n        mean_epoch_loss_val.append(val_loss.item())\\n\\n    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\\n        checkpoint = {\\n          \\'epoch\\': epoch,\\n          \\'model_state_dict\\': model.state_dict(), # model.state_dict()是存下param的的值和形狀\\n          \\'optimizer_state_dict\\': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\\n          \\'valid_loss\\' : np.mean(mean_epoch_loss_val),\\n          \\'loss\\' : np.mean(mean_epoch_loss), # 記得不能存tensor\\n          \\'gradients\\' : epoch_gradient\\n        }\\n\\n        torch.save(checkpoint, \\'weight_{}.pth\\'.format(epoch))\\n        source_path = \\'weight_{}.pth\\'.format(epoch)\\n        destination_path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save\\'\\n\\n\\n        # save them to the google drive\\n        shutil.copy(source_path, destination_path)\\n\\n\\n\\n        #-----以下是存loss的---vvv\\n        checkpoint = {\\n          \\'epoch\\': epoch,\\n          \\'valid_loss\\' : np.mean(mean_epoch_loss_val),\\n          \\'loss\\' : np.mean(mean_epoch_loss) # 記得不能存tensor\\n        }\\n\\n        torch.save(checkpoint, \\'loss_{}.pth\\'.format(epoch))\\n        source_path = \\'loss_{}.pth\\'.format(epoch)\\n        destination_path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save\\'\\n\\n\\n        # save them to the google drive\\n        shutil.copy(source_path, destination_path)\\n        #-----以下是存loss的---^^^\\n        print(\\'---\\')\\n        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\\n'"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# first epoch\n","'''\n","import shutil\n","\n","\n","for epoch in range(1, NO_EPOCHS + 1, 1):\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MkEE7uV2_mt"},"outputs":[],"source":["# import torch\n","# import shutil\n","# from google.colab import files\n","# initial = 16\n","# NO_EPOCHS = 200 # 要多做幾個epochs\n","# load_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","# checkpoint = torch.load(load_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hw122Fd47aPB","outputId":"d6d2e675-a103-43ea-d5b1-cc9ae9e79576"},"outputs":[{"name":"stdout","output_type":"stream","text":["---\n","Epoch: 91 | Train Loss 0.38567152300051283 | Val Loss 0.39286285638809204\n","---\n","Epoch: 92 | Train Loss 0.3859223084790366 | Val Loss 0.3921920657157898\n","---\n","Epoch: 93 | Train Loss 0.3854115701147488 | Val Loss 0.3909292121728261\n","---\n","Epoch: 94 | Train Loss 0.38506057752030237 | Val Loss 0.38761888941129047\n","---\n","Epoch: 95 | Train Loss 0.3845937837447439 | Val Loss 0.3917753994464874\n","---\n","Epoch: 96 | Train Loss 0.38600521215370726 | Val Loss 0.3998758594195048\n","---\n","Epoch: 97 | Train Loss 0.38719060378415243 | Val Loss 0.39039607842763263\n","---\n","Epoch: 98 | Train Loss 0.38408188096114565 | Val Loss 0.3900303343931834\n","---\n","Epoch: 99 | Train Loss 0.3831964518342699 | Val Loss 0.38861361145973206\n","---\n","Epoch: 100 | Train Loss 0.3895272814801761 | Val Loss 0.3920138378938039\n","---\n","Epoch: 101 | Train Loss 0.39488488116434645 | Val Loss 0.3970567186673482\n","---\n","Epoch: 102 | Train Loss 0.6200624565993037 | Val Loss 0.6198289394378662\n","---\n","Epoch: 103 | Train Loss 0.600775699530329 | Val Loss 0.6032946904500326\n","---\n","Epoch: 104 | Train Loss 0.5959067557539258 | Val Loss 0.6032663385073344\n","---\n","Epoch: 105 | Train Loss 0.5949824367250715 | Val Loss 0.6023848652839661\n","---\n","Epoch: 106 | Train Loss 0.5950001137597221 | Val Loss 0.602178951104482\n","---\n","Epoch: 107 | Train Loss 0.5938941793782371 | Val Loss 0.602675219376882\n","---\n","Epoch: 108 | Train Loss 0.5921581664255687 | Val Loss 0.6111739873886108\n","---\n","Epoch: 109 | Train Loss 0.5923646709748677 | Val Loss 0.6003530224164327\n","---\n","Epoch: 110 | Train Loss 0.5899561579738345 | Val Loss 0.6024161775906881\n","---\n","Epoch: 111 | Train Loss 0.5886165691273553 | Val Loss 0.5966901183128357\n"]}],"source":["# continue training\n","initial = 90\n","NO_EPOCHS = 20 # 要多做幾個epochs\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","checkpoint = torch.load(load_path)\n","\n","start = checkpoint['epoch'] + 1\n","# model_state_dict = checkpoint['model_state_dict']\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","for epoch in range(start , start + NO_EPOCHS + 1):\n","    start_time = time.time()\n","    epoch_gradient = {}\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","\n","\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == start + NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save_gradient_dev'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save_gradient_dev'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"o8eritXvUSMF","outputId":"49dc148b-852d-4894-b686-07bd7dd4bd4f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport torch\\nimport os\\nfor idx in range(133):\\n  if idx == 45:\\n    continue\\n\\n  path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save/loss_{}.pth\\'.format(idx)\\n  check = torch.load(path)\\n  print(\"epoch : {}, loss = {}  | val_loss = {}\".format(check[\\'epoch\\'], check[\\'loss\\'], check[\\'valid_loss\\']))\\n  print(\\'----------\\')\\n\\n'"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","import torch\n","import os\n","for idx in range(133):\n","  if idx == 45:\n","    continue\n","\n","  path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save/loss_{}.pth'.format(idx)\n","  check = torch.load(path)\n","  print(\"epoch : {}, loss = {}  | val_loss = {}\".format(check['epoch'], check['loss'], check['valid_loss']))\n","  print('----------')\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H00KhF04so4U"},"outputs":[],"source":["'''\n","# continue training\n","# train for another account\n","# change the lr\n","\n","# continue training\n","# train for another account\n","# change the lr\n","\n","import shutil\n","from google.colab import files\n","initial = 100\n","NO_EPOCHS = 2000 # 要多做幾個epochs\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save_gradient_dev/weight_{}.pth'.format(initial) # 位置要改\n","checkpoint = torch.load(load_path)\n","\n","start = checkpoint['epoch'] + 1\n","# model_state_dict = checkpoint['model_state_dict']\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","for epoch in range(start , start + NO_EPOCHS + 1):\n","    for param_group in optimizer.param_groups:\n","      param_group['lr'] = 0.0001\n","    epoch_gradient = {}\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","\n","\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == start + NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dw8wH8EQs8B6"},"outputs":[],"source":["'''\n","for idx in range(91, 111):\n","  path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/loss_save_gradient_dev/loss_{}.pth'.format(idx)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Nl3NJwQtGkP"},"outputs":[],"source":["'''\n","# load gradient\n","epoch = 102\n","path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save_gradient_dev/weight_{}.pth'.format(epoch)\n","check = torch.load(path, map_location = 'cpu')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tdK8tgbtKw1"},"outputs":[],"source":["'''\n","gradients = check['gradients']\n","torch.set_printoptions(precision=10)\n","\n","path\n","\n","for name, grad in gradients.items():\n","  # if name == 'fpn.ConvList.0.conv.bias':\n","    maximum = torch.max(grad)\n","    minimum = torch.min(grad)\n","    mean_val = torch.mean(grad)\n","    print(name, \" : \", grad.shape, \" | min = \", minimum, \" | max = \", maximum, \" | mean = \", mean_val)\n","    print(\"-----------------------\")\n","    # break\n","\n","torch.min(grad)\n","minimum\n","\n","torch.set_printoptions(precision=6)\n","print(maximum)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1MjK8pF-GG7"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}