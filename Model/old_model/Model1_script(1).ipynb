{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TL6RdTKY4KNI","executionInfo":{"status":"ok","timestamp":1710764175943,"user_tz":-480,"elapsed":8624,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["# model 1 把所有normalize換成 batch normalize, 所有nonlinear 換成leaky relu, 在fpn最前面加入norm並且在所有相加的部分前加入norm\n","import torch\n","import cv2\n","import os\n","import numpy as np\n","import shutil\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","import random\n","from google.colab import files\n","import sys\n","import time\n","from torch.utils.data import random_split\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","source":["# check = {\"test\" : 2}\n","\n","# torch.save(checkpoint, 'test1.pth')\n","# source_path = 'test1.pth'\n","# destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","# shutil.copy(source_path, destination_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"4Mwhzqrgq8aa","outputId":"9a84923e-706f-40a2-a5f7-d33b6050c4c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle/test1.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# check2 = torch.load('/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle/test1.pth')\n","# check2['test']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vw_LcKtMrlQE","outputId":"69d4212a-7a85-4c1a-c819-36fb6af1950f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# ori_data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip_shuffle'\n","# ori_data_path = ori_data_path + '/' + 'dataset_1.pth'\n","# check = torch.load(ori_data_path)\n"],"metadata":{"id":"APGii29d9iif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(check['depth_name'][:20])\n","# print(check['depth_name'][-20:])"],"metadata":{"id":"3LStxbF29i-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # shuffle dataset\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle/dataset_1.pth_2'\n","\n","# new_file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle/dataset_1.pth_3'\n","# # os.rename(data_path, new_file_path)\n","# data_path = new_file_path\n","# os.remove(data_path)\n"],"metadata":{"id":"XyKOvYrzGIcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# shuffle dataset\n","start_time = time.time()\n","how_many_times = 1000\n","for count in range(how_many_times):\n","    ori_data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","    ori_name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip_shuffle'\n","\n","    ori_data_path_list = sorted(os.listdir(ori_data_path))\n","    ori_name_path_list = sorted(os.listdir(ori_name_path))\n","    ori_length = len(ori_data_path_list)\n","\n","    # path = os.path.join(ori_data_path, ori_data_path_list[0])\n","    random_number1 = random.randint(0, ori_length - 1)\n","    random_number2 = random_number1\n","    while(random_number2 == random_number1):\n","        random_number2 = random.randint(0, ori_length - 1)\n","\n","    shuffle_data_path1 = os.path.join(ori_data_path, ori_data_path_list[random_number1])\n","    shuffle_data_path2 = os.path.join(ori_data_path, ori_data_path_list[random_number2])\n","    shuffle_name_path1 = os.path.join(ori_name_path, ori_name_path_list[random_number1])\n","    shuffle_name_path2 = os.path.join(ori_name_path, ori_name_path_list[random_number2])\n","\n","    data_check1 = torch.load(shuffle_data_path1)\n","    data_check2 = torch.load(shuffle_data_path2)\n","    name_check1 = torch.load(shuffle_name_path1)\n","    name_check2 = torch.load(shuffle_name_path2)\n","\n","\n","    image1 = data_check1['image_list']\n","    depth1 = data_check1['depth_list']\n","    image_name1 = name_check1['image_name']\n","    depth_name1  = name_check1['depth_name']\n","    # print(image_name1[:5])\n","    image2 = data_check2['image_list']\n","    depth2 = data_check2['depth_list']\n","    image_name2 = name_check2['image_name']\n","    depth_name2 = name_check2['depth_name']\n","    start = 0\n","    end = 4999\n","    num_samples = 5000\n","    random_list1 = random.sample(range(start, end + 1), num_samples)\n","    random_list2 = random.sample(range(start, end + 1), num_samples)\n","\n","    image1_tmp = []\n","    for idx in random_list1:\n","        image1_tmp.append(image1[idx])\n","    image1 = image1_tmp\n","    del image1_tmp\n","\n","    depth1_tmp = []\n","    for idx in random_list1:\n","        depth1_tmp.append(depth1[idx])\n","    depth1 = depth1_tmp\n","    del depth1_tmp\n","\n","    image_name1_tmp = []\n","    for idx in random_list1:\n","        image_name1_tmp.append(image_name1[idx])\n","    image_name1 = image_name1_tmp\n","    del image_name1_tmp\n","\n","    depth_name1_tmp = []\n","    for idx in random_list1:\n","        depth_name1_tmp.append(depth_name1[idx])\n","    depth_name1 = depth_name1_tmp\n","    del depth_name1_tmp\n","\n","    image2_tmp = []\n","    for idx in random_list2:\n","        image2_tmp.append(image2[idx])\n","    image2 = image2_tmp\n","    image2_tmp = image2_tmp[:2500]\n","\n","    depth2_tmp = []\n","    for idx in random_list2:\n","        depth2_tmp.append(depth2[idx])\n","    depth2 = depth2_tmp\n","    depth2_tmp = depth2_tmp[:2500]\n","\n","    image_name2_tmp = []\n","    for idx in random_list2:\n","        image_name2_tmp.append(image_name2[idx])\n","    image_name2 = image_name2_tmp\n","    image_name2_tmp = image_name2_tmp[:2500]\n","\n","    depth_name2_tmp = []\n","    for idx in random_list2:\n","        depth_name2_tmp.append(depth_name2[idx])\n","    depth_name2 = depth_name2_tmp\n","    depth_name2_tmp = depth_name2_tmp[:2500]\n","\n","    image2[:2500] = image1[:2500]\n","    depth2[:2500] = depth1[:2500]\n","    depth_name2[:2500] = depth_name1[:2500]\n","    image_name2[:2500] = image_name1[:2500]\n","\n","    image1[:2500] = image2_tmp\n","    depth1[:2500] = depth2_tmp\n","    image_name1[:2500] = image_name2_tmp\n","    depth_name1[:2500] = depth_name2_tmp\n","\n","    print(len(image_name1))\n","    print(image_name1[-5:])\n","\n","\n","    # print(image_name1[:5])\n","\n","    data_check1 = {\n","                'depth_list': depth1,\n","                'image_list' : image1,\n","            }\n","    name_check1 = {\n","                'depth_name' : depth_name1,\n","                'image_name' : image_name1\n","            }\n","\n","    data_check2 = {\n","                'depth_list': depth2,\n","                'image_list' : image2,\n","            }\n","    name_check2 = {\n","                'depth_name' : depth_name2,\n","                'image_name' : image_name2\n","            }\n","\n","    # os.remove(shuffle_data_path1)\n","    # os.remove(shuffle_data_path2)\n","    # os.remove(shuffle_name_path1)\n","    # os.remove(shuffle_name_path2)\n","\n","    torch.save(data_check1, ori_data_path_list[random_number1])\n","    source_path = ori_data_path_list[random_number1]\n","    destination_path = ori_data_path\n","    shutil.copy(source_path, destination_path)\n","\n","\n","    torch.save(data_check2, ori_data_path_list[random_number2])\n","    source_path = ori_data_path_list[random_number2]\n","    destination_path = ori_data_path\n","    shutil.copy(source_path, destination_path)\n","\n","    torch.save(name_check1, ori_name_path_list[random_number1])\n","    source_path = ori_name_path_list[random_number1]\n","    destination_path = ori_name_path\n","    shutil.copy(source_path, destination_path)\n","\n","    torch.save(name_check2, ori_name_path_list[random_number2])\n","    source_path = ori_name_path_list[random_number2]\n","    destination_path = ori_name_path\n","    shutil.copy(source_path, destination_path)\n","\n","\n","\n","\n","    print(random_number1, random_number2)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(\"代码段执行时间:\", elapsed_time, \"秒\")\n","from google.colab import runtime\n","runtime.unassign()\n","'''"],"metadata":{"id":"UrrF0NKVHeeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfQ1zEzuzZL2"},"outputs":[],"source":["def deb(param, str):\n","  print(str + \" = {}\".format(param))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5T3gP7QULTMt"},"outputs":[],"source":["def load_config(file_path):\n","    with open(file_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config"]},{"cell_type":"code","source":["# path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/depth_analysis.pth' # 讀取depth的統計數字\n","path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/depth_analysis.pth'\n","\n","check = torch.load(path)\n","total_sum = check['total_sum']\n","DEPTH_NONZERO = check['total_nonzero']\n","DEPTH_MEAN = check['total_mean']\n","DEPTH_STD = check['total_std']\n","del check"],"metadata":{"id":"E9m-sHCRz5Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fedtFZjG6Ucb"},"outputs":[],"source":["def count_params(model):\n","  sum = 0\n","  for param in model.parameters():\n","    sum = sum + param.numel()\n","  return sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H-DIrmZLVw4"},"outputs":[],"source":["# file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/config.yml'\n","file_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config.yml'\n","config = load_config(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9i9pX5s6Gzx"},"outputs":[],"source":["target_size = (config['data']['image_size'], config['data']['image_size'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzwmWbrdbkyo","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"d5fc6f03-384d-4557-8c6c-874b1ec17d31"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef find_next_file(first, second):\\n  first_list = sorted(os.listdir(first))\\n  second_list = sorted(os.listdir(second))\\n  length = len(first_list)\\n  while True:\\n    rand_int = random.randint(0, length - 1)\\n    target = first_list[rand_int]\\n    if target in second_list:\\n      return first + '/' + target, second + '/' + target\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["'''\n","def find_next_file(first, second):\n","  first_list = sorted(os.listdir(first))\n","  second_list = sorted(os.listdir(second))\n","  length = len(first_list)\n","  while True:\n","    rand_int = random.randint(0, length - 1)\n","    target = first_list[rand_int]\n","    if target in second_list:\n","      return first + '/' + target, second + '/' + target\n","'''"]},{"cell_type":"code","source":["'''\n","def compute_depth_mean(path):\n","  file_list = sorted(os.listdir(path))\n","  total_sum = 0\n","\n","  total_nonzero = 0\n","  # count = 0\n","  for name in file_list:\n","    file_path = path + '/' + name\n","    check = torch.load(file_path)\n","\n","    target = torch.tensor(check['depth_list']).to(torch.float64)\n","    print(target.dtype)\n","    total_sum += torch.sum(target)\n","\n","    total_nonzero += torch.nonzero(target).size(0)\n","    # count += 1\n","    # if count == 2:\n","    #   break\n","  total_mean = total_sum / total_nonzero\n","  return total_sum, total_nonzero, total_mean\n","\n","\n","'''"],"metadata":{"id":"oK1GUGywv-7X","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"fda9ae59-61fd-496c-d7fd-43ec8f4a1e4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef compute_depth_mean(path):\\n  file_list = sorted(os.listdir(path))\\n  total_sum = 0\\n\\n  total_nonzero = 0\\n  # count = 0\\n  for name in file_list:\\n    file_path = path + '/' + name\\n    check = torch.load(file_path)\\n\\n    target = torch.tensor(check['depth_list']).to(torch.float64)\\n    print(target.dtype)\\n    total_sum += torch.sum(target)\\n\\n    total_nonzero += torch.nonzero(target).size(0)\\n    # count += 1\\n    # if count == 2:\\n    #   break\\n  total_mean = total_sum / total_nonzero\\n  return total_sum, total_nonzero, total_mean\\n\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["'''\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","total_sum, total_nonzero, total_mean = compute_depth_mean(path)\n","print(total_sum, total_nonzero, total_mean)\n","'''"],"metadata":{"id":"Wo_UMEDwy-uE","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"43eaed00-7dd8-4cb9-d47f-a0e3500caaa4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\\ntotal_sum, total_nonzero, total_mean = compute_depth_mean(path)\\nprint(total_sum, total_nonzero, total_mean)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["'''\n","def compute_depth_std(path, total_mean, total_nonzero):\n","  file_list = sorted(os.listdir(path))\n","\n","\n","  total_sum = 0\n","  # count = 0\n","  for name in file_list:\n","    file_path = path + '/' + name\n","    check = torch.load(file_path)\n","\n","    target = torch.tensor(check['depth_list']).to(torch.float64)\n","    non_zero_mask = target != 0\n","\n","    target = target[non_zero_mask]\n","    target = target - total_mean\n","    target = target ** 2\n","    target = target / (total_nonzero - 1)\n","    total_sum += torch.sum(target)\n","\n","\n","    # count += 1\n","    # if count == 2:\n","    #   break\n","\n","  return total_sum\n","'''\n","\n"],"metadata":{"id":"Yd05l7k_40OY","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"660cdc0e-9855-4c33-bf42-021a77b4fb28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef compute_depth_std(path, total_mean, total_nonzero):\\n  file_list = sorted(os.listdir(path))\\n\\n\\n  total_sum = 0\\n  # count = 0\\n  for name in file_list:\\n    file_path = path + '/' + name\\n    check = torch.load(file_path)\\n\\n    target = torch.tensor(check['depth_list']).to(torch.float64)\\n    non_zero_mask = target != 0\\n\\n    target = target[non_zero_mask]\\n    target = target - total_mean\\n    target = target ** 2\\n    target = target / (total_nonzero - 1)\\n    total_sum += torch.sum(target)\\n\\n\\n    # count += 1\\n    # if count == 2:\\n    #   break\\n\\n  return total_sum\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["'''\n","std_square = compute_depth_std(path, total_mean, total_nonzero)\n","print(std_square)\n","'''"],"metadata":{"id":"ffZtBw7m-VCw","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"81d4d0ba-4789-4573-e639-28ac9059711a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nstd_square = compute_depth_std(path, total_mean, total_nonzero)\\nprint(std_square)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["'''\n","checkpoint = {\n","          'total_sum': total_sum,\n","          'total_nonzero': total_nonzero, # model.state_dict()是存下param的的值和形狀\n","          'total_mean': total_mean, # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'total_std' : total_std\n","        }\n","\n","torch.save(checkpoint, 'depth_analysis2.pth')\n","source_path = 'depth_analysis2.pth'\n","destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model'\n","shutil.copy(source_path, destination_path)\n","'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"yA8gEQSWBoWl","outputId":"2ebd9f24-78db-4e75-958f-1ca082e23d91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ncheckpoint = {\\n          'total_sum': total_sum,\\n          'total_nonzero': total_nonzero, # model.state_dict()是存下param的的值和形狀\\n          'total_mean': total_mean, # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\\n          'total_std' : total_std\\n        }\\n\\ntorch.save(checkpoint, 'depth_analysis2.pth')\\nsource_path = 'depth_analysis2.pth'\\ndestination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model'\\nshutil.copy(source_path, destination_path)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNeKReI2rjYC","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"d80e8d17-fa95-4e09-dd0a-e76c736e39d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef find_next_file(first, second):\\n  first_list = sorted(os.listdir(first))\\n  second_list = sorted(os.listdir(second))\\n  length = len(first_list)\\n  while True:\\n    rand_int = random.randint(0, length - 1)\\n    target = first_list[rand_int]\\n    if target in second_list:\\n      return first + '/' + target, second + '/' + target\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["'''\n","def find_next_file(first, second):\n","  first_list = sorted(os.listdir(first))\n","  second_list = sorted(os.listdir(second))\n","  length = len(first_list)\n","  while True:\n","    rand_int = random.randint(0, length - 1)\n","    target = first_list[rand_int]\n","    if target in second_list:\n","      return first + '/' + target, second + '/' + target\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V91AbNDUKCF5","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"204a8531-9462-4f97-b54f-0e9411e68284"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef create_dataset_save(first_path, second_path, epoch_size = 5000): # first is depth, second is image\\n  depth_list = []\\n  image_list = []\\n  depth_name_list = []\\n  image_name_list = []\\n  count = 0\\n  file_count = 8  #改\\n\\n  first_path_list1 = sorted(os.listdir(first_path))\\n  second_path_list1 = sorted(os.listdir(second_path))\\n  idx1_start = 109   #改\\n\\n  for idx1 in range(idx1_start, len(first_path_list1)):\\n\\n    first_seq1 = first_path_list1[idx1]\\n    # if first_seq1 == '2011_09_28_drive_0090_sync': #改\\n    #   continue\\n    first_path2 = first_path + '/' + first_seq1 + '/proj_depth/groundtruth'\\n    second_path2 = second_path + '/' + first_seq1\\n    first_path_list2 = sorted(os.listdir(first_path2))\\n\\n    if idx1 == idx1_start:\\n      idx2_start = 1  # 改\\n    else:\\n      idx2_start = 0\\n    for idx2 in range(idx2_start, len(first_path_list2)): # image02, image03\\n      first_seq2 = first_path_list2[idx2]\\n      first_path3 = first_path2 + '/' + first_seq2\\n      second_path3 = second_path2 + '/' + first_seq2 + '/data'\\n      first_path_list3 = sorted(os.listdir(first_path3))\\n      if idx1 == idx1_start and idx2 == idx2_start:\\n        idx3_start = 58 #改成+1\\n      else:\\n        idx3_start = 0\\n      for idx3 in range(idx3_start, len(first_path_list3)):\\n        first_seq3 = first_path_list3[idx3]\\n        first_path4 = first_path3 + '/' + first_seq3\\n        second_path4 = second_path3 + '/' + first_seq3\\n        print(count)\\n        print(first_path4)\\n        print(second_path4)\\n        if first_path4[-5] == ')':\\n          continue\\n        depth = np.array(Image.open(first_path4), dtype=np.int16)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n\\n\\n        image = cv2.imread(second_path4)\\n\\n\\n        image = cv2.resize(image, target_size, interpolation = cv2.INTER_LANCZOS4)\\n        depth_list.append(depth)\\n        depth_name_list.append(first_path4)\\n        image_list.append(image)\\n        image_name_list.append(second_path4)\\n        count = count + 1\\n\\n\\n        if count == epoch_size:\\n          count = 0\\n          checkpoint = {\\n              'depth_list': depth_list,\\n              'image_list' : image_list,\\n          }\\n          checkpoint2 = {\\n              'depth_name' : depth_name_list,\\n              'image_name' : image_name_list\\n          }\\n          name = 'dataset_{}.pth'.format(file_count)\\n          name2 = 'name_list_{}.pth'.format(file_count)\\n          file_count = file_count + 1\\n          torch.save(checkpoint, name)\\n          torch.save(checkpoint2, name2)\\n          source_path = name\\n          source_path2 = name2\\n          destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\\n          destination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\n\\n          shutil.copy(source_path, destination_path)\\n          shutil.copy(source_path2, destination_path2)\\n\\n          depth_name_list = []\\n          image_name_list = []\\n          depth_list = []\\n          image_list = []\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["'''\n","def create_dataset_save(first_path, second_path, epoch_size = 5000): # first is depth, second is image\n","  depth_list = []\n","  image_list = []\n","  depth_name_list = []\n","  image_name_list = []\n","  count = 0\n","  file_count = 8  #改\n","\n","  first_path_list1 = sorted(os.listdir(first_path))\n","  second_path_list1 = sorted(os.listdir(second_path))\n","  idx1_start = 109   #改\n","\n","  for idx1 in range(idx1_start, len(first_path_list1)):\n","\n","    first_seq1 = first_path_list1[idx1]\n","    # if first_seq1 == '2011_09_28_drive_0090_sync': #改\n","    #   continue\n","    first_path2 = first_path + '/' + first_seq1 + '/proj_depth/groundtruth'\n","    second_path2 = second_path + '/' + first_seq1\n","    first_path_list2 = sorted(os.listdir(first_path2))\n","\n","    if idx1 == idx1_start:\n","      idx2_start = 1  # 改\n","    else:\n","      idx2_start = 0\n","    for idx2 in range(idx2_start, len(first_path_list2)): # image02, image03\n","      first_seq2 = first_path_list2[idx2]\n","      first_path3 = first_path2 + '/' + first_seq2\n","      second_path3 = second_path2 + '/' + first_seq2 + '/data'\n","      first_path_list3 = sorted(os.listdir(first_path3))\n","      if idx1 == idx1_start and idx2 == idx2_start:\n","        idx3_start = 58 #改成+1\n","      else:\n","        idx3_start = 0\n","      for idx3 in range(idx3_start, len(first_path_list3)):\n","        first_seq3 = first_path_list3[idx3]\n","        first_path4 = first_path3 + '/' + first_seq3\n","        second_path4 = second_path3 + '/' + first_seq3\n","        print(count)\n","        print(first_path4)\n","        print(second_path4)\n","        if first_path4[-5] == ')':\n","          continue\n","        depth = np.array(Image.open(first_path4), dtype=np.int16)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","\n","\n","        image = cv2.imread(second_path4)\n","\n","\n","        image = cv2.resize(image, target_size, interpolation = cv2.INTER_LANCZOS4)\n","        depth_list.append(depth)\n","        depth_name_list.append(first_path4)\n","        image_list.append(image)\n","        image_name_list.append(second_path4)\n","        count = count + 1\n","\n","\n","        if count == epoch_size:\n","          count = 0\n","          checkpoint = {\n","              'depth_list': depth_list,\n","              'image_list' : image_list,\n","          }\n","          checkpoint2 = {\n","              'depth_name' : depth_name_list,\n","              'image_name' : image_name_list\n","          }\n","          name = 'dataset_{}.pth'.format(file_count)\n","          name2 = 'name_list_{}.pth'.format(file_count)\n","          file_count = file_count + 1\n","          torch.save(checkpoint, name)\n","          torch.save(checkpoint2, name2)\n","          source_path = name\n","          source_path2 = name2\n","          destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\n","          destination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","\n","          shutil.copy(source_path, destination_path)\n","          shutil.copy(source_path2, destination_path2)\n","\n","          depth_name_list = []\n","          image_name_list = []\n","          depth_list = []\n","          image_list = []\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"yj7P7vU4eRNg","outputId":"37c1ab78-f02b-4af7-ea63-d097086ad2eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfirst_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\\nsecond_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Image'\\ncreate_dataset_save(first_path, second_path, epoch_size = 5000) # first is depth, second is image\\n\\nfrom google.colab import runtime\\nruntime.unassign()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["'''\n","first_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\n","second_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Image'\n","create_dataset_save(first_path, second_path, epoch_size = 5000) # first is depth, second is image\n","\n","from google.colab import runtime\n","runtime.unassign()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrfRJnibPUX8","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"8c496c95-2863-4fcb-fed6-ab0774c64b39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nimage_name_list = checkpoint['depth_name']\\ndepth_name_list = checkpoint['depth_name']\\nprint(depth_name_list[4999])\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["'''\n","image_name_list = checkpoint['depth_name']\n","depth_name_list = checkpoint['depth_name']\n","print(depth_name_list[4999])\n","'''"]},{"cell_type":"code","source":["'''\n","# for finding the location where it breaked\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == '2011_09_28_drive_0184_sync':\n","    print(idx)\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == 'image_03':\n","    print(idx)\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth/image_03'\n","file = sorted(os.listdir(path))\n","for idx in range(len(file)):\n","  if file[idx] == '0000000062.png':\n","    print(idx)\n","'''"],"metadata":{"id":"S2eyANJ8GiqA","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"755c0a7b-6e66-493a-b452-b7168b47882b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# for finding the location where it breaked\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == '2011_09_28_drive_0184_sync':\\n    print(idx)\\n\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == 'image_03':\\n    print(idx)\\n\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/Depth/2011_09_28_drive_0184_sync/proj_depth/groundtruth/image_03'\\nfile = sorted(os.listdir(path))\\nfor idx in range(len(file)):\\n  if file[idx] == '0000000062.png':\\n    print(idx)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41yd1a4oz7ih","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"142b5a01-2ca1-4af3-d82d-78b064911be4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef create_dataset(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\\n\\n  name_list = sorted(os.listdir(path))\\n\\n  if data_per_epoch % amount_from_file != 0:\\n    print(\"error, data_per_epoch can\\'t divide amount_from_file !!!!!\")\\n    sys.exit(1)\\n\\n  file_idx = data_per_epoch // amount_from_file\\n\\n  output_depth = []\\n  output_image = []\\n  length = len(name_list)\\n  for idx in range(file_idx):\\n    rand_int = random.randint(0, length - 1)\\n    tmp_file = name_list[rand_int]\\n    name = path + \\'/\\' + tmp_file\\n    checkpoint = torch.load(name)\\n    depth_list = checkpoint[\\'depth_list\\']\\n    image_list = checkpoint[\\'image_list\\']\\n    length_inside = len(depth_list)\\n    random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n\\n    for jdx in range(amount_from_file):\\n\\n      output_depth.append(depth_list[random_number[jdx]])\\n      output_image.append(image_list[random_number[jdx]])\\n  return output_image, output_depth\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["'''\n","def create_dataset(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\n","\n","  name_list = sorted(os.listdir(path))\n","\n","  if data_per_epoch % amount_from_file != 0:\n","    print(\"error, data_per_epoch can't divide amount_from_file !!!!!\")\n","    sys.exit(1)\n","\n","  file_idx = data_per_epoch // amount_from_file\n","\n","  output_depth = []\n","  output_image = []\n","  length = len(name_list)\n","  for idx in range(file_idx):\n","    rand_int = random.randint(0, length - 1)\n","    tmp_file = name_list[rand_int]\n","    name = path + '/' + tmp_file\n","    checkpoint = torch.load(name)\n","    depth_list = checkpoint['depth_list']\n","    image_list = checkpoint['image_list']\n","    length_inside = len(depth_list)\n","    random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","\n","    for jdx in range(amount_from_file):\n","\n","      output_depth.append(depth_list[random_number[jdx]])\n","      output_image.append(image_list[random_number[jdx]])\n","  return output_image, output_depth\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a20iYQxBFvk","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"aa8baa95-b36e-4bc3-f47f-9b7f68bf6c8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef create_dataset_for_debug(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\\n  name_list = sorted(os.listdir(path))\\n  file_idx = data_per_epoch // amount_from_file\\n  output_depth_path = []\\n  output_image_path = []\\n  output_depth = []\\n  output_image = []\\n  sym = 0\\n  if data_per_epoch % amount_from_file  != 0:\\n    file_idx = file_idx + 1\\n    sym = 1\\n  length = len(name_list)\\n  for idx in range(file_idx):\\n    rand_int = random.randint(0, length - 1)\\n    tmp_file = name_list[rand_int]\\n    name = path + '/' + tmp_file\\n    checkpoint = torch.load(name)\\n    depth_list = checkpoint['depth_list']\\n    image_list = checkpoint['image_list']\\n    length_inside = len(depth_list)\\n    if sym == 1:\\n      if idx == file_idx - 1:\\n\\n        random_number = torch.randint(0, length_inside, ((data_per_epoch % amount_from_file), ))\\n      else:\\n\\n        random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n\\n\\n    else:\\n      random_number = torch.randint(0, length_inside, (amount_from_file, ))\\n    for jdx in range(amount_from_file):\\n\\n      output_depth.append(depth_list[random_number[jdx]])\\n      output_image.append(image_list[random_number[jdx]])\\n  return output_image, output_depth\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["'''\n","def create_dataset_for_debug(data_per_epoch, amount_from_file, path): #data_per_epoch / amount_from_file要是整數\n","  name_list = sorted(os.listdir(path))\n","  file_idx = data_per_epoch // amount_from_file\n","  output_depth_path = []\n","  output_image_path = []\n","  output_depth = []\n","  output_image = []\n","  sym = 0\n","  if data_per_epoch % amount_from_file  != 0:\n","    file_idx = file_idx + 1\n","    sym = 1\n","  length = len(name_list)\n","  for idx in range(file_idx):\n","    rand_int = random.randint(0, length - 1)\n","    tmp_file = name_list[rand_int]\n","    name = path + '/' + tmp_file\n","    checkpoint = torch.load(name)\n","    depth_list = checkpoint['depth_list']\n","    image_list = checkpoint['image_list']\n","    length_inside = len(depth_list)\n","    if sym == 1:\n","      if idx == file_idx - 1:\n","\n","        random_number = torch.randint(0, length_inside, ((data_per_epoch % amount_from_file), ))\n","      else:\n","\n","        random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","\n","\n","    else:\n","      random_number = torch.randint(0, length_inside, (amount_from_file, ))\n","    for jdx in range(amount_from_file):\n","\n","      output_depth.append(depth_list[random_number[jdx]])\n","      output_image.append(image_list[random_number[jdx]])\n","  return output_image, output_depth\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxKsLDplcOiV"},"outputs":[],"source":["def create_dataset_large_epoch(random_list, now, data_path, name_path): #data_per_epoch / amount_from_file要是整數\n","\n","  data_list = sorted(os.listdir(data_path))\n","  name_list = sorted(os.listdir(name_path))\n","\n","  # file_idx = data_per_epoch // amount_from_file\n","  output_depth_path = []\n","  output_image_path = []\n","  output_depth = []\n","  output_image = []\n","\n","  now_number = random_list[now]\n","  data_path = data_path + '/' + data_list[now_number]\n","  name_path = name_path + '/' + name_list[now_number]\n","  data_checkpoint = torch.load(data_path)\n","  name_checkpoint = torch.load(name_path)\n","  output_image_path = name_checkpoint['image_name']\n","  output_depth_path = name_checkpoint['depth_name']\n","  output_depth = data_checkpoint['depth_list']\n","  output_image = data_checkpoint['image_list']\n","  return output_image_path, output_depth_path, output_depth, output_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL_iQzCkcRfA","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"189abab9-e84c-4dcd-a65b-d72e9b3e2a76"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\\nname_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\nrandom_list = []\\n\\ncheck_length = len(sorted(os.listdir(data_path)))\\nfor idx in range(1, check_length + 1):\\n  random_list.append(idx)\\nrandom.shuffle(random_list)\\nnow = 2 # 實際上now也要從0一路走到最大值\\n\\n\\noutput_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}],"source":["'''\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","random_list = []\n","\n","check_length = len(sorted(os.listdir(data_path)))\n","for idx in range(1, check_length + 1):\n","  random_list.append(idx)\n","random.shuffle(random_list)\n","now = 2 # 實際上now也要從0一路走到最大值\n","\n","\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cl4Zkgx8cV7_"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img, depth):\n","        self.img = img\n","        self.depth = depth\n","\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","        return sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcupty-acbv2","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"126f8fcb-98af-4740-cfc8-3414de3022f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ncustom_dataset = CustomDataset(output_image, output_depth)\\ntrain_size = int(0.9 * len(custom_dataset))\\nval_size = len(custom_dataset) - train_size\\ntrain_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\\n\\nbatch_size = 128\\n\\n\\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nvalidloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}],"source":["'''\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(0.9 * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","\n","batch_size = 128\n","\n","\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0u7iwvychi6"},"outputs":[],"source":["def image_loader_to_tensor(tensor):\n","  tensor = tensor.to(torch.float32)\n","  tensor = tensor / 255.0\n","  tensor = tensor * 2.0\n","  tensor = tensor - 1.0\n","  tensor = tensor.permute(0, 3, 1, 2)\n","  return tensor"]},{"cell_type":"code","source":["def depth_loader_to_tensor(tensor, DEPTH_MEAN, DEPTH_STD):\n","\n","\n","\n","  nonzero_mask = tensor != 0\n","  zero_mask = tensor == 0\n","  # mean = tensor[nonzero_mask].mean()\n","  # std = tensor[nonzero_mask].std()\n","  result = (tensor[nonzero_mask] - DEPTH_MEAN) / DEPTH_STD\n","  tensor[nonzero_mask] = result\n","  tensor[zero_mask] = -1\n","\n","\n","\n","  return tensor"],"metadata":{"id":"LGHsElDov5jL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","for batch in trainloader:\n","  print(batch['img'].shape)\n","  print(batch['img'].dtype)\n","  print(torch.max(batch['img']))\n","  print(torch.min(batch['img']))\n","  print(torch.mean(batch['img'].to(torch.float32)))\n","  print(batch['depth'].shape)\n","  print(batch['depth'].dtype)\n","  print(torch.max(batch['depth']))\n","  print(torch.min(batch['depth']))\n","  print(torch.mean(batch['depth']))\n","  break\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"wu-O30R51a_p","outputId":"09c3493f-693a-46da-8b39-6d4bad795dd6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor batch in trainloader:\\n  print(batch['img'].shape)\\n  print(batch['img'].dtype)\\n  print(torch.max(batch['img']))\\n  print(torch.min(batch['img']))\\n  print(torch.mean(batch['img'].to(torch.float32)))\\n  print(batch['depth'].shape)\\n  print(batch['depth'].dtype)\\n  print(torch.max(batch['depth']))\\n  print(torch.min(batch['depth']))\\n  print(torch.mean(batch['depth']))\\n  break\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# for batch in trainloader:\n","\n","#   batch['img'] = image_loader_to_tensor(batch['img'])\n","#   batch['depth'] = depth_loader_to_tensor(batch['depth'], DEPTH_MEAN, DEPTH_STD)\n","#   break\n","# nonzero = batch['depth'] != -1\n","# print(batch['img'].shape)\n","# print(batch['img'].dtype)\n","# print(torch.max(batch['img']))\n","# print(torch.min(batch['img']))\n","# print(torch.mean(batch['img']))\n","# print(batch['depth'].shape)\n","# print(batch['depth'].dtype)\n","# print(torch.max(batch['depth'][nonzero]))\n","# print(torch.min(batch['depth'][nonzero]))\n","# print(torch.mean(batch['depth'][nonzero]))\n","# print(torch.std(batch['depth'][nonzero]))"],"metadata":{"id":"BynQ0xH2yIDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mC3-WzjWHV1P","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"f7168894-b912-455d-9462-8d25301ed345"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef image_tensor_to_numpy(tensor):\\n\\n    tensor = tensor.permute(0, 2, 3, 1)\\n    output = tensor.numpy()\\n    output = output + 1.0\\n    output = output / 2.0\\n    output = output * 255.0\\n    output = output.astype(np.uint8)\\n\\n    return output\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["'''\n","def image_tensor_to_numpy(tensor):\n","\n","    tensor = tensor.permute(0, 2, 3, 1)\n","    output = tensor.numpy()\n","    output = output + 1.0\n","    output = output / 2.0\n","    output = output * 255.0\n","    output = output.astype(np.uint8)\n","\n","    return output\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"p6tNy_NqY22R","outputId":"a83d4652-a128-45f3-d5f2-058e4975aeaa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n\\n    mini = torch.min(tensor[tensor != 0])\\n    tensor = (tensor - mini) / (tensor.max() - mini)\\n    tensor = torch.where(tensor < 0, -1, tensor)\\n\\n\\n\\n\\n\\n\\n    return tensor\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["# this is min max normalize\n","'''\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","\n","    mini = torch.min(tensor[tensor != 0])\n","    tensor = (tensor - mini) / (tensor.max() - mini)\n","    tensor = torch.where(tensor < 0, -1, tensor)\n","\n","\n","\n","\n","\n","\n","    return tensor\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TepPXyX-YZ9P","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"3e065a7a-4ae1-47b9-e250-24c70795100f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# this is z-score normalization\\n\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n    nonzero_mask = tensor != 0\\n    zero_mask = tensor == 0\\n    mean = tensor[nonzero_mask].mean()\\n    std = tensor[nonzero_mask].std()\\n    result = (tensor[nonzero_mask] - mean) / std\\n    tensor[nonzero_mask] = result\\n    tensor[zero_mask] = -1\\n\\n\\n\\n    return tensor, mean, std\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}],"source":["'''\n","# this is z-score normalization\n","\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","    nonzero_mask = tensor != 0\n","    zero_mask = tensor == 0\n","    mean = tensor[nonzero_mask].mean()\n","    std = tensor[nonzero_mask].std()\n","    result = (tensor[nonzero_mask] - mean) / std\n","    tensor[nonzero_mask] = result\n","    tensor[zero_mask] = -1\n","\n","\n","\n","    return tensor, mean, std\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbfNMqSeP5pZ"},"outputs":[],"source":["def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","    def sigmoid(x):\n","        return 1 / (np.exp(-x) + 1)\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM6MyJPrGPUT"},"outputs":[],"source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiqjRlDrHpok"},"outputs":[],"source":["def get_timestep_embedding(timesteps, embedding_dim):\n","\n","    assert len(timesteps.shape) == 1\n","\n","    half_dim = embedding_dim // 2\n","    emb = math.log(10000) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","    emb = emb.to(device=timesteps.device)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:  # zero pad\n","        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n","    return emb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMYCE8V6JK22","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"5e9d5035-bc93-44a9-da40-31099bbf3efc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef Normalize(in_channels):\\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}],"source":["'''\n","def Normalize(in_channels):\n","    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwmQi1zV77Q8"},"outputs":[],"source":["# for the batch normalization\n","def Normalize(input, channels, momentum = 0.1, epsilon = 1e-5):\n","  bn = nn.BatchNorm2d(channels, momentum = momentum, eps = epsilon)\n","  return bn(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cr4mMbzN6tXr","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"503d5669-3636-4ebb-bb29-3a4a9f1d8e2e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef nonlinearity(x):\\n\\n    return x*torch.sigmoid(x)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["'''\n","def nonlinearity(x):\n","\n","    return x*torch.sigmoid(x)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqFsXjkX_8D2"},"outputs":[],"source":["# def nonlinearity(input, negative_slope = 0.1, inplace = False):\n","#   return torch.nn.functional.leaky_relu(input_tensor, negative_slope = negative_slope, inplace = inplace)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WRkUz1I6-JB"},"outputs":[],"source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_be0bSq7A88"},"outputs":[],"source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILHJ1zbU8LYb"},"outputs":[],"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 =nn.BatchNorm2d(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = nn.BatchNorm2d(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","    def forward(self, x, temb):\n","        h = x\n","        h = self.norm1(h)    # normalize\n","\n","        h = self.Lrelu(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","\n","        h = h + self.temb_proj(self.Lrelu(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        h = self.norm2(h)\n","        h = self.Lrelu(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy0-VDEM8uIl"},"outputs":[],"source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = nn.BatchNorm2d(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qQRtwwWkXTJ"},"outputs":[],"source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpRkaCZE2FM6"},"outputs":[],"source":["class UpsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n","                                        int(out_channels),\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6dIsb3rx6CP"},"outputs":[],"source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        self.bn0 = nn.BatchNorm2d(self.resolutions[0])\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","        self.norm_seq = nn.ModuleList()\n","        for idx in range((len(self.resolutions) - 2) * 2 + 2):\n","          print(\"up idx is = {}\".format(idx))\n","          self.norm_seq.append(nn.BatchNorm2d(self.target_channel))\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = x\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.pool(self.Lrelu(self.ConvList[idx](h)))\n","          else:\n","            h = self.Lrelu(self.ConvList[idx](temp))\n","\n","          temp = h\n","          h = self.Lrelu(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        count = 0\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","            hold = self.convOut(hold + self.norm_seq[count](FPN_list[idx]))\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","          else:\n","            hold = self.norm_seq[count](hold)\n","            count += 1\n","\n","            hold = hold + self.norm_seq[count](FPN_list[idx])\n","            count = count + 1\n","            hold = self.Upsampple[idx - 1](hold)\n","\n","        return hold\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esdKjIMr9wcF"},"outputs":[],"source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icPtSBzxAKCO"},"outputs":[],"source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fz0EX77czh3m"},"outputs":[],"source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        for idx in range(phase2):\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","\n","        for idx in range(len(self.phase1_model)):\n","          h = self.phase1_model[idx](h)\n","\n","        for idx in range(len(self.phase2_model)):\n","          h = self.phase2_model[idx](h)\n","\n","        return h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2Vzf5ZRC8wM"},"outputs":[],"source":["class depth_decode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        self.ch = config['model']['ch']\n","        # self.ch = 128\n","        self.resolution = config['data']['image_size']\n","        # self.resolution = 256\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        # self.targeted_size = 64\n","        count = 0\n","        tmp = self.targeted_size\n","        while True:\n","          if tmp == self.resolution:\n","            break\n","          count = count + 1\n","          tmp = tmp * 2\n","\n","        self.decode = nn.ModuleList()\n","        in_cha = self.ch\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","        # self.Lrelu = nonlinearity\n","        for idx in range(count):\n","          out_cha = in_cha / 4\n","          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n","          in_cha = out_cha\n","        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n","                                        1,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","\n","\n","\n","\n","\n","\n","\n","    def forward(self, pred):\n","        for idx in range(len(self.decode)):\n","          pred = self.decode[idx](pred)\n","        pred = self.final_conv(pred)\n","        pred = pred.squeeze(1)\n","\n","\n","\n","\n","\n","        return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xq591w1qfVBe"},"outputs":[],"source":["def get_index_from_list(values, t, x_shape):\n","    batch_size = t.shape[0]\n","    \"\"\"\n","    pick the values from vals\n","    according to the indices stored in `t`\n","    \"\"\"\n","    result = values.gather(-1, t.cpu())\n","    \"\"\"\n","    if\n","    x_shape = (5, 3, 64, 64)\n","        -> len(x_shape) = 4\n","        -> len(x_shape) - 1 = 3\n","\n","    and thus we reshape `out` to dims\n","    (batch_size, 1, 1, 1)\n","\n","    \"\"\"\n","    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeTTDyc0fZOI"},"outputs":[],"source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n"]},{"cell_type":"code","source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod =      [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","            alphas_cumprod_prev = [1,   0.9, 0.9 * 0.8, 0.9 * 0.8 * 0.7]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","        self.alphas_cumprod_prev = torch.cat(\n","            [torch.ones(1).to(device), self.alphas_cumprod[:-1]], dim=0\n","        )\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n","\n","    def backward(self, model, image, weight_path, skip, eta = 0.1):\n","        checkpoint = torch.load(weight_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        with torch.inference_mode():\n","\n","            skip = self.timesteps // self.args.timesteps\n","            seq = range(0, self.num_timesteps, skip)\n","            seq_next = [-1] + list(seq[:-1])\n","            x0_preds = []\n","            depth = torch.randn([image.shape[0], image.shape[-1], image.shape[-1]])\n","            xs = [depth]\n","            n = image.shape[0]\n","\n","\n","\n","            for i, j in zip(reversed(seq), reversed(seq_next)):\n","                t = (torch.ones(n) * i).to(image.device)\n","                next_t = (torch.ones(n) * j).to(image.device)\n","                at = self.alphas_cumprod.gather(-1, t.cpu())\n","\n","                at_next = self.alphas_cumprod.gather(-1, next_t.cpu())\n","\n","                xt = xs[-1].to('cuda') # x_t\n","                x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\n","\n","                x0_preds.append(x0_t.to('cpu'))\n","                et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\n","                c1 = (\n","                    eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\n","                                                                                                # can make the sampling process of x_{t - 1} become identical\n","                                                                                                # as ddpm\n","                )\n","                c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","                xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\n","                xs.append(xt_next.to('cpu'))\n","            return xs, x0_preds\n","\n","\n","\n"],"metadata":{"id":"sb2qOrGHgPgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW2SNSqY0qys"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","        depth_enc_channels = config['model']['depth_enc_channels']\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","# nonlinear\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = nn.BatchNorm2d(block_in)\n","\n","        self.depth_decode = depth_decode(config)\n","        self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","    def forward(self, image, depth, t, sampling = False):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = self.Lrelu(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","\n","        img_enc = self.fpn(image)\n","        # if sampling == False:\n","        depth = self.depth_encode(depth)\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","        if sampling == False:\n","            noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","            noisy_map = noisy_map.to(torch.float32)\n","            noise = noise.to(torch.float32)\n","        else:\n","            noisy_map = depth\n","\n","\n","\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","        # return backbone_input\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        # hs = [self.conv_in(image)]\n","\n","        hs = [self.conv_in(backbone_input)]\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = self.Lrelu(h)\n","        h = self.depth_decode(h)\n","        return h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ParlMz0UnMrq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f6b48b6-3da9-4c36-e96d-69d38fbb0935"},"outputs":[{"output_type":"stream","name":"stdout","text":["up idx is = 0\n","up idx is = 1\n","up idx is = 2\n","up idx is = 3\n","up idx is = 4\n","up idx is = 5\n"]}],"source":["model = Model(config)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"PtMRQmVtaAUR","outputId":"0e87f427-9c7b-4cfe-b87a-65905fb5d626"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# analysis for model weights\\npath = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(113)\\ncheckpoint = torch.load(path, map_location = device)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}],"source":["'''\n","# analysis for model weights\n","path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(113)\n","checkpoint = torch.load(path, map_location = device)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QVuuuiQoa9v0","outputId":"88f3edb4-d47c-408a-cb2f-ed5af8487bd8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# analysis for model weights\\nweights = checkpoint['model_state_dict']\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}],"source":["'''\n","# analysis for model weights\n","weights = checkpoint['model_state_dict']\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"oXnDKCSAbSfl","outputId":"0e537432-9acd-4360-efe0-8dae81a2fd34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# analysis for model weights\\nprint(\"number of keys : {}\".format(len(weights)))\\nfor key, value in weights.items():\\n    print(key, \"\\t\", value.shape, \"min = {}\".format(torch.min(value)))\\n    print(\"------------\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["'''\n","# analysis for model weights\n","print(\"number of keys : {}\".format(len(weights)))\n","for key, value in weights.items():\n","    print(key, \"\\t\", value.shape, \"min = {}\".format(torch.min(value)))\n","    print(\"------------\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VKW8xA20JkQ"},"outputs":[],"source":["# BATCH_SIZE = 256\n","\n","NO_LARGE_EPOCHS = 10\n","save_frequency = 5\n","LR = 0.001\n","VERBOSE = False\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/name_zip'\n","batch_size = 32\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"]},{"cell_type":"code","source":["'''\n","# debug01 build dataset\n","\n","random_list = []\n","check_length = len(sorted(os.listdir(data_path)))\n","for idx in range(1, check_length + 1):\n","  random_list.append(idx)\n","now = 0\n","random.shuffle(random_list)\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(0.9 * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","del train_dataset, val_dataset, custom_dataset\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"2T2UiToTIYf6","outputId":"c2e882fb-471e-430a-e978-f150af637510"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# debug01 build dataset\\n\\nrandom_list = []\\ncheck_length = len(sorted(os.listdir(data_path)))\\nfor idx in range(1, check_length + 1):\\n  random_list.append(idx)\\nnow = 0\\nrandom.shuffle(random_list)\\noutput_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\\n\\ncustom_dataset = CustomDataset(output_image, output_depth)\\ntrain_size = int(0.9 * len(custom_dataset))\\nval_size = len(custom_dataset) - train_size\\ntrain_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nvalidloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\ndel train_dataset, val_dataset, custom_dataset\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["'''\n","# debug02\n","for batch in trainloader:\n","  # print(batch.keys())\n","  t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","  input_img = batch['img'].to(torch.float32).to(device)\n","  input_img = image_loader_to_tensor(input_img)\n","  target_depth = batch['depth'].to(torch.float32).to(device)\n","  target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","  # print(t)\n","  # print(input_img.shape)\n","  # print(torch.max(input_img))\n","  # print(torch.max(target_depth[0]))\n","  pred_depth = model(input_img, target_depth, t)\n","  break\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"55fHsl7wI2Y0","outputId":"b7bd4c3a-5c86-4fed-b083-eabfe573edf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# debug02\\nfor batch in trainloader:\\n  # print(batch.keys())\\n  t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\\n  input_img = batch['img'].to(torch.float32).to(device)\\n  input_img = image_loader_to_tensor(input_img)\\n  target_depth = batch['depth'].to(torch.float32).to(device)\\n  target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\\n  # print(t)\\n  # print(input_img.shape)\\n  # print(torch.max(input_img))\\n  # print(torch.max(target_depth[0]))\\n  pred_depth = model(input_img, target_depth, t)\\n  break\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["# debug 04\n","# 接著可以每個dataset取一些值出來變成一個dataset來train\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","'''\n","data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/name_zip'\n","data_list = sorted(os.listdir(data_path))\n","\n","new_image_list = []\n","new_depth_list = []\n","new_image_name_list = []\n","new_depth_name_list = []\n","data_list_length = len(data_list)\n","for idx in range(data_list_length):\n","  check = torch.load(data_path + '/' + data_list[idx])\n","  check_name = torch.load(name_path + '/' + data_list[idx])\n","  for jdx in range(294):\n","    random_number = random.randint(0, 4999)\n","    new_image_list.append(check['image_list'][random_number])\n","    new_depth_list.append(check['depth_list'][random_number])\n","    new_image_name_list.append(check_name['image_name'][random_number])\n","    new_depth_name_list.append(check_name['depth_name'][random_number])\n","\n","checkpoint = {\n","              'depth_list': new_depth_list,\n","              'image_list' : new_image_list,\n","          }\n","checkpoint2 = {\n","              'depth_name' : new_depth_name_list,\n","              'image_name' : new_image_name_list\n","          }\n","\n","name = 'dataset_new.pth'\n","name2 = 'name_list_new.pth'\n","torch.save(checkpoint, name)\n","torch.save(checkpoint2, name2)\n","\n","source_path = name\n","source_path2 = name2\n","destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\n","destination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","shutil.copy(source_path, destination_path)\n","shutil.copy(source_path2, destination_path2)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"D8ToEfSlB2v9","outputId":"35f310a5-3e38-4855-fbdc-512b70f90981"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip'\\nname_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/name_zip'\\ndata_list = sorted(os.listdir(data_path))\\n\\nnew_image_list = []\\nnew_depth_list = []\\nnew_image_name_list = []\\nnew_depth_name_list = []\\ndata_list_length = len(data_list)\\nfor idx in range(data_list_length):\\n  check = torch.load(data_path + '/' + data_list[idx])\\n  check_name = torch.load(name_path + '/' + data_list[idx])\\n  for jdx in range(294):\\n    random_number = random.randint(0, 4999)\\n    new_image_list.append(check['image_list'][random_number])\\n    new_depth_list.append(check['depth_list'][random_number])\\n    new_image_name_list.append(check_name['image_name'][random_number])\\n    new_depth_name_list.append(check_name['depth_name'][random_number])\\n\\ncheckpoint = {\\n              'depth_list': new_depth_list,\\n              'image_list' : new_image_list,\\n          }\\ncheckpoint2 = {\\n              'depth_name' : new_depth_name_list,\\n              'image_name' : new_image_name_list\\n          }\\n\\nname = 'dataset_new.pth'\\nname2 = 'name_list_new.pth'\\ntorch.save(checkpoint, name)\\ntorch.save(checkpoint2, name2)\\n\\nsource_path = name\\nsource_path2 = name2\\ndestination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'    # 要改\\ndestination_path2 = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\nshutil.copy(source_path, destination_path)\\nshutil.copy(source_path2, destination_path2)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["# debug 04\n","# 還有寫繼續train在大epoch還有哪些部分沒train完畢\n","# first epoch\n","epoch = 0\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip/dataset_new.pth'\n","\n","\n","\n","\n","check = torch.load(data_path)\n","output_depth = check['depth_list']\n","output_image = check['image_list']\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(0.9 * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","\n","\n","for now in range(100): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","  epoch += 1\n","\n","\n","  start_time = time.time()\n","  mean_epoch_loss = []\n","  mean_epoch_loss_val = []\n","  epoch_gradient = {}\n","  for batch in trainloader:\n","      t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","      input_img = batch['img'].to(torch.float32).to(device)\n","      input_img = image_loader_to_tensor(input_img)\n","      target_depth = batch['depth'].to(torch.float32).to(device)\n","      target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","      pred_depth = model(input_img, target_depth, t)\n","\n","      optimizer.zero_grad()\n","      loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","      mean_epoch_loss.append(loss.item())\n","      loss.backward()\n","      optimizer.step()\n","\n","      # for name, param in model.named_pn arameters():\n","      #   if name not in epoch_gradient:\n","      #     epoch_gradient[name] = param.grad.clone()\n","      #   else:\n","      #     epoch_gradient[name] += param.grad\n","  with torch.inference_mode():\n","    for batch in validloader:\n","      t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","      input_img = batch['img'].to(torch.float32).to(device)\n","\n","      input_img = image_loader_to_tensor(input_img)\n","      target_depth = batch['depth'].to(torch.float32).to(device)\n","      target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","      pred_depth = model(input_img, target_depth, t)\n","\n","      val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","      mean_epoch_loss_val.append(val_loss.item())\n","\n","  if epoch % save_frequency == 0 :\n","    checkpoint = {\n","      'epoch': epoch,\n","      'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","      'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","      'valid_loss' : np.mean(mean_epoch_loss_val),\n","      'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","      # 'gradients' : epoch_gradient\n","    }\n","\n","    torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","    source_path = 'weight_{}.pth'.format(epoch)\n","    destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight_debug04'\n","\n","\n","    # save them to the google drive\n","    shutil.copy(source_path, destination_path)\n","\n","  #---計算時間---vvv\n","  end_time = time.time()\n","  exe_time = end_time - start_time\n","  hours, remainder = divmod(exe_time, 3600)\n","  minutes, seconds = divmod(remainder, 60)\n","  #---計算時間---^^^\n","\n","  #-----以下是存loss的---vvv\n","  checkpoint = {\n","    'epoch': epoch,\n","    'valid_loss' : np.mean(mean_epoch_loss_val),\n","    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","    # 'time' : exe_time\n","  }\n","\n","  torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","  source_path = 'loss_{}.pth'.format(epoch)\n","  destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_loss_debug04'\n","\n","\n","  # save them to the google drive\n","  shutil.copy(source_path, destination_path)\n","  #-----以下是存loss的---^^^\n","\n","  print('---')\n","  print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","  print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0Gc_AC53B3N","outputId":"2700e12e-cda6-49f2-bd27-6f84cfdcfc5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","Epoch: 1 | Train Loss 0.49356757679155894 | Val Loss 0.4185864965120951\n","time = 0.0:2.0:28.972631216049194\n","---\n","Epoch: 2 | Train Loss 0.4025674879550934 | Val Loss 0.3932868798573812\n","time = 0.0:2.0:29.553516387939453\n","---\n","Epoch: 3 | Train Loss 0.3905226579734257 | Val Loss 0.3907417853673299\n","time = 0.0:2.0:29.566131830215454\n","---\n","Epoch: 4 | Train Loss 0.3817241621868951 | Val Loss 0.38105852007865904\n","time = 0.0:2.0:29.52786874771118\n","---\n","Epoch: 5 | Train Loss 0.3698875901954515 | Val Loss 0.366618146498998\n","time = 0.0:2.0:58.72800421714783\n","---\n","Epoch: 6 | Train Loss 0.36634673731667655 | Val Loss 0.3631379723548889\n","time = 0.0:2.0:30.235275506973267\n","---\n","Epoch: 7 | Train Loss 0.36133480817079544 | Val Loss 0.3597558875878652\n","time = 0.0:2.0:29.53512692451477\n","---\n","Epoch: 8 | Train Loss 0.3564714510525976 | Val Loss 0.3534411648909251\n","time = 0.0:2.0:29.8062903881073\n","---\n","Epoch: 9 | Train Loss 0.3542402682559831 | Val Loss 0.35052496790885923\n","time = 0.0:2.0:29.923146963119507\n","---\n","Epoch: 10 | Train Loss 0.35094809510878155 | Val Loss 0.3514124572277069\n","time = 0.0:2.0:57.07910490036011\n","---\n","Epoch: 11 | Train Loss 0.3475660428404808 | Val Loss 0.35110023021698\n","time = 0.0:2.0:30.187400817871094\n","---\n","Epoch: 12 | Train Loss 0.3466924841914858 | Val Loss 0.3456621964772542\n","time = 0.0:2.0:29.504377126693726\n","---\n","Epoch: 13 | Train Loss 0.34392542093992234 | Val Loss 0.3440539360046387\n","time = 0.0:2.0:29.550973176956177\n","---\n","Epoch: 14 | Train Loss 0.34011111706495284 | Val Loss 0.33932700951894124\n","time = 0.0:2.0:29.685561656951904\n","---\n","Epoch: 15 | Train Loss 0.3399047821760178 | Val Loss 0.3430582880973816\n","time = 0.0:2.0:53.44525408744812\n","---\n","Epoch: 16 | Train Loss 0.3360555682863508 | Val Loss 0.34006994565327964\n","time = 0.0:2.0:30.30346655845642\n","---\n","Epoch: 17 | Train Loss 0.3332529951419149 | Val Loss 0.3359160085519155\n","time = 0.0:2.0:29.561672925949097\n","---\n","Epoch: 18 | Train Loss 0.3310715079307556 | Val Loss 0.33179406126340233\n","time = 0.0:2.0:29.824180603027344\n","---\n","Epoch: 19 | Train Loss 0.3276467542563166 | Val Loss 0.32570215066274005\n","time = 0.0:2.0:29.895768880844116\n","---\n","Epoch: 20 | Train Loss 0.32680620082787104 | Val Loss 0.3287039458751678\n","time = 0.0:2.0:46.51970100402832\n","---\n","Epoch: 21 | Train Loss 0.32567766862256187 | Val Loss 0.32509063482284545\n","time = 0.0:2.0:30.305896043777466\n"]}]},{"cell_type":"code","source":["# debug 03\n","# 思考LOSS這樣的原因\n","# 可能是因為每個EPOCH自身的資料太相近\n","# 而EPOCH和EPOCH之間的資料又差距太大\n","# 試試看只對某個PTH做會發生什麼事情\n","epoch = 0\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","\n","random_list = []\n","large_epoch = 1\n","for idx in range(1, check_length + 1):\n","  random_list.append(idx)\n","\n","# random.shuffle(random_list)\n","\n","for now in range(100): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","  epoch += 1\n","  if now == 0:\n","    output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, 0, data_path, name_path)\n","  custom_dataset = CustomDataset(output_image, output_depth)\n","  train_size = int(0.9 * len(custom_dataset))\n","  val_size = len(custom_dataset) - train_size\n","  train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","  trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","  validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","  start_time = time.time()\n","  mean_epoch_loss = []\n","  mean_epoch_loss_val = []\n","  epoch_gradient = {}\n","  for batch in trainloader:\n","      t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","      input_img = batch['img'].to(torch.float32).to(device)\n","      input_img = image_loader_to_tensor(input_img)\n","      target_depth = batch['depth'].to(torch.float32).to(device)\n","      target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","      pred_depth = model(input_img, target_depth, t)\n","\n","      optimizer.zero_grad()\n","      loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","      mean_epoch_loss.append(loss.item())\n","      loss.backward()\n","      optimizer.step()\n","\n","      # for name, param in model.named_pn arameters():\n","      #   if name not in epoch_gradient:\n","      #     epoch_gradient[name] = param.grad.clone()\n","      #   else:\n","      #     epoch_gradient[name] += param.grad\n","  with torch.inference_mode():\n","    for batch in validloader:\n","      t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","      input_img = batch['img'].to(torch.float32).to(device)\n","\n","      input_img = image_loader_to_tensor(input_img)\n","      target_depth = batch['depth'].to(torch.float32).to(device)\n","      target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","      pred_depth = model(input_img, target_depth, t)\n","\n","      val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","      mean_epoch_loss_val.append(val_loss.item())\n","\n","  if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","    checkpoint = {\n","      'large_epoch' : large_epoch,\n","      'epoch': epoch,\n","      'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","      'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","      'valid_loss' : np.mean(mean_epoch_loss_val),\n","      'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","      'now' : now\n","      # 'gradients' : epoch_gradient\n","    }\n","\n","    torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","    source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","    destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","    # save them to the google drive\n","    shutil.copy(source_path, destination_path)\n","\n","  #---計算時間---vvv\n","  end_time = time.time()\n","  exe_time = end_time - start_time\n","  hours, remainder = divmod(exe_time, 3600)\n","  minutes, seconds = divmod(remainder, 60)\n","  #---計算時間---^^^\n","\n","  #-----以下是存loss的---vvv\n","  checkpoint = {\n","    'large_epoch' : large_epoch,\n","    'epoch': epoch,\n","    'valid_loss' : np.mean(mean_epoch_loss_val),\n","    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","    # 'time' : exe_time\n","  }\n","\n","  torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","  source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","  destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","  # save them to the google drive\n","  shutil.copy(source_path, destination_path)\n","  #-----以下是存loss的---^^^\n","\n","  print('---')\n","  print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","  print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"id":"CqcfBO93_DED","outputId":"45783802-b11e-4cf0-fe83-c7c13716265b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","Large Epoch: 1, Epoch: 1 | Train Loss 0.49343522403921397 | Val Loss 0.4880920072396596\n","time = 0.0:2.0:36.17216897010803\n","---\n","Large Epoch: 1, Epoch: 2 | Train Loss 0.46241805787597384 | Val Loss 0.44704047838846844\n","time = 0.0:2.0:41.61445426940918\n","---\n","Large Epoch: 1, Epoch: 3 | Train Loss 0.43448203078338077 | Val Loss 0.4255649824937185\n","time = 0.0:2.0:41.41892766952515\n","---\n","Large Epoch: 1, Epoch: 4 | Train Loss 0.42302542754581995 | Val Loss 0.4182444016138713\n","time = 0.0:2.0:41.69097113609314\n","---\n","Large Epoch: 1, Epoch: 5 | Train Loss 0.4146978112203734 | Val Loss 0.408508958419164\n","time = 0.0:2.0:57.86175298690796\n","---\n","Large Epoch: 1, Epoch: 6 | Train Loss 0.41101817403520857 | Val Loss 0.4117868423461914\n","time = 0.0:2.0:41.74451184272766\n","---\n","Large Epoch: 1, Epoch: 7 | Train Loss 0.4078986849103655 | Val Loss 0.39844924012819927\n","time = 0.0:2.0:41.30629348754883\n","---\n","Large Epoch: 1, Epoch: 8 | Train Loss 0.4028422081044742 | Val Loss 0.3999204834302266\n","time = 0.0:2.0:41.77875900268555\n","---\n","Large Epoch: 1, Epoch: 9 | Train Loss 0.40226285308599474 | Val Loss 0.39044082164764404\n","time = 0.0:2.0:41.47903037071228\n","---\n","Large Epoch: 1, Epoch: 10 | Train Loss 0.39839718490839005 | Val Loss 0.409128604332606\n","time = 0.0:3.0:8.916417360305786\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-4c914c14c294>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mmean_epoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# 還有寫繼續train在大epoch還有哪些部分沒train完畢\n","# first epoch\n","epoch = 0\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","for large_epoch in range(1, NO_LARGE_EPOCHS + 1):\n","\n","    random_list = []\n","\n","    for idx in range(1, check_length + 1):\n","        random_list.append(idx)\n","\n","    random.shuffle(random_list)\n","\n","    for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","        epoch += 1\n","        output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","        custom_dataset = CustomDataset(output_image, output_depth)\n","        train_size = int(0.9 * len(custom_dataset))\n","        val_size = len(custom_dataset) - train_size\n","        train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        start_time = time.time()\n","        mean_epoch_loss = []\n","        mean_epoch_loss_val = []\n","        epoch_gradient = {}\n","        for batch in trainloader:\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","            pred_depth = model(input_img, target_depth, t)\n","\n","            optimizer.zero_grad()\n","            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            mean_epoch_loss.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","\n","            # for name, param in model.named_pn arameters():\n","            #   if name not in epoch_gradient:\n","            #     epoch_gradient[name] = param.grad.clone()\n","            #   else:\n","            #     epoch_gradient[name] += param.grad\n","        with torch.inference_mode():\n","            for batch in validloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                input_img = batch['img'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss_val.append(val_loss.item())\n","\n","        if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","            checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'now' : now\n","                # 'gradients' : epoch_gradient\n","            }\n","\n","            torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","\n","            #---計算時間---vvv\n","            end_time = time.time()\n","            exe_time = end_time - start_time\n","            hours, remainder = divmod(exe_time, 3600)\n","            minutes, seconds = divmod(remainder, 60)\n","            #---計算時間---^^^\n","\n","            #-----以下是存loss的---vvv\n","            checkpoint = {\n","            'large_epoch' : large_epoch,\n","            'epoch': epoch,\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","            # 'time' : exe_time\n","            }\n","\n","            torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","            #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JXPioJu24vDX","outputId":"32c312ac-bef1-4987-85ab-03d562472d98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","Large Epoch: 1, Epoch: 1 | Train Loss 0.13994502164423467 | Val Loss 0.10218588908513387\n","time = 0.0:2.0:37.41733717918396\n","---\n","Large Epoch: 1, Epoch: 2 | Train Loss 0.5529141611286572 | Val Loss 0.5212518314520518\n","time = 0.0:2.0:26.342310667037964\n","---\n","Large Epoch: 1, Epoch: 3 | Train Loss 0.6504759699106216 | Val Loss 0.641002341111501\n","time = 0.0:2.0:22.769662618637085\n"]}]},{"cell_type":"code","source":["\n","\n","large_epoch =\n","epoch =\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save1/weight_{}_{}.pth'.format(large_epoch, epoch)\n","data_path = ''\n","check_length = len(sorted(os.listdir(data_path)))\n","checkpoint = torch.load(load_path)\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","if (checkpoint['now'].item() == check_length - 1):\n","    large_epoch_start = large_epoch + 1\n","    del large_epoch\n","\n","    for large_epoch in range(large_epoch_start, NO_LARGE_EPOCHS + 1):\n","        random_list = []\n","        for idx in range(check_length):\n","            random_list.append(idx)\n","        random.shuffle(random_list)\n","\n","        for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","            epoch += 1\n","            output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","            custom_dataset = CustomDataset(output_image, output_depth)\n","            train_size = int(0.9 * len(custom_dataset))\n","            val_size = len(custom_dataset) - train_size\n","            train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","            trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            start_time = time.time()\n","            mean_epoch_loss = []\n","            mean_epoch_loss_val = []\n","            epoch_gradient = {}\n","            for batch in trainloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","                input_img = batch['img'].to(torch.float32).to(device)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                optimizer.zero_grad()\n","                loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss.append(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","\n","                # for name, param in model.named_parameters():\n","                #   if name not in epoch_gradient:\n","                #     epoch_gradient[name] = param.grad.clone()\n","                #   else:\n","                #     epoch_gradient[name] += param.grad\n","            with torch.inference_mode():\n","                for batch in validloader:\n","                    t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                    input_img = batch['img'].to(torch.float32).to(device)\n","                    target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","                    pred_depth = model(input_img, target_depth, t)\n","\n","                    val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                    mean_epoch_loss_val.append(val_loss.item())\n","\n","            if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","                checkpoint = {\n","                    'large_epoch' : large_epoch,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                    'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                    'valid_loss' : np.mean(mean_epoch_loss_val),\n","                    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                    'now' : now,\n","                    'random_list' : random_list\n","                    # 'gradients' : epoch_gradient\n","                }\n","\n","                torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","\n","                #---計算時間---vvv\n","                end_time = time.time()\n","                exe_time = end_time - start_time\n","                hours, remainder = divmod(exe_time, 3600)\n","                minutes, seconds = divmod(remainder, 60)\n","                #---計算時間---^^^\n","\n","                #-----以下是存loss的---vvv\n","                checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'time' : exe_time\n","                }\n","\n","                torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","                #-----以下是存loss的---^^^\n","\n","            print('---')\n","            print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","            print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","else:\n","    large_epoch_start = large_epoch\n","    del large_epoch\n","\n","    for largh_epoch in range(large_epoch_start, NO_LARGE_EPOCHS + 1):\n","        if large_epoch == large_epoch_start:\n","            now_start = checkpoint['now'].item() + 1\n","            random_list = checkpoint['random_list']\n","        else:\n","            now_start = 0\n","            random_list = []\n","            for idx in range(check_length):\n","                random_list.append(idx)\n","            random.shuffle(random_list)\n","\n","\n","        for now in range(now_start, check_length):\n","            epoch += 1\n","            output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","            custom_dataset = CustomDataset(output_image, output_depth)\n","            train_size = int(0.9 * len(custom_dataset))\n","            val_size = len(custom_dataset) - train_size\n","            train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","            trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            start_time = time.time()\n","            mean_epoch_loss = []\n","            mean_epoch_loss_val = []\n","            epoch_gradient = {}\n","            for batch in trainloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","                input_img = batch['img'].to(torch.float32).to(device)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                optimizer.zero_grad()\n","                loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss.append(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","\n","                # for name, param in model.named_parameters():\n","                #   if name not in epoch_gradient:\n","                #     epoch_gradient[name] = param.grad.clone()\n","                #   else:\n","                #     epoch_gradient[name] += param.grad\n","            with torch.inference_mode():\n","                for batch in validloader:\n","                    t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                    input_img = batch['img'].to(torch.float32).to(device)\n","                    target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","                    pred_depth = model(input_img, target_depth, t)\n","\n","                    val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                    mean_epoch_loss_val.append(val_loss.item())\n","\n","            if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","                checkpoint = {\n","                    'large_epoch' : large_epoch,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                    'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                    'valid_loss' : np.mean(mean_epoch_loss_val),\n","                    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                    'now' : now,\n","                    'random_list' : random_list\n","                    # 'gradients' : epoch_gradient\n","                }\n","\n","                torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save1'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","\n","                #---計算時間---vvv\n","                end_time = time.time()\n","                exe_time = end_time - start_time\n","                hours, remainder = divmod(exe_time, 3600)\n","                minutes, seconds = divmod(remainder, 60)\n","                #---計算時間---^^^\n","\n","                #-----以下是存loss的---vvv\n","                checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'time' : exe_time\n","                }\n","\n","                torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save1'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","                #-----以下是存loss的---^^^\n","\n","            print('---')\n","            print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","            print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","\n"],"metadata":{"id":"8pE2-xpLDCXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = torch.tensor([1]).item()\n","test == 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCo3iTI1D3WA","outputId":"34fd606e-6dca-4b78-e5d1-50c79e7a0ad5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["# first epoch\n","'''\n","\n","\n","\n","for epoch in range(1, NO_EPOCHS + 1, 1):\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","'''"],"metadata":{"id":"s7nsCtRP6Vu9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZDEnqO21HxI"},"outputs":[],"source":["\n","\n","\n","# train_size = int(0.9 * len(custom_dataset))\n","# val_size = len(custom_dataset) - train_size\n","\n","\n","# train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","\n","# # 使用DataLoader加载数据集\n","# batch_size = 32\n","# trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","# validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvNvuYEqONOK"},"outputs":[],"source":["# print(input_img.shape)\n","# print(target_depth.shape)\n","# print(t.shape)\n","# count = 0\n","# for batch in trainloader:\n","#   count = count + 1\n","# print(count)\n","# print(32 * 31)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"EkbS3lIMvitZ","outputId":"186fcd65-ec61-411b-e0c7-da6f6fd2a530"},"outputs":[{"ename":"IndentationError","evalue":"unindent does not match any outer indentation level (<tokenize>, line 94)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    if epoch % PRINT_FREQUENCY == 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"]}],"source":["def training(start_epoch, steps, load_path, model, PRINT_FREQUENCY,  optimizer, save_frequency, trainloader, validloader, datasets_path, batch_size, config, device, weights_save_path, loss_save_path, gradient_save_path = None, save_gradient = False): # load path is where the already existed weights save\n","  initial = start_epoch\n","\n","  NO_EPOCHS = steps # 要多做幾個epochs\n","  # load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","  checkpoint = torch.load(load_path)\n","\n","  start = checkpoint['epoch'] + 1\n","  # model_state_dict = checkpoint['model_state_dict']\n","\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  for epoch in range(start , start + NO_EPOCHS + 1):\n","      start_time = time.time()\n","      epoch_gradient = {}\n","      mean_epoch_loss = []\n","      mean_epoch_loss_val = []\n","      for batch in trainloader:\n","          t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","          input_img = batch['img'].to(torch.float32).to(device)\n","          target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","          pred_depth = model(input_img, target_depth, t)\n","\n","          optimizer.zero_grad()\n","          loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","          mean_epoch_loss.append(loss.item())\n","          loss.backward()\n","\n","          if save_gradient :\n","            for name, param in model.named_parameters():\n","              if name not in epoch_gradient:\n","                epoch_gradient[name] = param.grad.clone()\n","              else:\n","                epoch_gradient[name] += param.grad\n","\n","\n","          optimizer.step()\n","\n","      with torch.inference_mode():\n","        for batch in validloader:\n","          t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","          input_img = batch['img'].to(torch.float32).to(device)\n","          target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","          pred_depth = model(input_img, target_depth, t)\n","\n","          val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","          mean_epoch_loss_val.append(val_loss.item())\n","\n","      if epoch % save_frequency == 0 or epoch == start + NO_EPOCHS:\n","          checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","            'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","          }\n","\n","          torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","          source_path = 'weight_{}.pth'.format(epoch)\n","          destination_path = weights_save_path\n","\n","\n","          # save them to the google drive\n","          shutil.copy(source_path, destination_path)\n","          #---計算時間---vvv\n","          end_time = time.time()\n","          exe_time = end_time - start_time\n","          hours, remainder = divmod(execution_time, 3600)\n","          minutes, seconds = divmod(remainder, 60)\n","          #---計算時間---^^^\n","          #-----以下是存loss的---vvv\n","          checkpoint = {\n","            'epoch': epoch,\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","            'time' : exe_time\n","          }\n","\n","          torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","          source_path = 'loss_{}.pth'.format(epoch)\n","          destination_path = loss_save_path\n","          #-----存gradient---vvv\n","          if save_gradient:\n","            checkpoint = {\n","            'gradients' : epoch_gradient\n","          }\n","          torch.save(checkpoint, 'gradient_{}.pth'.format(epoch))\n","          source_path = 'gradient_{}.pth'.format(epoch)\n","          destination_path = gradient_save_path\n","          shutil.copy(source_path, destination_path)\n","\n","          #-----存gradient---^^^\n","          # save them to the google drive\n","          shutil.copy(source_path, destination_path)\n","          #-----以下是存loss的---^^^\n","\n","        if epoch % PRINT_FREQUENCY == 0:\n","          print('---')\n","          print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","          print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"E67RH3azzzGn","outputId":"f055dba6-cdc1-4914-8deb-c8f9b3ee6aa9"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport shutil\\nfrom google.colab import files\\n\\nfor epoch in range(1, NO_EPOCHS + 1, 1):\\n    mean_epoch_loss = []\\n    mean_epoch_loss_val = []\\n    epoch_gradient = {}\\n    for batch in trainloader:\\n        t = torch.randint(0, config[\\'diffusion\\'][\\'num_diffusion_timesteps\\'], (batch_size,)).long().to(device)\\n\\n        input_img = batch[\\'img\\'].to(torch.float32).to(device)\\n        target_depth = batch[\\'depth\\'].to(torch.float32).to(device)\\n\\n        pred_depth = model(input_img, target_depth, t)\\n\\n        optimizer.zero_grad()\\n        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\\n        mean_epoch_loss.append(loss.item())\\n        loss.backward()\\n\\n\\n        for name, param in model.named_parameters():\\n          if name not in epoch_gradient:\\n            epoch_gradient[name] = param.grad.clone()\\n          else:\\n            epoch_gradient[name] += param.grad\\n        optimizer.step()\\n\\n    with torch.inference_mode():\\n      for batch in validloader:\\n        t = torch.randint(0, config[\\'diffusion\\'][\\'num_diffusion_timesteps\\'], (batch_size,)).long().to(device)\\n        input_img = batch[\\'img\\'].to(torch.float32).to(device)\\n        target_depth = batch[\\'depth\\'].to(torch.float32).to(device)\\n\\n        pred_depth = model(input_img, target_depth, t)\\n\\n        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\\n        mean_epoch_loss_val.append(val_loss.item())\\n\\n    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\\n        checkpoint = {\\n          \\'epoch\\': epoch,\\n          \\'model_state_dict\\': model.state_dict(), # model.state_dict()是存下param的的值和形狀\\n          \\'optimizer_state_dict\\': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\\n          \\'valid_loss\\' : np.mean(mean_epoch_loss_val),\\n          \\'loss\\' : np.mean(mean_epoch_loss), # 記得不能存tensor\\n          \\'gradients\\' : epoch_gradient\\n        }\\n\\n        torch.save(checkpoint, \\'weight_{}.pth\\'.format(epoch))\\n        source_path = \\'weight_{}.pth\\'.format(epoch)\\n        destination_path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save\\'\\n\\n\\n        # save them to the google drive\\n        shutil.copy(source_path, destination_path)\\n\\n\\n\\n        #-----以下是存loss的---vvv\\n        checkpoint = {\\n          \\'epoch\\': epoch,\\n          \\'valid_loss\\' : np.mean(mean_epoch_loss_val),\\n          \\'loss\\' : np.mean(mean_epoch_loss) # 記得不能存tensor\\n        }\\n\\n        torch.save(checkpoint, \\'loss_{}.pth\\'.format(epoch))\\n        source_path = \\'loss_{}.pth\\'.format(epoch)\\n        destination_path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save\\'\\n\\n\\n        # save them to the google drive\\n        shutil.copy(source_path, destination_path)\\n        #-----以下是存loss的---^^^\\n        print(\\'---\\')\\n        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\\n'"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# first epoch\n","'''\n","import shutil\n","\n","\n","for epoch in range(1, NO_EPOCHS + 1, 1):\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MkEE7uV2_mt"},"outputs":[],"source":["# import torch\n","# import shutil\n","# from google.colab import files\n","# initial = 16\n","# NO_EPOCHS = 200 # 要多做幾個epochs\n","# load_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","# checkpoint = torch.load(load_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hw122Fd47aPB","outputId":"d6d2e675-a103-43ea-d5b1-cc9ae9e79576"},"outputs":[{"name":"stdout","output_type":"stream","text":["---\n","Epoch: 91 | Train Loss 0.38567152300051283 | Val Loss 0.39286285638809204\n","---\n","Epoch: 92 | Train Loss 0.3859223084790366 | Val Loss 0.3921920657157898\n","---\n","Epoch: 93 | Train Loss 0.3854115701147488 | Val Loss 0.3909292121728261\n","---\n","Epoch: 94 | Train Loss 0.38506057752030237 | Val Loss 0.38761888941129047\n","---\n","Epoch: 95 | Train Loss 0.3845937837447439 | Val Loss 0.3917753994464874\n","---\n","Epoch: 96 | Train Loss 0.38600521215370726 | Val Loss 0.3998758594195048\n","---\n","Epoch: 97 | Train Loss 0.38719060378415243 | Val Loss 0.39039607842763263\n","---\n","Epoch: 98 | Train Loss 0.38408188096114565 | Val Loss 0.3900303343931834\n","---\n","Epoch: 99 | Train Loss 0.3831964518342699 | Val Loss 0.38861361145973206\n","---\n","Epoch: 100 | Train Loss 0.3895272814801761 | Val Loss 0.3920138378938039\n","---\n","Epoch: 101 | Train Loss 0.39488488116434645 | Val Loss 0.3970567186673482\n","---\n","Epoch: 102 | Train Loss 0.6200624565993037 | Val Loss 0.6198289394378662\n","---\n","Epoch: 103 | Train Loss 0.600775699530329 | Val Loss 0.6032946904500326\n","---\n","Epoch: 104 | Train Loss 0.5959067557539258 | Val Loss 0.6032663385073344\n","---\n","Epoch: 105 | Train Loss 0.5949824367250715 | Val Loss 0.6023848652839661\n","---\n","Epoch: 106 | Train Loss 0.5950001137597221 | Val Loss 0.602178951104482\n","---\n","Epoch: 107 | Train Loss 0.5938941793782371 | Val Loss 0.602675219376882\n","---\n","Epoch: 108 | Train Loss 0.5921581664255687 | Val Loss 0.6111739873886108\n","---\n","Epoch: 109 | Train Loss 0.5923646709748677 | Val Loss 0.6003530224164327\n","---\n","Epoch: 110 | Train Loss 0.5899561579738345 | Val Loss 0.6024161775906881\n","---\n","Epoch: 111 | Train Loss 0.5886165691273553 | Val Loss 0.5966901183128357\n"]}],"source":["# continue training\n","initial = 90\n","NO_EPOCHS = 20 # 要多做幾個epochs\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save/weight_{}.pth'.format(initial) # 位置要改\n","checkpoint = torch.load(load_path)\n","\n","start = checkpoint['epoch'] + 1\n","# model_state_dict = checkpoint['model_state_dict']\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","for epoch in range(start , start + NO_EPOCHS + 1):\n","    start_time = time.time()\n","    epoch_gradient = {}\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","\n","\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == start + NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save_gradient_dev'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(execution_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save_gradient_dev'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"o8eritXvUSMF","outputId":"49dc148b-852d-4894-b686-07bd7dd4bd4f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport torch\\nimport os\\nfor idx in range(133):\\n  if idx == 45:\\n    continue\\n\\n  path = \\'/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save/loss_{}.pth\\'.format(idx)\\n  check = torch.load(path)\\n  print(\"epoch : {}, loss = {}  | val_loss = {}\".format(check[\\'epoch\\'], check[\\'loss\\'], check[\\'valid_loss\\']))\\n  print(\\'----------\\')\\n\\n'"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","import torch\n","import os\n","for idx in range(133):\n","  if idx == 45:\n","    continue\n","\n","  path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save/loss_{}.pth'.format(idx)\n","  check = torch.load(path)\n","  print(\"epoch : {}, loss = {}  | val_loss = {}\".format(check['epoch'], check['loss'], check['valid_loss']))\n","  print('----------')\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H00KhF04so4U"},"outputs":[],"source":["'''\n","# continue training\n","# train for another account\n","# change the lr\n","\n","# continue training\n","# train for another account\n","# change the lr\n","\n","import shutil\n","from google.colab import files\n","initial = 100\n","NO_EPOCHS = 2000 # 要多做幾個epochs\n","load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save_gradient_dev/weight_{}.pth'.format(initial) # 位置要改\n","checkpoint = torch.load(load_path)\n","\n","start = checkpoint['epoch'] + 1\n","# model_state_dict = checkpoint['model_state_dict']\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","for epoch in range(start , start + NO_EPOCHS + 1):\n","    for param_group in optimizer.param_groups:\n","      param_group['lr'] = 0.0001\n","    epoch_gradient = {}\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","\n","        for name, param in model.named_parameters():\n","          if name not in epoch_gradient:\n","            epoch_gradient[name] = param.grad.clone()\n","          else:\n","            epoch_gradient[name] += param.grad\n","\n","\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0 or epoch == start + NO_EPOCHS:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","          'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}.pth'.format(epoch))\n","        source_path = 'weight_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}.pth'.format(epoch))\n","        source_path = 'loss_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dw8wH8EQs8B6"},"outputs":[],"source":["'''\n","for idx in range(91, 111):\n","  path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/loss_save_gradient_dev/loss_{}.pth'.format(idx)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Nl3NJwQtGkP"},"outputs":[],"source":["'''\n","# load gradient\n","epoch = 102\n","path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/weight_save_gradient_dev/weight_{}.pth'.format(epoch)\n","check = torch.load(path, map_location = 'cpu')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tdK8tgbtKw1"},"outputs":[],"source":["'''\n","gradients = check['gradients']\n","torch.set_printoptions(precision=10)\n","\n","path\n","\n","for name, grad in gradients.items():\n","  # if name == 'fpn.ConvList.0.conv.bias':\n","    maximum = torch.max(grad)\n","    minimum = torch.min(grad)\n","    mean_val = torch.mean(grad)\n","    print(name, \" : \", grad.shape, \" | min = \", minimum, \" | max = \", maximum, \" | mean = \", mean_val)\n","    print(\"-----------------------\")\n","    # break\n","\n","torch.min(grad)\n","minimum\n","\n","torch.set_printoptions(precision=6)\n","print(maximum)\n","'''"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"C1MjK8pF-GG7","executionInfo":{"status":"ok","timestamp":1710764253033,"user_tz":-480,"elapsed":1105,"user":{"displayName":"Archer914","userId":"03017676274708570535"}}},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}