{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TL6RdTKY4KNI","executionInfo":{"status":"ok","timestamp":1709815099540,"user_tz":-480,"elapsed":10009,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["import torch\n","import cv2\n","import os\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","source":["def deb(param, str):\n","  print(str + \" = {}\".format(param))"],"metadata":{"id":"sfQ1zEzuzZL2","executionInfo":{"status":"ok","timestamp":1709815099542,"user_tz":-480,"elapsed":38,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5T3gP7QULTMt","executionInfo":{"status":"ok","timestamp":1709815099543,"user_tz":-480,"elapsed":34,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["\n","\n","def load_config(file_path):\n","    with open(file_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config\n"]},{"cell_type":"code","source":["def count_params(model):\n","\n","  sum = 0\n","  for param in model.parameters():\n","    sum = sum + param.numel()\n","  return sum"],"metadata":{"id":"fedtFZjG6Ucb","executionInfo":{"status":"ok","timestamp":1709815099543,"user_tz":-480,"elapsed":33,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5H-DIrmZLVw4","executionInfo":{"status":"ok","timestamp":1709815099543,"user_tz":-480,"elapsed":31,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/config.yml'\n","config = load_config(file_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RfB2wbY_5tWb","executionInfo":{"status":"ok","timestamp":1709815099543,"user_tz":-480,"elapsed":30,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# img = cv2.imread(\"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Image_Sample/2011_09_26_drive_0002_sync_image_0000000005_image_02.png\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"n9i9pX5s6Gzx","executionInfo":{"status":"ok","timestamp":1709815099544,"user_tz":-480,"elapsed":30,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["target_size = (config['data']['image_size'], config['data']['image_size'])\n","# img = cv2.resize(img, target_size)\n","# img.shape\n","\n","# cv2_imshow(img)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1709815099544,"user":{"displayName":"黃勁元","userId":"11372567893123424236"},"user_tz":-480},"id":"LlHbWDs0L9bl","outputId":"7f9f6266-acc4-4974-ad58-95c1bac6ccfc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(256, 256)"]},"metadata":{},"execution_count":8}],"source":["target_size"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"uvBywcbd6ed_","executionInfo":{"status":"ok","timestamp":1709815099544,"user_tz":-480,"elapsed":15,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def image_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","    # 获取文件夹中所有文件的列表\n","    # files = os.listdir(folder_path)\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","\n","        img = cv2.imread(file_path)\n","        img = cv2.resize(img, target_size, interpolation = cv2.INTER_LANCZOS4)\n","        images_list.append(img)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","\n","    tensor = tensor.to(torch.float16)\n","\n","\n","    tensor = tensor / 255.0\n","    tensor = tensor * 2.0\n","    tensor = tensor - 1.0\n","    tensor = tensor.permute(0, 3, 1, 2)\n","    # output = torch.tensor(img2)\n","    # output = output.permute(0, 3, 1, 2)\n","\n","    # output = tensor\n","\n","\n","    return tensor\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"mC3-WzjWHV1P","executionInfo":{"status":"ok","timestamp":1709815099544,"user_tz":-480,"elapsed":14,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def image_tensor_to_numpy(tensor):\n","\n","    tensor = tensor.permute(0, 2, 3, 1)\n","    output = tensor.numpy()\n","    output = output + 1.0\n","    output = output / 2.0\n","    output = output * 255.0\n","    output = output.astype(np.uint8)\n","\n","\n","\n","    return output\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpK8t4bbYtHL","executionInfo":{"status":"ok","timestamp":1709815100040,"user_tz":-480,"elapsed":507,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"d71ef6d5-2088-4305-89be-6243334588ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 3, 256, 256])"]},"metadata":{},"execution_count":11}],"source":["# folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/image\"\n","folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Image_Sample\"\n","img = image_folder_to_tensor(folder_path)\n","img.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"vxUPyGLvcIAx","executionInfo":{"status":"ok","timestamp":1709815100040,"user_tz":-480,"elapsed":18,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# test = torch.tensor([0, 1, 2])\n","# non_zero_min = torch.min(test[test != 0])\n","# non_zero_min\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"p6tNy_NqY22R","executionInfo":{"status":"ok","timestamp":1709815100041,"user_tz":-480,"elapsed":17,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"f5c8fe02-720d-48f1-b720-d2faa25e4a68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef depth_folder_to_tensor(folder_path):\\n\\n    images_list = []\\n\\n\\n\\n    files = sorted(os.listdir(folder_path))\\n\\n    for file in files:\\n\\n        file_path = os.path.join(folder_path, file)\\n        depth = np.array(Image.open(file_path), dtype=np.int16)\\n        assert(np.max(depth) > 255)\\n        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\\n        depth = depth.astype(np.float16) / 256.0\\n        images_list.append(depth)\\n\\n\\n\\n    img2 = np.stack(images_list, axis=0)\\n    tensor = torch.tensor(img2)\\n\\n    mini = torch.min(tensor[tensor != 0])\\n    tensor = (tensor - mini) / (tensor.max() - mini)\\n    tensor = torch.where(tensor < 0, -1, tensor)\\n\\n\\n\\n\\n\\n\\n    return tensor\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["# this is min max normalize\n","'''\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","\n","    mini = torch.min(tensor[tensor != 0])\n","    tensor = (tensor - mini) / (tensor.max() - mini)\n","    tensor = torch.where(tensor < 0, -1, tensor)\n","\n","\n","\n","\n","\n","\n","    return tensor\n","'''"]},{"cell_type":"code","source":["# this is z-score normalization\n","\n","def depth_folder_to_tensor(folder_path):\n","\n","    images_list = []\n","\n","\n","\n","    files = sorted(os.listdir(folder_path))\n","\n","    for file in files:\n","\n","        file_path = os.path.join(folder_path, file)\n","        depth = np.array(Image.open(file_path), dtype=np.int16)\n","        assert(np.max(depth) > 255)\n","        depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","        depth = depth.astype(np.float16) / 256.0\n","        images_list.append(depth)\n","\n","\n","\n","    img2 = np.stack(images_list, axis=0)\n","    tensor = torch.tensor(img2)\n","    nonzero_mask = tensor != 0\n","    zero_mask = tensor == 0\n","    mean = tensor[nonzero_mask].mean()\n","    std = tensor[nonzero_mask].std()\n","    result = (tensor[nonzero_mask] - mean) / std\n","    tensor[nonzero_mask] = result\n","    tensor[zero_mask] = -1\n","\n","\n","\n","    return tensor, mean, std"],"metadata":{"id":"TepPXyX-YZ9P","executionInfo":{"status":"ok","timestamp":1709815100041,"user_tz":-480,"elapsed":8,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"hIVB5KbFc9Dj","executionInfo":{"status":"ok","timestamp":1709815100041,"user_tz":-480,"elapsed":7,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# file_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/groundtruth_depth/2011_09_26_drive_0002_sync_groundtruth_depth_0000000005_image_02.png\"\n","# depth = np.array(Image.open(file_path), dtype=np.int16)\n","# assert(np.max(depth) > 255)\n","# # depth_png = depth_png.astype(np.int16)\n","# depth = cv2.resize(depth, target_size, cv2.INTER_LANCZOS4)\n","# depth = depth.astype(np.float16) / 256.0\n","\n","\n","\n","# '''\n","# depth = cv2.resize(depth_png, target_size, cv2.INTER_LANCZOS4)\n","# depth = depth.astype(np.float32) / 256.\n","# cv2_imshow(depth)\n","# '''\n","# # print((depth_png.dtype))\n","# # images_list.append(depth)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"YixjgdVllOy7","executionInfo":{"status":"ok","timestamp":1709815101193,"user_tz":-480,"elapsed":1158,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# tensor = torch.tensor([[1, 0, 3], [0, 5, 6]])\n","\n","# # 将张量内的所有零值替换为-1\n","# tensor_with_negative_one = torch.where(tensor == 0, -1, tensor)\n","\n","# print(\"原始张量:\")\n","# print(tensor)\n","\n","# print(\"\\n零值替换为-1后的张量:\")\n","# print(tensor_with_negative_one)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"VL6JEZ3Q87-a","executionInfo":{"status":"ok","timestamp":1709815101195,"user_tz":-480,"elapsed":21,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/groundtruth_depth\"\n","folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Depth_Sample\"\n","dep, depth_mean, depth_std = depth_folder_to_tensor(folder_path)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThuEp1EvP1pE","executionInfo":{"status":"ok","timestamp":1709815101888,"user_tz":-480,"elapsed":706,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"950b0323-df8e-45c9-dc51-ac457d83b746"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 256, 256])"]},"metadata":{},"execution_count":18}],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img, depth):\n","        self.img = img\n","        self.depth = depth\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","        return sample\n","\n","\n","\n","\n","# 创建数据集实例\n","custom_dataset = CustomDataset(img, dep)\n","\n","# 使用DataLoader加载数据集\n","batch_size = 32\n","batch_size = 8\n","batch_size = 2\n","trainloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n","count = 0\n","# 迭代DataLoader获取批次数据\n","for batch in trainloader:\n","    count = count + 1\n","    input_img = batch['img']\n","    target_depth = batch['depth']\n","    break\n","input_img.shape"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRXvVR8RPQWG","executionInfo":{"status":"ok","timestamp":1709815101888,"user_tz":-480,"elapsed":99,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"21c4142a-3e7d-48ef-884a-8fe464eaa783"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-1., dtype=torch.float16)"]},"metadata":{},"execution_count":19}],"source":["dep.min()"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oo5e_yf2CLu9","executionInfo":{"status":"ok","timestamp":1709815101888,"user_tz":-480,"elapsed":88,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"249773d3-cd0c-4e32-de3c-9f66a521e874"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-1., dtype=torch.float16)"]},"metadata":{},"execution_count":20}],"source":["img.min()"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"uzp5unsLJHej","executionInfo":{"status":"ok","timestamp":1709815101889,"user_tz":-480,"elapsed":80,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/groundtruth_depth\"\n","# files = sorted(os.listdir(folder_path))\n","\n","# for file in files:\n","\n","#   file_path = os.path.join(folder_path, file)\n","#   img = cv2.imread(file_path)\n","#   print(type(img))\n","#   break\n","# folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/image\"\n","\n","# files = sorted(os.listdir(folder_path))\n","\n","# for file in files:\n","\n","#   file_path = os.path.join(folder_path, file)\n","#   img2 = cv2.imread(file_path)\n","#   print(type(img))\n","#   break\n","\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ggjXa699KEIK","executionInfo":{"status":"ok","timestamp":1709815101889,"user_tz":-480,"elapsed":79,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# nonzero_indices = np.nonzero(img)\n","# cv2_imshow(img)\n","# print(nonzero_indices)\n","# print(img[90, 1135, 0])"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"dG_LEJfXP6iX","executionInfo":{"status":"ok","timestamp":1709815101889,"user_tz":-480,"elapsed":78,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["\n","\n","filename = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/groundtruth_depth/2011_09_26_drive_0002_sync_groundtruth_depth_0000000005_image_02.png\"\n","def depth_read(filename):\n","    # loads depth map D from png file\n","    # and returns it as a numpy array,\n","    # for details see readme.txt\n","\n","    depth_png = np.array(Image.open(filename), dtype=int)\n","    # make sure we have a proper 16bit depth map here.. not 8bit!\n","    assert(np.max(depth_png) > 255)\n","\n","    depth = depth_png.astype(np.float32) / 256.\n","    depth[depth_png == 0] = -1.\n","    return depth\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"zRNPPHX_P63D","executionInfo":{"status":"ok","timestamp":1709815101889,"user_tz":-480,"elapsed":78,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["test = depth_read(filename)\n","# test = np.array([1, 2])\n","# test.astype(np.float)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TpjzjJ_TQzl","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":78,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"22db8971-59a2-45c7-be79-7f15a2ba65fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["15.930007"]},"metadata":{},"execution_count":25}],"source":["exclude = -1.0\n","masked_arr = test[test != exclude]\n","masked_arr.shape\n","mean = np.mean(masked_arr)\n","mean\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"2N6t9v7FG-cJ","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":68,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["# class CustomDataset(Dataset):\n","#     def __init__(self, img, depth):\n","#         self.img = img\n","#         self.depth = depth\n","\n","#     def __len__(self):\n","#         return len(self.img)\n","\n","#     def __getitem__(self, idx):\n","#         sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","#         return sample\n","\n","\n","\n","\n","# # 创建数据集实例\n","# custom_dataset = CustomDataset(img, dep)\n","\n","# # 使用DataLoader加载数据集\n","# batch_size = 32\n","# data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n","# count = 0\n","# # 迭代DataLoader获取批次数据\n","# for batch in data_loader:\n","#     count = count + 1\n","#     input_img = batch['img']\n","#     target_depth = batch['depth']\n","#     break\n","# input_img.shape"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"EbfNMqSeP5pZ","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":67,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","    def sigmoid(x):\n","        return 1 / (np.exp(-x) + 1)\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"i42KCaoIFtbx","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":66,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["config['diffusion']['beta_start']\n","beta_schedule = config['diffusion']['beta_schedule']\n","beta_start = config['diffusion']['beta_start']\n","beta_end = config['diffusion']['beta_end']\n","num_diffusion_timesteps = config['diffusion']['num_diffusion_timesteps']"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfltklszFw2M","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":66,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"1cd35c7b-037c-4ad0-ba01-b092b901ce92"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["beta = torch.tensor(get_beta_schedule(beta_schedule, beta_start = beta_start, beta_end = beta_end, num_diffusion_timesteps = num_diffusion_timesteps))\n","beta = beta.to(device)\n","print(beta.device)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"AM6MyJPrGPUT","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":57,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"kiqjRlDrHpok","executionInfo":{"status":"ok","timestamp":1709815101890,"user_tz":-480,"elapsed":56,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def get_timestep_embedding(timesteps, embedding_dim):\n","\n","    assert len(timesteps.shape) == 1\n","\n","    half_dim = embedding_dim // 2\n","    emb = math.log(10000) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","    emb = emb.to(device=timesteps.device)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:  # zero pad\n","        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n","    return emb"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFOTCzfsI01j","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":57,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"5ed12bdd-1e35-4a9d-f4be-4ac13c4bf81e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00],\n","        [ 8.4147e-01,  1.0000e-04,  5.4030e-01,  1.0000e+00],\n","        [ 9.0930e-01,  2.0000e-04, -4.1615e-01,  1.0000e+00],\n","        [ 1.4112e-01,  3.0000e-04, -9.8999e-01,  1.0000e+00],\n","        [-9.5892e-01,  5.0000e-04,  2.8366e-01,  1.0000e+00]])"]},"metadata":{},"execution_count":32}],"source":["timesteps = torch.tensor([0, 1, 2, 3, 5])\n","test = get_timestep_embedding(timesteps, 4)\n","test"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"JMYCE8V6JK22","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":47,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def Normalize(in_channels):\n","    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"cr4mMbzN6tXr","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":46,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["def nonlinearity(x):\n","\n","    return x*torch.sigmoid(x)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"_WRkUz1I6-JB","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":46,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"V_be0bSq7A88","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":45,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"ILHJ1zbU8LYb","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":45,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["\n","class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 = Normalize(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = Normalize(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","    def forward(self, x, temb):\n","        # print(\"in = {}\".format(self.in_channels))\n","        # print(\"out = {}\".format(self.out_channels))\n","        # print(\"x.shape = {}\".format(x.shape))\n","        # print(\"temb.shape = {}\".format(temb.shape))\n","        # print(\"temb_channel = {}\".format(self.temb_channels))\n","        # print(\"here\")\n","        h = x\n","        # print(\"here is = {}\".format(h.dtype))\n","        h = self.norm1(h)    # normalize\n","\n","        h = nonlinearity(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","        # print(\"h.shape = {}\".format(h.shape))\n","        # print(\"self.temb_proj(nonlinearity(temb)).shape = {}\".format(self.temb_proj(nonlinearity(temb)).shape))\n","        # print(\"temb_proj(nonlinearity(temb))[:, :, None, None].shape = {}\".format(self.temb_proj(nonlinearity(temb))[:, :, None, None].shape))\n","        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        # temb是什麼 是用來做positional embedding的嗎????????\n","\n","        h = self.norm2(h)\n","        h = nonlinearity(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Zy0-VDEM8uIl","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":44,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"outputs":[],"source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = Normalize(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"]},{"cell_type":"code","source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"],"metadata":{"id":"2qQRtwwWkXTJ","executionInfo":{"status":"ok","timestamp":1709815101891,"user_tz":-480,"elapsed":44,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["class UpsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            print(in_channels)\n","            print(out_channels)\n","            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n","                                        int(out_channels),\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","            print(\"here\")\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","\n","        return x"],"metadata":{"id":"QpRkaCZE2FM6","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":44,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["'''\n","class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的一半\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        self.resolutions = resolutions.copy()\n","\n","        self.target_channel = int(resolutions[0] / 2)\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","\n","          self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      1,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = x\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","          print(idx)\n","          if idx == 0:\n","            h = nonlinearity(self.ConvList[idx](h))\n","\n","          else:\n","            h = nonlinearity(self.ConvList[idx](temp))\n","          temp = h\n","          h = nonlinearity(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx](FPN_list[idx])\n","          else:\n","            hold = hold + FPN_list[idx]\n","            hold = self.Upsampple[idx](hold)\n","        return self.convOut(hold)\n","\n","\n","\n","\n","'''"],"metadata":{"id":"ziqGL2YNfLxU","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":44,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"colab":{"base_uri":"https://localhost:8080/","height":157},"outputId":"2b667cc8-88be-4464-8939-c470d5142e87"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nclass FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的一半\\n    def __init__(self, config):\\n        super().__init__()\\n        resolutions = config['model']['FPN_conv_res'].copy()\\n\\n        self.resolutions = resolutions.copy()\\n\\n        self.target_channel = int(resolutions[0] / 2)\\n\\n        resolutions.insert(0, 3)\\n        # self.resolutions = resolutions # which is list\\n        self.ConvList = nn.ModuleList()\\n        self.tuneChannels = nn.ModuleList()\\n        self.Upsampple = nn.ModuleList()\\n\\n        for idx in range(len(resolutions) - 1):\\n          self.ConvList.append(DownsampleFPN(resolutions[idx],\\n                                          resolutions[idx + 1],\\n                                          True))\\n\\n          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\\n                                                   self.target_channel,\\n                                                   kernel_size = 3,\\n                                                   stride = 1,\\n                                                   padding = 1))\\n\\n          self.Upsampple.append(Upsample(self.target_channel, True))\\n\\n        self.convOut = torch.nn.Conv2d(self.target_channel,\\n                                      1,\\n                                      kernel_size = 1,\\n                                      stride = 1,\\n                                      padding = 0)\\n\\n\\n\\n\\n    def forward(self, x):\\n        h = x\\n        FPN_list = []\\n\\n        for idx in range(len(self.resolutions)):\\n\\n          print(idx)\\n          if idx == 0:\\n            h = nonlinearity(self.ConvList[idx](h))\\n\\n          else:\\n            h = nonlinearity(self.ConvList[idx](temp))\\n          temp = h\\n          h = nonlinearity(self.tuneChannels[idx](h))\\n          FPN_list.append(h)\\n        for idx in reversed(range(len(self.resolutions))):\\n          if idx == len(self.resolutions) - 1:\\n            hold = self.Upsampple[idx](FPN_list[idx])\\n          else:\\n            hold = hold + FPN_list[idx]\\n            hold = self.Upsampple[idx](hold)\\n        return self.convOut(hold)\\n\\n\\n\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# fpn = FPN(config)\n","# input_img = input_img.to(torch.float32)"],"metadata":{"id":"dMxcbRCTxr6S","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":41,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# ans = fpn(input_img)\n","# ans.shape"],"metadata":{"id":"EdZsLPfszkOD","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":40,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# input_img.shape"],"metadata":{"id":"TdbnQTKAxzVE","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":40,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","\n","\n","\n","\n","    def forward(self, x):\n","        h = x\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.pool(nonlinearity(self.ConvList[idx](h)))\n","          else:\n","            h = nonlinearity(self.ConvList[idx](temp))\n","\n","          temp = h\n","          h = nonlinearity(self.tuneChannels[idx](h))\n","          FPN_list.append(h)\n","        for i in range(len(FPN_list)):\n","          print(\"i = {}\".format(i))\n","          print((\"FPN_list[i].shape = {}\".format(FPN_list[i].shape)))\n","\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            hold = self.convOut(hold + FPN_list[idx])\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","          else:\n","            hold = hold + FPN_list[idx]\n","            hold = self.Upsampple[idx - 1](hold)\n","          print(\"idx = {}\".format(idx))\n","          print(\"hold.shape = {}\".format(hold.shape))\n","        return hold\n"],"metadata":{"id":"H6dIsb3rx6CP","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":39,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    self.relu = nn.ReLU()\n","    self.bn = nn.BatchNorm2d(out_cha)\n","\n","  def forward(self, depth):\n","    return self.relu(self.bn(self.conv(depth)))"],"metadata":{"id":"esdKjIMr9wcF","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":39,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    self.relu = nn.ReLU()\n","    self.bn = nn.BatchNorm2d(out_cha)\n","\n","  def forward(self, depth):\n","    return self.relu(self.bn(self.conv(depth)))"],"metadata":{"id":"icPtSBzxAKCO","executionInfo":{"status":"ok","timestamp":1709815101892,"user_tz":-480,"elapsed":38,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        # print(enc_channels)\n","        for idx in range(phase2):\n","          print(enc_channels[phase1 + idx])\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","        # print(len(self.phase2_model))\n","\n","\n","\n","\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","\n","        for idx in range(len(self.phase1_model)):\n","          # print(idx)\n","          h = self.phase1_model[idx](h)\n","        for idx in range(len(self.phase2_model)):\n","          # print(idx)\n","          h = self.phase2_model[idx](h)\n","\n","\n","\n","\n","        return h\n"],"metadata":{"id":"Fz0EX77czh3m","executionInfo":{"status":"ok","timestamp":1709815101893,"user_tz":-480,"elapsed":38,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["class depth_decode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        # self.depth_enc_targeted_size = config['model']['depth_enc_targeted_size']\n","        #self.depth_enc_targeted_size = 64\n","        self.ch = config['model']['ch']\n","        # self.ch = 128\n","        self.resolution = config['data']['image_size']\n","        # self.resolution = 256\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        # self.targeted_size = 64\n","        count = 0\n","        tmp = self.targeted_size\n","        while True:\n","          if tmp == self.resolution:\n","            break\n","          count = count + 1\n","          tmp = tmp * 2\n","\n","        self.decode = nn.ModuleList()\n","        in_cha = self.ch\n","        self.relu = nn.ReLU()\n","        for idx in range(count):\n","          out_cha = in_cha / 4\n","          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n","          in_cha = out_cha\n","        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n","                                        1,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","\n","\n","\n","\n","\n","\n","\n","    def forward(self, pred):\n","        for idx in range(len(self.decode)):\n","          pred = self.decode[idx](pred)\n","        pred = self.final_conv(pred)\n","        pred = pred.squeeze(1)\n","\n","\n","\n","\n","\n","        return pred"],"metadata":{"id":"N2Vzf5ZRC8wM","executionInfo":{"status":"ok","timestamp":1709815102302,"user_tz":-480,"elapsed":447,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["depth_test = depth_encode(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z19ZYq4-CFx5","executionInfo":{"status":"ok","timestamp":1709815102303,"user_tz":-480,"elapsed":23,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"e840cbbf-d3f6-4679-eb16-6caf771a8250"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","64\n"]}]},{"cell_type":"code","source":["target_depth = target_depth.to(torch.float32)"],"metadata":{"id":"B2w3wSChCXu9","executionInfo":{"status":"ok","timestamp":1709815102303,"user_tz":-480,"elapsed":19,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["ans = depth_test(target_depth)"],"metadata":{"id":"_G4BCOO2CSjT","executionInfo":{"status":"ok","timestamp":1709815102303,"user_tz":-480,"elapsed":19,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["ans.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Torjy-K1EEBP","executionInfo":{"status":"ok","timestamp":1709815102304,"user_tz":-480,"elapsed":19,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"b5a82660-f715-4cf1-82af-4ed0a7000698"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 256, 64, 64])"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["fpn = FPN(config)\n","# config['model']\n","# test = 1\n","# test2 = test.copy()  FPN_target_C"],"metadata":{"id":"I5Iz580bwASy","executionInfo":{"status":"ok","timestamp":1709815102304,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["config['model']['FPN_target_C']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLPuoz1C39g6","executionInfo":{"status":"ok","timestamp":1709815102949,"user_tz":-480,"elapsed":650,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"c1810e10-67f9-4b6a-ffcb-dbdc91af4674"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["256"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["\n","# ans = fpn(input_img)"],"metadata":{"id":"JnJVQxq1zQO_","executionInfo":{"status":"ok","timestamp":1709815102949,"user_tz":-480,"elapsed":7,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# ans.shape"],"metadata":{"id":"4b-TKchdzcGg","executionInfo":{"status":"ok","timestamp":1709815102949,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# def count_params(model):\n","\n","#   sum = 0\n","#   for param in model.parameters():\n","#     sum = sum + param.numel()\n","#   return sum"],"metadata":{"id":"BIo9er_Y5st3","executionInfo":{"status":"ok","timestamp":1709815102949,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# sum = count_params(fpn)\n","# sum"],"metadata":{"id":"drFJHZFN6Oy2","executionInfo":{"status":"ok","timestamp":1709815102949,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["def get_index_from_list(values, t, x_shape):\n","    batch_size = t.shape[0]\n","    \"\"\"\n","    pick the values from vals\n","    according to the indices stored in `t`\n","    \"\"\"\n","    result = values.gather(-1, t.cpu())\n","    \"\"\"\n","    if\n","    x_shape = (5, 3, 64, 64)\n","        -> len(x_shape) = 4\n","        -> len(x_shape) - 1 = 3\n","\n","    and thus we reshape `out` to dims\n","    (batch_size, 1, 1, 1)\n","\n","    \"\"\"\n","    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"],"metadata":{"id":"xq591w1qfVBe","executionInfo":{"status":"ok","timestamp":1709815102950,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","        noise = torch.randn_like(x_0)\n","        print(\"alphas_cumprod.sqrt().dtype = {}\".format(self.alphas_cumprod.sqrt().dtype) )\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n"],"metadata":{"id":"AeTTDyc0fZOI","executionInfo":{"status":"ok","timestamp":1709815102950,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","        depth_enc_channels = config['model']['depth_enc_channels']\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = Normalize(block_in)\n","        # self.conv_out = torch.nn.Conv2d(block_in,\n","        #                                 out_ch,\n","        #                                 kernel_size=3,\n","        #                                 stride=1,\n","        #                                 padding=1)\n","\n","        self.depth_decode = depth_decode(config)\n","\n","    def forward(self, image, depth, t):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = nonlinearity(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","\n","        img_enc = self.fpn(image)\n","        depth = self.depth_encode(depth)\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","\n","        noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","        noisy_map = noisy_map.to(torch.float32)\n","        noise = noise.to(torch.float32)\n","\n","\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","        # return backbone_input\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        # hs = [self.conv_in(image)]\n","\n","        hs = [self.conv_in(backbone_input)]\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                # return h\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        print(\"h.shape_new = {}\".format(h.shape))\n","        h = self.norm_out(h)\n","        h = nonlinearity(h)\n","        # h = self.conv_out(h)\n","\n","        h = self.depth_decode(h)\n","        return h"],"metadata":{"id":"BW2SNSqY0qys","executionInfo":{"status":"ok","timestamp":1709815102950,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["model = Model(config)\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ParlMz0UnMrq","executionInfo":{"status":"ok","timestamp":1709815108802,"user_tz":-480,"elapsed":5856,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"248c8c74-a121-4257-ef93-95f7947a416b"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","64\n","128\n","32.0\n","here\n","32.0\n","8.0\n","here\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"m3mPoThW1G32","executionInfo":{"status":"ok","timestamp":1709815108803,"user_tz":-480,"elapsed":7,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# BATCH_SIZE = 256\n","NO_EPOCHS = 100\n","PRINT_FREQUENCY = 1\n","LR = 0.001\n","VERBOSE = False\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"],"metadata":{"id":"8VKW8xA20JkQ","executionInfo":{"status":"ok","timestamp":1709815108803,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# 使用DataLoader加载数据集\n","batch_size = 32\n","batch_size = 8\n","batch_size = 2\n","trainloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n","validloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n","# count = 0\n","# 迭代DataLoader获取批次数据\n","# for batch in trainloader:\n","#     count = count + 1\n","#     input_img = batch['img']\n","#     target_depth = batch['depth']\n","#     break\n","# input_img.shape"],"metadata":{"id":"6ZDEnqO21HxI","executionInfo":{"status":"ok","timestamp":1709815108803,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# first epoch\n","\n","import shutil\n","from google.colab import files\n","\n","for epoch in range(NO_EPOCHS):\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'model_checkpoint_epoch_{}.pth'.format(epoch))\n","        source_path = 'model_checkpoint_epoch_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'model_checkpoint_loss_epoch_{}.pth'.format(epoch))\n","        source_path = 'model_checkpoint_loss_epoch_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        # if VERBOSE:\n","        #     with torch.no_grad():\n","        #         plot_noise_prediction(noise[0], predicted_noise[0])\n","        #         plot_noise_distribution(noise, predicted_noise)\n","        # save_path = \"/content/drive/MyDrive/Colab Notebooks/ dtransposed_diffusion_model/weight_save\"\n","        # torch.save(unet.state_dict(), f\"{save_path}epoch_{epoch}.pth\")\n","        # torch.save(unet.state_dict(), f\"epoch: {epoch}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"E67RH3azzzGn","executionInfo":{"status":"error","timestamp":1709815569542,"user_tz":-480,"elapsed":460744,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"b4170fa7-08a8-4189-f471-70aea080eff7"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","---\n","Epoch: 0 | Train Loss 1.0743154168128968 | Val Loss 0.6958552241325379\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","---\n","Epoch: 1 | Train Loss 0.7370443344116211 | Val Loss 0.6442231893539428\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","---\n","Epoch: 2 | Train Loss 0.6342132329940796 | Val Loss 0.8446057915687561\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","---\n","Epoch: 3 | Train Loss 0.9689184427261353 | Val Loss 0.7096362590789795\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","---\n","Epoch: 4 | Train Loss 0.8727844476699829 | Val Loss 0.7400511860847473\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n","i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-fe8283dc9882>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         }\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_checkpoint_epoch_{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_checkpoint_epoch_{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdestination_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Simple_DE/weight_save'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# continue training\n","\n","import shutil\n","from google.colab import files\n","initial = 210\n","NO_EPOCHS = 200 # 要多做幾個epochs\n","load_path = '/content/drive/MyDrive/Colab Notebooks/dtransposed_diffusion_model/weight_save/model_checkpoint_epoch_{}.pth'.format(initial) # 位置要改\n","checkpoint = torch.load(load_path)\n","\n","start = checkpoint['epoch'] + 1\n","model_state_dict = checkpoint['model_state_dict']\n","\n","model.load_state_dict(model_state_dict)\n","\n","for epoch in range(start , start + NO_EPOCHS + 1):\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","    with torch.inference_mode():\n","      for batch in validloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % PRINT_FREQUENCY == 0:\n","        checkpoint = {\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","          'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'model_checkpoint_epoch_{}.pth'.format(epoch))\n","        source_path = 'model_checkpoint_epoch_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/weight_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","          'epoch': epoch,\n","          'valid_loss' : np.mean(mean_epoch_loss_val),\n","          'loss' : np.mean(mean_epoch_loss) # 記得不能存tensor\n","        }\n","\n","        torch.save(checkpoint, 'model_checkpoint_loss_epoch_{}.pth'.format(epoch))\n","        source_path = 'model_checkpoint_loss_epoch_{}.pth'.format(epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/loss_save'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","\n","        print('---')\n","        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        # if VERBOSE:\n","        #     with torch.no_grad():\n","        #         plot_noise_prediction(noise[0], predicted_noise[0])\n","        #         plot_noise_distribution(noise, predicted_noise)\n","        # save_path = \"/content/drive/MyDrive/Colab Notebooks/ dtransposed_diffusion_model/weight_save\"\n","        # torch.save(unet.state_dict(), f\"{save_path}epoch_{epoch}.pth\")\n","        # torch.save(unet.state_dict(), f\"epoch: {epoch}\")"],"metadata":{"id":"hw122Fd47aPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.tensor([1, 2]).to(device)\n","input_img = input_img.to(torch.float32)\n","input_img = input_img.to(device)\n","target_depth = target_depth.to(device)"],"metadata":{"id":"gzpYYwYNzv-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_model = depth_decode(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Wt7K3T_OR3h","executionInfo":{"status":"ok","timestamp":1709805369822,"user_tz":-480,"elapsed":14,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"de9f0ae8-86fc-46cd-df5f-e8c5aaae59d4"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["128\n","32.0\n","here\n","32.0\n","8.0\n","here\n"]}]},{"cell_type":"code","source":["ans = model(input_img, target_depth, t)\n","# print(ans.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKB9Mm7PnnDb","executionInfo":{"status":"ok","timestamp":1709805372544,"user_tz":-480,"elapsed":2733,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"3772b8a6-acb2-4a94-a078-e1d155b65cd9"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["i = 0\n","FPN_list[i].shape = torch.Size([2, 256, 64, 64])\n","i = 1\n","FPN_list[i].shape = torch.Size([2, 256, 32, 32])\n","i = 2\n","FPN_list[i].shape = torch.Size([2, 256, 16, 16])\n","i = 3\n","FPN_list[i].shape = torch.Size([2, 256, 8, 8])\n","idx = 3\n","hold.shape = torch.Size([2, 256, 16, 16])\n","idx = 2\n","hold.shape = torch.Size([2, 256, 32, 32])\n","idx = 1\n","hold.shape = torch.Size([2, 256, 64, 64])\n","alphas_cumprod.sqrt().dtype = torch.float64\n","h.shape_new = torch.Size([2, 128, 64, 64])\n"]}]},{"cell_type":"code","source":["ans.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McBCATp107We","executionInfo":{"status":"ok","timestamp":1709805376755,"user_tz":-480,"elapsed":21,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"5b517c7b-6f0a-4266-cbe6-2337412019db"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 256, 256])"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["print(count_params(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ErUiSYbXGO2_","executionInfo":{"status":"ok","timestamp":1709805517866,"user_tz":-480,"elapsed":346,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"f5c9fcde-39af-4727-d2b0-c44c16f2847e"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["120057044\n"]}]},{"cell_type":"code","source":["def deb(param, str):\n","  print(str + \" = {}\".format(param))"],"metadata":{"id":"Y0ZOue0TXUXA","executionInfo":{"status":"ok","timestamp":1709813005462,"user_tz":-480,"elapsed":443,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["abc =1\n","deb(abc, \"abc\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vt8qH8-DXfLA","executionInfo":{"status":"ok","timestamp":1709813007890,"user_tz":-480,"elapsed":676,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"d186fa2b-7d82-40f6-aa13-6ec83ae2f6d9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["abc = 1\n"]}]},{"cell_type":"code","source":["print(ans1.shape)\n","print(ans2.shape)\n","print(ans3.shape)\n","print(t.device)\n","print(ans1[0][0] == ans2[0][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxXvfKIKoW3r","executionInfo":{"status":"ok","timestamp":1709735065851,"user_tz":-480,"elapsed":8,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"6ebcd5a3-a9bf-4ba9-8fa1-6a5a1cbc2d42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 512, 64, 64])\n","torch.Size([2, 256, 64, 64])\n","torch.Size([2, 256, 64, 64])\n","cuda:0\n","tensor([[True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        ...,\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["class Model_test2(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","\n","        self.ch = ch\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(in_channels,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","\n","        curr_res = resolution\n","        in_ch_mult = (1,)+ch_mult\n","        self.down = nn.ModuleList()\n","        block_in = None\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch*in_ch_mult[i_level]\n","            block_out = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","                block.append(ResnetBlock(in_channels=block_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions-1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","        self.mid.attn_1 = AttnBlock(block_in)\n","        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","                                       out_channels=block_in,\n","                                       temb_channels=self.temb_ch,\n","                                       dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch*ch_mult[i_level]\n","            skip_in = ch*ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks+1):\n","                if i_block == self.num_res_blocks:\n","                    skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","                block.append(ResnetBlock(in_channels=block_in+skip_in,\n","                                         out_channels=block_out,\n","                                         temb_channels=self.temb_ch,\n","                                         dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(AttnBlock(block_in))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","        # end\n","        self.norm_out = Normalize(block_in)\n","        self.conv_out = torch.nn.Conv2d(block_in,\n","                                        out_ch,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","        self.conv_test = torch.nn.Conv2d(3 ,\n","                                          1,\n","                                          kernel_size = 3,\n","                                          stride = 1,\n","                                          padding = 1)\n","\n","    def forward(self, image, depth, t):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch)\n","        temb = self.temb.dense[0](temb)\n","        temb = nonlinearity(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","        after_image = self.conv_test(image)\n","        depth = depth.unsqueeze(1)\n","        output = torch.cat([after_image, depth], dim = 1)\n","        return after_image, output\n","\n","\n","\n","        # diffusion process ---vvv\n","\n","        noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","        # diffusion process ---^^^\n","\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        hs = [self.conv_in(image)]\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h, temb)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = nonlinearity(h)\n","        h = self.conv_out(h)\n","        h = self.conv_merge(h)\n","        h = h.squeeze(dim = 1)\n","        h = torch.cat([h, noisy_map], dim = 0)\n","        # return h, noisy_map, noise"],"metadata":{"id":"9M8oWlI3qtoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model_test2(config)\n","model.to(device)\n","t = torch.tensor([1, 2])\n","input_img = input_img.to(torch.float32)\n","input_img.to(device)\n","t = t.to(torch.float32)\n","t = t.to(device)"],"metadata":{"id":"ktBVmEE5rYRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ans, noisy_map, noise = model(input_img, target_depth, t)\n","ans1, ans2= model(input_img, target_depth, t)\n","# target_depth.shape"],"metadata":{"id":"dA_cQEOPpbwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"ans1.shape = {}\".format(ans1.shape))\n","print(\"ans2.shape = {}\".format(ans2.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nq4r5R1hcOIW","executionInfo":{"status":"ok","timestamp":1709639434868,"user_tz":-480,"elapsed":16,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"5e7bb9c4-7720-4bbb-9c37-c2fae6b8b96d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ans1.shape = torch.Size([2, 1, 256, 256])\n","ans2.shape = torch.Size([2, 2, 256, 256])\n"]}]},{"cell_type":"code","source":["print(\"ans.shape = {}\".format(ans.shape))\n","print(\"noisy_map.shape = {}\".format(noisy_map.shape))\n","print(\"noise.shape = {}\".format(noise.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Uw3DiYo5G_n","executionInfo":{"status":"ok","timestamp":1709632597577,"user_tz":-480,"elapsed":17,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"7f510b9f-f4fb-4083-bbac-a4d1c0d95c69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ans.shape = torch.Size([4, 256, 256])\n","noisy_map.shape = torch.Size([2, 256, 256])\n","noise.shape = torch.Size([2, 256, 256])\n"]}]},{"cell_type":"code","source":["del ans, noisy_map, noise\n"],"metadata":{"id":"Vnnb5GzKpEAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test1 = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[11, 12, 13], [14, 15, 16], [17, 18, 19]]])\n","test1 = test1.unsqueeze(1)\n","test1\n","test2 = torch.tensor([[[21, 22, 23], [24, 25, 26], [27, 28, 29]], [[31, 32, 33], [34, 35, 36], [37, 38, 39]]])\n","test2 = test2.unsqueeze(1)\n","\n","h = torch.cat([test1, test2], dim = 1)\n","h[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KL8Zqh3HDKlx","executionInfo":{"status":"ok","timestamp":1709633022027,"user_tz":-480,"elapsed":485,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"9d599f40-5337-48ba-a1fb-746d5c11f5f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1,  2,  3],\n","         [ 4,  5,  6],\n","         [ 7,  8,  9]],\n","\n","        [[21, 22, 23],\n","         [24, 25, 26],\n","         [27, 28, 29]]])"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["'''\n","這樣concat很怪 應該是希望2x256x256跟2x256x256 concat成為2x2x256x256\n","其中dim為(batch, channel, height, width)\n","channel為2代表把depth和condition concat起來\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"NCvcYZQfq01h","executionInfo":{"status":"ok","timestamp":1709629888855,"user_tz":-480,"elapsed":85,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"7570484b-c3f0-494c-b99e-72cf218ce6c5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n這樣concat很怪 應該是希望2x256x256跟2x256x256 concat成為2x2x256x256\\n其中dim為(batch, channel, height, width)\\nchannel為2代表把depth和condition concat起來\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5DV-aXpu0sc","executionInfo":{"status":"ok","timestamp":1709629888855,"user_tz":-480,"elapsed":83,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"fe652561-fdba-4bcb-8c48-6b08922baac2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model_test2(\n","  (temb): Module(\n","    (dense): ModuleList(\n","      (0): Linear(in_features=128, out_features=512, bias=True)\n","      (1): Linear(in_features=512, out_features=512, bias=True)\n","    )\n","  )\n","  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (down): ModuleList(\n","    (0-1): 2 x Module(\n","      (block): ModuleList(\n","        (0-1): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n","          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (downsample): Downsample(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n","      )\n","    )\n","    (2): Module(\n","      (block): ModuleList(\n","        (0): ResnetBlock(\n","          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): ResnetBlock(\n","          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (downsample): Downsample(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n","      )\n","    )\n","    (3): Module(\n","      (block): ModuleList(\n","        (0-1): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (downsample): Downsample(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n","      )\n","    )\n","    (4): Module(\n","      (block): ModuleList(\n","        (0): ResnetBlock(\n","          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): ResnetBlock(\n","          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList(\n","        (0-1): 2 x AttnBlock(\n","          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (downsample): Downsample(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n","      )\n","    )\n","    (5): Module(\n","      (block): ModuleList(\n","        (0-1): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","    )\n","  )\n","  (mid): Module(\n","    (block_1): ResnetBlock(\n","      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","    (attn_1): AttnBlock(\n","      (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n","      (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (block_2): ResnetBlock(\n","      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (up): ModuleList(\n","    (0): Module(\n","      (block): ModuleList(\n","        (0-2): 3 x ResnetBlock(\n","          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n","          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","    )\n","    (1): Module(\n","      (block): ModuleList(\n","        (0): ResnetBlock(\n","          (norm1): GroupNorm(32, 384, eps=1e-06, affine=True)\n","          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n","          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1-2): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=128, bias=True)\n","          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (upsample): Upsample(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (2): Module(\n","      (block): ModuleList(\n","        (0-1): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): ResnetBlock(\n","          (norm1): GroupNorm(32, 384, eps=1e-06, affine=True)\n","          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (upsample): Upsample(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (3): Module(\n","      (block): ModuleList(\n","        (0): ResnetBlock(\n","          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n","          (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1-2): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=256, bias=True)\n","          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (upsample): Upsample(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (4): Module(\n","      (block): ModuleList(\n","        (0-1): 2 x ResnetBlock(\n","          (norm1): GroupNorm(32, 1024, eps=1e-06, affine=True)\n","          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): ResnetBlock(\n","          (norm1): GroupNorm(32, 768, eps=1e-06, affine=True)\n","          (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList(\n","        (0-2): 3 x AttnBlock(\n","          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (upsample): Upsample(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (5): Module(\n","      (block): ModuleList(\n","        (0-2): 3 x ResnetBlock(\n","          (norm1): GroupNorm(32, 1024, eps=1e-06, affine=True)\n","          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (temb_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (nin_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (attn): ModuleList()\n","      (upsample): Upsample(\n","        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n","  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv_merge): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",")"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"HqYixBFQuB-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAds2WCgLLcz"},"outputs":[],"source":["# to do next 建構兩個模型 一個送image 一個把depth做diffusion動作後兩者合併再送進一個model後算loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRsTUx2WCfAD","executionInfo":{"status":"ok","timestamp":1709629888856,"user_tz":-480,"elapsed":67,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"3409bfb3-1fc9-43fe-b2e2-abbe940315e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1, 2, 2, 4, 4)"]},"metadata":{},"execution_count":49}],"source":["ch_mult = config['model']['ch_mult']\n","tuple(ch_mult)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wha3ICNYDRE7","executionInfo":{"status":"ok","timestamp":1709629888856,"user_tz":-480,"elapsed":49,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"49ae1827-0448-4fbf-92cd-37f4ad2311b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1, 1, 2, 2, 4, 4)"]},"metadata":{},"execution_count":50}],"source":["in_ch_mult = (1,)+tuple(ch_mult)\n","in_ch_mult"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R35zXxyRDX6X","executionInfo":{"status":"ok","timestamp":1709629888856,"user_tz":-480,"elapsed":40,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"a8c97ec9-558c-4666-f5a3-191373aeb267"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 256, 256])"]},"metadata":{},"execution_count":51}],"source":["for batch in data_loader:\n","    count = count + 1\n","    target_img = batch['img']\n","    target_depth = batch['depth']\n","    break\n","target_depth.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7Yytl5JON-c","executionInfo":{"status":"ok","timestamp":1709629888856,"user_tz":-480,"elapsed":31,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"561fc385-21ac-42b6-b80c-47635f3e4f6d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-1., dtype=torch.float16)"]},"metadata":{},"execution_count":52}],"source":["target_depth.min()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydZsYQG6OeNO","executionInfo":{"status":"ok","timestamp":1709629888856,"user_tz":-480,"elapsed":23,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"2b3b1135-8615-4432-cf67-1f0bbddd7793"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 3, 256, 256])"]},"metadata":{},"execution_count":53}],"source":["folder_path = \"/content/drive/MyDrive/Colab Notebooks/Simple_DE/Image_Sample\"\n","test = image_folder_to_tensor(folder_path)\n","test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"G-CCKnNBQ9s2","executionInfo":{"status":"ok","timestamp":1709629888857,"user_tz":-480,"elapsed":14,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"86db39c8-548a-4292-f6e8-de86c711ce9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\\n    def sigmoid(x):\\n        return 1 / (np.exp(-x) + 1)\\n\\n    if beta_schedule == \"quad\":\\n        betas = (\\n            np.linspace(\\n                beta_start ** 0.5,\\n                beta_end ** 0.5,\\n                num_diffusion_timesteps,\\n                dtype=np.float64,\\n            )\\n            ** 2\\n        )\\n    elif beta_schedule == \"linear\":\\n        betas = np.linspace(\\n            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\\n        )\\n    elif beta_schedule == \"const\":\\n        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\\n    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\\n        betas = 1.0 / np.linspace(\\n            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\\n        )\\n    elif beta_schedule == \"sigmoid\":\\n        betas = np.linspace(-6, 6, num_diffusion_timesteps)\\n        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\\n    else:\\n        raise NotImplementedError(beta_schedule)\\n    assert betas.shape == (num_diffusion_timesteps,)\\n    return betas\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}],"source":["'''\n","def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","    def sigmoid(x):\n","        return 1 / (np.exp(-x) + 1)\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aUwPSSaRhwf"},"outputs":[],"source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOYIH2bBO3Wg"},"outputs":[],"source":["class DiffusionModel:\n","    def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n","\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","        noise = torch.randn_like(x_0)\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t, x_0.shape)\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r47uWE40T1FL"},"outputs":[],"source":["diff = DiffusionModel()\n","x_0 = target_depth\n","t = torch.randint(low = 1, high = 300, size = (32,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6fu1-UNUktd","executionInfo":{"status":"error","timestamp":1709629888857,"user_tz":-480,"elapsed":12,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"d39ea18f-3e07-4104-99d8-e5f2e0900449"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-0eed483dac2d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-56-94b98d21b71b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_0, t, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msqrt_one_minus_alphas_cumprod_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_from_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas_cumprod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt_alphas_cumprod_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt_one_minus_alphas_cumprod_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (2) at non-singleton dimension 0"]}],"source":["test1, test2 = diff.forward(x_0, t, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D37hWE-4gbAv"},"outputs":[],"source":["test2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxHQZ5yHSlsv"},"outputs":[],"source":["beta_schedule = \"linear\"\n","start_schedule = 0.0002\n","end_schedule = 0.2\n","timesteps = 1000\n","betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wx4-eG86S2Dw"},"outputs":[],"source":["type(betas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILwWtwLnS8GR"},"outputs":[],"source":["alphas_temp = 1 - betas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbOHQiG6S-xS"},"outputs":[],"source":["alphas = torch.tensor(alphas_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FyGEOQtjTSB3"},"outputs":[],"source":["alphas.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3OlMdojTGmh"},"outputs":[],"source":["alphas_cumprod = torch.cumprod(alphas, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"raG9ZEwuSYDO"},"outputs":[],"source":["diff = DiffusionModel()"]},{"cell_type":"code","source":["# target = torch.tensor([[[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], [[[13, 14], [15, 16]], [[17, 18], [19, 20]], [[21, 22], [23, 24]]], [[[25, 26], [27, 28]], [[29, 30], [31, 32]], [[33, 34], [35, 36]]], [[[37, 38], [39, 40]], [[41, 42], [43, 44]], [[45, 46], [47, 48]]]], dtype = torch.float16)\n","# target.dtype\n","depth_tensor = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]], [[1, 0], [0, 0]]], dtype = torch.float16)\n","# depth_tensor.shape\n","nonzero_mask = depth_tensor != 0\n","zero_mask = depth_tensor == 0\n","mean = depth_tensor[nonzero_mask].mean()\n","std = depth_tensor[nonzero_mask].std()\n","result = (depth_tensor[nonzero_mask] - mean) / std\n","print(depth_tensor,  \"\\n-----\")\n","depth_tensor[nonzero_mask] = result\n","# norm_depth = (depth_tensor - mean) / std\n","# mean = target.mean(dim=(0, 2, 3), keepdim=True)\n","depth_tensor[zero_mask] = -1\n","print(depth_tensor)"],"metadata":{"id":"FJsP5-6mR7jL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([[1, 0, 2], [0, 3, 0], [4, 0, 5]])\n","nonzero_mask = x != 0\n","result = x[nonzero_mask] + 2\n","x[nonzero_mask] = result\n","print(x)"],"metadata":{"id":"ozGSTMfZXRhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["depth_tensor[nonzero_mask]"],"metadata":{"id":"DxJQg5PVSmsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wd_rRA1QSbSb"},"outputs":[],"source":["'''\n","@DATASETS.register_module()\n","class KITTIDataset(Dataset):\n","    \"\"\"KITTI dataset for depth estimation. An example of file structure\n","    is as followed.\n","    .. code-block:: none\n","        ├── data\n","        │   ├── KITTI\n","        │   │   ├── kitti_eigen_train.txt\n","        │   │   ├── kitti_eigen_test.txt\n","        │   │   ├── input (RGB, img_dir)\n","        │   │   │   ├── date_1\n","        │   │   │   ├── date_2\n","        │   │   │   |   ...\n","        │   │   │   |   ...\n","        |   │   ├── gt_depth (ann_dir)\n","        │   │   │   ├── date_drive_number_sync\n","    split file format:\n","    input_image: 2011_09_26/2011_09_26_drive_0002_sync/image_02/data/0000000069.png\n","    gt_depth:    2011_09_26_drive_0002_sync/proj_depth/groundtruth/image_02/0000000069.png\n","    focal:       721.5377 (following the focal setting in BTS, but actually we do not use it)\n","    Args:\n","        pipeline (list[dict]): Processing pipeline\n","        img_dir (str): Path to image directory\n","        ann_dir (str, optional): Path to annotation directory. Default: None\n","        split (str, optional): Split txt file. Split should be specified, only file in the splits will be loaded.\n","        data_root (str, optional): Data root for img_dir/ann_dir. Default: None.\n","        test_mode (bool): If test_mode=True, gt wouldn't be loaded.\n","        depth_scale=256: Default KITTI pre-process. divide 256 to get gt measured in meters (m)\n","        garg_crop=True: Following Adabins, use grag crop to eval results.\n","        eigen_crop=False: Another cropping setting.\n","        min_depth=1e-3: Default min depth value.\n","        max_depth=80: Default max depth value.\n","    \"\"\"\n","\n","\n","    def __init__(self,\n","                 pipeline,\n","                 img_dir,\n","                 ann_dir=None,\n","                 split=None,\n","                 data_root=None,\n","                 test_mode=False,\n","                 depth_scale=256,\n","                 garg_crop=True,\n","                 eigen_crop=False,\n","                 min_depth=1e-3,\n","                 max_depth=80):\n","\n","        self.pipeline = Compose(pipeline)\n","        self.img_dir = img_dir\n","        self.ann_dir = ann_dir\n","        self.split = split\n","        self.data_root = data_root\n","        self.test_mode = test_mode\n","        self.depth_scale = depth_scale\n","        self.garg_crop = garg_crop\n","        self.eigen_crop = eigen_crop\n","        self.min_depth = min_depth # just for evaluate. (crop gt to certain range)\n","        self.max_depth = max_depth # just for evaluate.\n","\n","        # join paths if data_root is specified\n","        if self.data_root is not None:\n","            if not (self.img_dir is None or osp.isabs(self.img_dir)):\n","                self.img_dir = osp.join(self.data_root, self.img_dir)\n","            if not (self.ann_dir is None or osp.isabs(self.ann_dir)):\n","                self.ann_dir = osp.join(self.data_root, self.ann_dir)\n","            if not (self.split is None or osp.isabs(self.split)):\n","                self.split = osp.join(self.data_root, self.split)\n","\n","        # load annotations\n","        self.img_infos = self.load_annotations(self.img_dir, self.ann_dir, self.split)\n","\n","\n","    def __len__(self):\n","        \"\"\"Total number of samples of data.\"\"\"\n","        return len(self.img_infos)\n","\n","    def load_annotations(self, img_dir, ann_dir, split):\n","        \"\"\"Load annotation from directory.\n","        Args:\n","            img_dir (str): Path to image directory\n","            ann_dir (str|None): Path to annotation directory.\n","            split (str|None): Split txt file. Split should be specified, only file in the splits will be loaded.\n","        Returns:\n","            list[dict]: All image info of dataset.\n","        \"\"\"\n","\n","        self.invalid_depth_num = 0\n","        img_infos = []\n","        if split is not None:\n","            with open(split) as f:\n","                for line in f:\n","                    img_info = dict()\n","                    if ann_dir is not None: # benchmark test or unsupervised future\n","                        depth_map = line.strip().split(\" \")[1]\n","                        if depth_map == 'None':\n","                            self.invalid_depth_num += 1\n","                            continue\n","                        img_info['ann'] = dict(depth_map=depth_map)\n","                    img_name = line.strip().split(\" \")[0]\n","                    img_info['filename'] = img_name\n","                    img_infos.append(img_info)\n","        else:\n","            print(\"Split should be specified, NotImplementedError\")\n","            raise NotImplementedError\n","\n","        # github issue:: make sure the same order\n","        img_infos = sorted(img_infos, key=lambda x: x['filename'])\n","        print_log(f'Loaded {len(img_infos)} images. Totally {self.invalid_depth_num} invalid pairs are filtered', logger=get_root_logger())\n","\n","        return img_infos\n","\n","    def get_ann_info(self, idx):\n","        \"\"\"Get annotation by index.\n","        Args:\n","            idx (int): Index of data.\n","        Returns:\n","            dict: Annotation info of specified index.\n","        \"\"\"\n","\n","        return self.img_infos[idx]['ann']\n","\n","    def pre_pipeline(self, results):\n","        \"\"\"Prepare results dict for pipeline.\"\"\"\n","        results['depth_fields'] = []\n","        results['img_prefix'] = self.img_dir\n","        results['depth_prefix'] = self.ann_dir\n","        results['depth_scale'] = self.depth_scale\n","\n","        results['cam_intrinsic_dict'] = {\n","            '2011_09_26' : [[7.215377e+02, 0.000000e+00, 6.095593e+02, 4.485728e+01],\n","                            [0.000000e+00, 7.215377e+02, 1.728540e+02, 2.163791e-01],\n","                            [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.745884e-03]],\n","            '2011_09_28' : [[7.070493e+02, 0.000000e+00, 6.040814e+02, 4.575831e+01],\n","                            [0.000000e+00, 7.070493e+02, 1.805066e+02, -3.454157e-01],\n","                            [0.000000e+00, 0.000000e+00, 1.000000e+00, 4.981016e-03]],\n","            '2011_09_29' : [[7.183351e+02, 0.000000e+00, 6.003891e+02, 4.450382e+01],\n","                            [0.000000e+00, 7.183351e+02, 1.815122e+02, -5.951107e-01],\n","                            [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.616315e-03]],\n","            '2011_09_30' : [[7.070912e+02, 0.000000e+00, 6.018873e+02, 4.688783e+01],\n","                            [0.000000e+00, 7.070912e+02, 1.831104e+02, 1.178601e-01],\n","                            [0.000000e+00, 0.000000e+00, 1.000000e+00, 6.203223e-03]],\n","            '2011_10_03' : [[7.188560e+02, 0.000000e+00, 6.071928e+02, 4.538225e+01],\n","                            [0.000000e+00, 7.188560e+02, 1.852157e+02, -1.130887e-01],\n","                            [0.000000e+00, 0.000000e+00, 1.000000e+00, 3.779761e-03]],\n","        }\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Get training/test data after pipeline.\n","        Args:\n","            idx (int): Index of data.\n","        Returns:\n","            dict: Training/test data (with annotation if `test_mode` is set\n","                False).\n","        \"\"\"\n","\n","        if self.test_mode:\n","            return self.prepare_test_img(idx)\n","        else:\n","            return self.prepare_train_img(idx)\n","\n","    def prepare_train_img(self, idx):\n","        \"\"\"Get training data and annotations after pipeline.\n","        Args:\n","            idx (int): Index of data.\n","        Returns:\n","            dict: Training data and annotation after pipeline with new keys\n","                introduced by pipeline.\n","        \"\"\"\n","\n","        img_info = self.img_infos[idx]\n","        ann_info = self.get_ann_info(idx)\n","        results = dict(img_info=img_info, ann_info=ann_info)\n","        self.pre_pipeline(results)\n","        return self.pipeline(results)\n","\n","    def prepare_test_img(self, idx):\n","        \"\"\"Get testing data after pipeline.\n","        Args:\n","            idx (int): Index of data.\n","        Returns:\n","            dict: Testing data after pipeline with new keys introduced by\n","                pipeline.\n","        \"\"\"\n","\n","        img_info = self.img_infos[idx]\n","        results = dict(img_info=img_info)\n","        self.pre_pipeline(results)\n","        return self.pipeline(results)\n","\n","    def format_results(self, results, imgfile_prefix=None, indices=None, **kwargs):\n","        \"\"\"Place holder to format result to dataset specific output.\"\"\"\n","        results[0] = (results[0] * self.depth_scale).astype(np.uint16)\n","        return results\n","\n","    def get_gt_depth_maps(self):\n","        \"\"\"Get ground truth depth maps for evaluation.\"\"\"\n","        for img_info in self.img_infos:\n","            depth_map = osp.join(self.ann_dir, img_info['ann']['depth_map'])\n","            depth_map_gt = np.asarray(Image.open(depth_map), dtype=np.float32) / self.depth_scale\n","            yield depth_map_gt\n","\n","    def eval_kb_crop(self, depth_gt):\n","        \"\"\"Following Adabins, Do kb crop for testing\"\"\"\n","        height = depth_gt.shape[0]\n","        width = depth_gt.shape[1]\n","        top_margin = int(height - 352)\n","        left_margin = int((width - 1216) / 2)\n","        depth_cropped = depth_gt[top_margin: top_margin + 352, left_margin: left_margin + 1216]\n","        depth_cropped = np.expand_dims(depth_cropped, axis=0)\n","        return depth_cropped\n","\n","    def eval_mask(self, depth_gt):\n","        \"\"\"Following Adabins, Do grag_crop or eigen_crop for testing\"\"\"\n","        depth_gt = np.squeeze(depth_gt)\n","        valid_mask = np.logical_and(depth_gt > self.min_depth, depth_gt < self.max_depth)\n","        if self.garg_crop or self.eigen_crop:\n","            gt_height, gt_width = depth_gt.shape\n","            eval_mask = np.zeros(valid_mask.shape)\n","\n","            if self.garg_crop:\n","                eval_mask[int(0.40810811 * gt_height):int(0.99189189 * gt_height),\n","                          int(0.03594771 * gt_width):int(0.96405229 * gt_width)] = 1\n","\n","            elif self.eigen_crop:\n","                eval_mask[int(0.3324324 * gt_height):int(0.91351351 * gt_height),\n","                          int(0.0359477 * gt_width):int(0.96405229 * gt_width)] = 1\n","        valid_mask = np.logical_and(valid_mask, eval_mask)\n","        valid_mask = np.expand_dims(valid_mask, axis=0)\n","        return valid_mask\n","\n","    def pre_eval(self, preds, indices):\n","        \"\"\"Collect eval result from each iteration.\n","        Args:\n","            preds (list[torch.Tensor] | torch.Tensor): the depth estimation.\n","            indices (list[int] | int): the prediction related ground truth\n","                indices.\n","        Returns:\n","            list[torch.Tensor]: (area_intersect, area_union, area_prediction,\n","                area_ground_truth).\n","        \"\"\"\n","        # In order to compat with batch inference\n","        if not isinstance(indices, list):\n","            indices = [indices]\n","        if not isinstance(preds, list):\n","            preds = [preds]\n","\n","        pre_eval_results = []\n","        pre_eval_preds = []\n","\n","        for i, (pred, index) in enumerate(zip(preds, indices)):\n","            depth_map = osp.join(self.ann_dir,\n","                               self.img_infos[index]['ann']['depth_map'])\n","\n","            depth_map_gt = np.asarray(Image.open(depth_map), dtype=np.float32) / self.depth_scale\n","            depth_map_gt = self.eval_kb_crop(depth_map_gt)\n","            valid_mask = self.eval_mask(depth_map_gt)\n","\n","            eval = metrics(depth_map_gt[valid_mask],\n","                           pred[valid_mask],\n","                           min_depth=self.min_depth,\n","                           max_depth=self.max_depth)\n","\n","            pre_eval_results.append(eval)\n","\n","            # save prediction results\n","            pre_eval_preds.append(pred)\n","\n","        return pre_eval_results, pre_eval_preds\n","\n","    def evaluate(self, results, metric='eigen', logger=None, **kwargs):\n","        \"\"\"Evaluate the dataset.\n","        Args:\n","            results (list[tuple[torch.Tensor]] | list[str]): per image pre_eval\n","                 results or predict depth map for computing evaluation\n","                 metric.\n","            logger (logging.Logger | None | str): Logger used for printing\n","                related information during evaluation. Default: None.\n","        Returns:\n","            dict[str, float]: Default metrics.\n","        \"\"\"\n","        metric = [\"a1\", \"a2\", \"a3\", \"abs_rel\", \"rmse\", \"log_10\", \"rmse_log\", \"silog\", \"sq_rel\"]\n","\n","        eval_results = {}\n","        # test a list of files\n","        if mmcv.is_list_of(results, np.ndarray) or mmcv.is_list_of(\n","                results, str):\n","            gt_depth_maps = self.get_gt_depth_maps()\n","            ret_metrics = eval_metrics(\n","                gt_depth_maps,\n","                results)\n","        # test a list of pre_eval_results\n","        else:\n","            ret_metrics = pre_eval_to_metrics(results)\n","\n","        ret_metric_names = []\n","        ret_metric_values = []\n","        for ret_metric, ret_metric_value in ret_metrics.items():\n","            ret_metric_names.append(ret_metric)\n","            ret_metric_values.append(ret_metric_value)\n","\n","        num_table = len(ret_metrics) // 9\n","        for i in range(num_table):\n","            names = ret_metric_names[i*9: i*9 + 9]\n","            values = ret_metric_values[i*9: i*9 + 9]\n","\n","            # summary table\n","            ret_metrics_summary = OrderedDict({\n","                ret_metric: np.round(np.nanmean(ret_metric_value), 4)\n","                for ret_metric, ret_metric_value in zip(names, values)\n","            })\n","\n","            # for logger\n","            summary_table_data = PrettyTable()\n","            for key, val in ret_metrics_summary.items():\n","                summary_table_data.add_column(key, [val])\n","\n","            print_log('Summary:', logger)\n","            print_log('\\n' + summary_table_data.get_string(), logger=logger)\n","\n","        # each metric dict\n","        for key, value in ret_metrics.items():\n","            eval_results[key] = value\n","\n","        return eval_results\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mE2aOnBYk9pf"},"outputs":[],"source":["# 看一下paper怎麼normalize depth map的"]},{"cell_type":"code","source":["config['model']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHzdfFFLZv8Y","executionInfo":{"status":"ok","timestamp":1709640015229,"user_tz":-480,"elapsed":359,"user":{"displayName":"黃勁元","userId":"11372567893123424236"}},"outputId":"50309b0b-06d4-4794-b892-5b92ed4d83bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'FPN_conv_res': [128, 256, 512, 1024],\n"," 'type': 'simple',\n"," 'in_channels': 3,\n"," 'out_ch': 3,\n"," 'ch': 128,\n"," 'ch_mult': [1, 1, 2, 2, 4, 4],\n"," 'num_res_blocks': 2,\n"," 'attn_resolutions': [16],\n"," 'dropout': 0.0,\n"," 'var_type': 'fixedsmall',\n"," 'ema_rate': 0.999,\n"," 'ema': True,\n"," 'resamp_with_conv': True}"]},"metadata":{},"execution_count":57}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1y__02XKgPhWFn60Gqf9SvnHB6bYtAgYe","authorship_tag":"ABX9TyP/GiFu22g/Az4IJvN/qvnd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}