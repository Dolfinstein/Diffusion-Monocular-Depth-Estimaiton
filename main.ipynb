{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOJNBZ03QF+Gy7sVL+qOmA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W3QYseCtDZOG"},"outputs":[],"source":["import torch\n","import cv2\n","import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Simple_DE') # this path is the path of the current .ipynb\n","import numpy as np\n","import shutil\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","import random\n","from google.colab import files\n","import sys\n","import time\n","from torch.utils.data import random_split\n","import matplotlib.pyplot as plt\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","source":["path = 'Model/config/depth_analysis.pth' # 讀取depth的統計數字\n","# path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config/depth_analysis.pth'\n","\n","check = torch.load(path)\n","total_sum = check['total_sum']\n","DEPTH_NONZERO = check['total_nonzero']\n","DEPTH_MEAN = check['total_mean']\n","DEPTH_STD = check['total_std']\n","del check"],"metadata":{"id":"fSmKxE9sDcXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Model.functions.functions import load_config\n","file_path = 'Model/config/config.yml'\n","# file_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config/config.yml'\n","config = load_config(file_path)"],"metadata":{"id":"kNrQX4h-Dfdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_size = (config['data']['image_size'], config['data']['image_size'])"],"metadata":{"id":"Ez1cxgI1Dg2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Model.Model1.model1_script import Model\n","\n","model = Model(config)\n","model = model.to(device)"],"metadata":{"id":"2X3dsg81Dibo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BATCH_SIZE = 256\n","\n","NO_LARGE_EPOCHS = 10\n","save_frequency = 5\n","LR = 0.001\n","VERBOSE = False\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip_shuffle'\n","batch_size = 32\n","train_val_rate = 0.99\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"],"metadata":{"id":"eeWVYM5YDmVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Model.Data_Process.data_processing import create_dataset_large_epoch, CustomDataset, image_loader_to_tensor, depth_loader_to_tensor"],"metadata":{"id":"tCxnZN1_Dnvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#debug epoch\n","\n","epoch = 0\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","for large_epoch in range(1, NO_LARGE_EPOCHS + 1):\n","\n","\n","\n","    for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","        epoch += 1\n","\n","\n","        # output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","        # custom_dataset = CustomDataset(output_image, output_depth)\n","        # train_size = int(train_val_rate * len(custom_dataset))\n","        # val_size = len(custom_dataset) - train_size\n","        # train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","        # trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        # validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        start_time = time.time()\n","        mean_epoch_loss = []\n","        mean_epoch_loss_val = []\n","        epoch_gradient = {}\n","        for batch in trainloader:\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","            pred_depth = model(input_img, target_depth, t)\n","\n","            optimizer.zero_grad()\n","            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            mean_epoch_loss.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","            #---gradient---vvv\n","            for name, param in model.named_parameters():\n","                if param.grad == None:\n","                    epoch_gradient[name + 'zero'] = 1\n","                elif name not in epoch_gradient:\n","                    epoch_gradient[name] = param.grad.clone()\n","                else:\n","                    epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","        with torch.inference_mode():\n","            for batch in validloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                input_img = batch['img'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss_val.append(val_loss.item())\n","\n","        if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","            checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'now' : now,\n","                'random_list' : random_list,\n","                'gradients' : epoch_gradient\n","            }\n","\n","            torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model0_weight'\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_weight'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(exe_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_loss'\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model0_loss'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n"],"metadata":{"id":"QiimlAdpDzaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# continue training debug\n","large_epoch = 1\n","epoch = 10\n","# load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_weight/weight_{}_{}.pth'.format(large_epoch, epoch)\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle'\n","load_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model0_weight/weight_{}_{}.pth'.format(large_epoch, epoch)\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","check_length = len(sorted(os.listdir(data_path)))\n","checkpoint = torch.load(load_path, map_location=torch.device(device))\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","\n","\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","\n","\n","for now in range(epoch, 51): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","    epoch += 1\n","    start_time = time.time()\n","    mean_epoch_loss = []\n","    mean_epoch_loss_val = []\n","    epoch_gradient = {}\n","    for batch in trainloader:\n","        t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","        input_img = batch['img'].to(torch.float32).to(device)\n","        target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","        input_img = image_loader_to_tensor(input_img)\n","        target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","        pred_depth = model(input_img, target_depth, t)\n","\n","        optimizer.zero_grad()\n","        loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","        mean_epoch_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","        #---gradient---vvv\n","            for name, param in model.named_parameters():\n","                if param.grad == None:\n","                    epoch_gradient[name + 'zero'] = 1\n","                elif name not in epoch_gradient:\n","                    epoch_gradient[name] = param.grad.clone()\n","                else:\n","                    epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","    with torch.inference_mode():\n","        for batch in validloader:\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","            pred_depth = model(input_img, target_depth, t)\n","\n","            val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            mean_epoch_loss_val.append(val_loss.item())\n","\n","    if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","        checkpoint = {\n","            'large_epoch' : large_epoch,\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","            'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","            'now' : now,\n","            'random_list' : random_list,\n","            'gradients' : epoch_gradient\n","        }\n","\n","        torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(exe_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_loss'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","    print('---')\n","    print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","    print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","'''\n"],"metadata":{"id":"EUqV6JqED1um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# first epoch\n","epoch = 0\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","check_length = len(sorted(os.listdir(data_path)))\n","for large_epoch in range(1, NO_LARGE_EPOCHS + 1):\n","\n","    random_list = []\n","\n","    for idx in range(check_length):\n","        random_list.append(idx)\n","\n","    random.shuffle(random_list)\n","\n","    for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","        epoch += 1\n","        output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","        custom_dataset = CustomDataset(output_image, output_depth)\n","        train_size = int(train_val_rate * len(custom_dataset))\n","        val_size = len(custom_dataset) - train_size\n","        train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","        start_time = time.time()\n","        mean_epoch_loss = []\n","        mean_epoch_loss_val = []\n","        epoch_gradient = {}\n","        for batch in trainloader:\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","            pred_depth = model(input_img, target_depth, t)\n","\n","            optimizer.zero_grad()\n","            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            mean_epoch_loss.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","            #---gradient---vvv\n","            for name, param in model.named_parameters():\n","                if param.grad == None:\n","                    epoch_gradient[name + 'zero'] = 1\n","                elif name not in epoch_gradient:\n","                    epoch_gradient[name] = param.grad.clone()\n","                else:\n","                    epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","        with torch.inference_mode():\n","            for batch in validloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                input_img = batch['img'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss_val.append(val_loss.item())\n","\n","        if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","            checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'now' : now,\n","                'random_list' : random_list,\n","                'gradients' : epoch_gradient\n","            }\n","\n","            torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight'\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_weight'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(exe_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_loss'\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_loss'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n"],"metadata":{"id":"9vyCcxyLD4Fd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# continue training\n","large_epoch = 2\n","epoch = 23\n","# load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_weight/weight_{}_{}.pth'.format(large_epoch, epoch)\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle'\n","load_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight/weight_{}_{}.pth'.format(large_epoch, epoch)\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","check_length = len(sorted(os.listdir(data_path)))\n","checkpoint = torch.load(load_path, map_location=torch.device(device))\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","if (checkpoint['now'] == check_length - 1):\n","    large_epoch_start = large_epoch + 1\n","    del large_epoch\n","\n","    for large_epoch in range(large_epoch_start, NO_LARGE_EPOCHS + 1):\n","        random_list = []\n","        for idx in range(check_length):\n","            random_list.append(idx)\n","        random.shuffle(random_list)\n","\n","        for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","            epoch += 1\n","            output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","            custom_dataset = CustomDataset(output_image, output_depth)\n","            train_size = int(train_val_rate * len(custom_dataset))\n","            val_size = len(custom_dataset) - train_size\n","            train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","            trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            start_time = time.time()\n","            mean_epoch_loss = []\n","            mean_epoch_loss_val = []\n","            epoch_gradient = {}\n","            for batch in trainloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","                input_img = batch['img'].to(torch.float32).to(device)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                optimizer.zero_grad()\n","                loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss.append(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                #---gradient---vvv\n","                for name, param in model.named_parameters():\n","                    if param.grad == None:\n","                        epoch_gradient[name + 'zero'] = 1\n","                    elif name not in epoch_gradient:\n","                        epoch_gradient[name] = param.grad.clone()\n","                    else:\n","                        epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","            with torch.inference_mode():\n","                for batch in validloader:\n","                    t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                    input_img = batch['img'].to(torch.float32).to(device)\n","                    target_depth = batch['depth'].to(torch.float32).to(device)\n","                    input_img = image_loader_to_tensor(input_img)\n","                    target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                    pred_depth = model(input_img, target_depth, t)\n","\n","                    val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                    mean_epoch_loss_val.append(val_loss.item())\n","\n","            if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","                checkpoint = {\n","                    'large_epoch' : large_epoch,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                    'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                    'valid_loss' : np.mean(mean_epoch_loss_val),\n","                    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                    'now' : now,\n","                    'random_list' : random_list,\n","                    'gradients' : epoch_gradient\n","                }\n","\n","                torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","\n","                #---計算時間---vvv\n","                end_time = time.time()\n","                exe_time = end_time - start_time\n","                hours, remainder = divmod(exe_time, 3600)\n","                minutes, seconds = divmod(remainder, 60)\n","                #---計算時間---^^^\n","\n","                #-----以下是存loss的---vvv\n","                checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'time' : exe_time\n","                }\n","\n","                torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_loss'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","                #-----以下是存loss的---^^^\n","\n","            print('---')\n","            print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","            print(\"time = {}:{}:{}\".format(hours, minutes, seconds))\n","\n","else:\n","    large_epoch_start = large_epoch\n","    del large_epoch\n","\n","    for large_epoch in range(large_epoch_start, NO_LARGE_EPOCHS + 1):\n","        if large_epoch == large_epoch_start:\n","            now_start = checkpoint['now'] + 1\n","            random_list = checkpoint['random_list']\n","        else:\n","            now_start = 0\n","            random_list = []\n","            for idx in range(check_length):\n","                random_list.append(idx)\n","            random.shuffle(random_list)\n","\n","\n","        for now in range(now_start, check_length):\n","            epoch += 1\n","            output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","            custom_dataset = CustomDataset(output_image, output_depth)\n","            train_size = int(train_val_rate * len(custom_dataset))\n","            val_size = len(custom_dataset) - train_size\n","            train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","            trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","            start_time = time.time()\n","            mean_epoch_loss = []\n","            mean_epoch_loss_val = []\n","            epoch_gradient = {}\n","            for batch in trainloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","                input_img = batch['img'].to(torch.float32).to(device)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                optimizer.zero_grad()\n","                loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss.append(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                #---gradient---vvv\n","                for name, param in model.named_parameters():\n","                    if param.grad == None:\n","                        epoch_gradient[name + 'zero'] = 1\n","                    elif name not in epoch_gradient:\n","                        epoch_gradient[name] = param.grad.clone()\n","                    else:\n","                        epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","            with torch.inference_mode():\n","                for batch in validloader:\n","                    t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                    input_img = batch['img'].to(torch.float32).to(device)\n","                    target_depth = batch['depth'].to(torch.float32).to(device)\n","                    input_img = image_loader_to_tensor(input_img)\n","                    target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                    pred_depth = model(input_img, target_depth, t)\n","\n","                    val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                    mean_epoch_loss_val.append(val_loss.item())\n","\n","            if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","                checkpoint = {\n","                    'large_epoch' : large_epoch,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                    'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                    'valid_loss' : np.mean(mean_epoch_loss_val),\n","                    'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                    'now' : now,\n","                    'random_list' : random_list,\n","                    'gradients' : epoch_gradient\n","                }\n","\n","                torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","                source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","                destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_weight'\n","\n","\n","                # save them to the google drive\n","                shutil.copy(source_path, destination_path)\n","\n","            #---計算時間---vvv\n","            end_time = time.time()\n","            exe_time = end_time - start_time\n","            hours, remainder = divmod(exe_time, 3600)\n","            minutes, seconds = divmod(remainder, 60)\n","            #---計算時間---^^^\n","\n","            #-----以下是存loss的---vvv\n","            checkpoint = {\n","            'large_epoch' : large_epoch,\n","            'epoch': epoch,\n","            'valid_loss' : np.mean(mean_epoch_loss_val),\n","            'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","            'time' : exe_time\n","            }\n","\n","            torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/model1_loss'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","            #-----以下是存loss的---^^^\n","\n","            print('---')\n","            print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","            print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n","\n","\n","\n"],"metadata":{"id":"GOwDlw_ED_Nf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"izOakBwBED-7"},"execution_count":null,"outputs":[]}]}