{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Xi9icUpOy4oG"},"outputs":[],"source":["import torch\n","import cv2\n","import os\n","# os.chdir('/content/drive/MyDrive/Colab Notebooks/Simple_DE') # this path is the path of the current .ipynb\n","import numpy as np\n","import shutil\n","from google.colab.patches import cv2_imshow\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import math\n","from PIL import Image\n","import torch.nn as nn\n","import yaml\n","import random\n","from google.colab import files\n","import sys\n","import time\n","from torch.utils.data import random_split\n","import matplotlib.pyplot as plt\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 檢查是否有可用的 CUDA 設備（通常是顯卡，支援 GPU 運算），如果有，就將 device 變數設置為 \"cuda\"，否則設置為 \"cpu\"。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoV0RrSyy_TE"},"outputs":[],"source":["path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/config/depth_analysis.pth' # 讀取depth的統計數字\n","# path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config/depth_analysis.pth'\n","\n","check = torch.load(path)\n","total_sum = check['total_sum']\n","DEPTH_NONZERO = check['total_nonzero']\n","DEPTH_MEAN = check['total_mean']\n","DEPTH_STD = check['total_std']\n","del check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqXGUAL5zMwv"},"outputs":[],"source":["def deb(param, str):\n","  print(str + \" = {}\".format(param))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI-UVeaqzQP7"},"outputs":[],"source":["def load_config(file_path):\n","    with open(file_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config"]},{"cell_type":"code","source":["def Normalize(in_channels):\n","    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"],"metadata":{"id":"YKqYrm__ocPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhXlGZQyzMRS"},"outputs":[],"source":["\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Model/config/config.yml'\n","# file_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Model/config/config.yml'\n","config = load_config(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w31fjSeVzSw8"},"outputs":[],"source":["target_size = (config['data']['image_size'], config['data']['image_size'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YilIexAz54W"},"outputs":[],"source":["def count_params(model):\n","  sum = 0\n","  for param in model.parameters():\n","    sum = sum + param.numel()\n","  return sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKLoeVL8z6YR"},"outputs":[],"source":["def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n","\n","\n","    if beta_schedule == \"quad\":\n","        betas = (\n","            np.linspace(\n","                beta_start ** 0.5,\n","                beta_end ** 0.5,\n","                num_diffusion_timesteps,\n","                dtype=np.float64,\n","            )\n","            ** 2\n","        )\n","    elif beta_schedule == \"linear\":\n","        betas = np.linspace(\n","            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"const\":\n","        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n","    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n","        betas = 1.0 / np.linspace(\n","            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n","        )\n","    elif beta_schedule == \"sigmoid\":\n","        betas = np.linspace(-6, 6, num_diffusion_timesteps)\n","        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\n","    else:\n","        raise NotImplementedError(beta_schedule)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJsIkhlAz-Co"},"outputs":[],"source":["def compute_alpha(beta, t): # t給tensor 一維的\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoXoptpe0ALw"},"outputs":[],"source":["def get_timestep_embedding(timesteps, embedding_dim):\n","\n","    assert len(timesteps.shape) == 1\n","\n","    half_dim = embedding_dim // 2\n","    emb = math.log(10000) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","    emb = emb.to(device=timesteps.device)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:  # zero pad\n","        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n","    return emb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIF0PZWb0EY6"},"outputs":[],"source":["def get_index_from_list(values, t, x_shape):\n","    batch_size = t.shape[0]\n","    \"\"\"\n","    pick the values from vals\n","    according to the indices stored in `t`\n","    \"\"\"\n","    result = values.gather(-1, t)\n","    \"\"\"\n","    if\n","    x_shape = (5, 3, 64, 64)\n","        -> len(x_shape) = 4\n","        -> len(x_shape) - 1 = 3\n","\n","    and thus we reshape `out` to dims\n","    (batch_size, 1, 1, 1)\n","\n","    \"\"\"\n","    return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWOqvB84zZDq"},"outputs":[],"source":["class Upsample(nn.Module): # this\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(in_channels,  # this conv let the size unchanged\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPZtUuTCzbql"},"outputs":[],"source":["class Downsample(nn.Module):\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        in_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geK8FL3Uzdwh"},"outputs":[],"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n","                 dropout, temb_channels=512):\n","        super().__init__()\n","        self.temb_channels = temb_channels\n","        self.in_channels = in_channels\n","        self.Lrelu = nn.ELU()\n","        # self.Lrelu = nonlinearity\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 =nn.BatchNorm2d(in_channels)     # 這裡上面define的Normalize有點像是class的感覺\n","        self.conv1 = torch.nn.Conv2d(in_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        self.temb_proj = torch.nn.Linear(temb_channels,\n","                                         out_channels)\n","        self.norm2 = nn.BatchNorm2d(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout) # param為機率\n","        self.conv2 = torch.nn.Conv2d(out_channels, # size unchanged\n","                                     out_channels,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(in_channels,   # size unchanged\n","                                                     out_channels,\n","                                                     kernel_size=3,\n","                                                     stride=1,\n","                                                     padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(in_channels,    # size unchanged\n","                                                    out_channels,\n","                                                    kernel_size=1,\n","                                                    stride=1,\n","                                                    padding=0)\n","\n","\n","    def forward(self, x, temb):\n","        h = x\n","        h = self.norm1(h)    # normalize\n","\n","        h = self.Lrelu(h)  # sigmoid\n","        h = self.conv1(h)    # channel become out_channel\n","\n","        h = h + self.temb_proj(self.Lrelu(temb))[:, :, None, None] # 後面加入None增加空的維度，針對temb_proj(nonlinearity(temb))使用，使其可以跟h相加，但是是使用broadcasting的方式\n","        h = self.norm2(h)\n","        h = self.Lrelu(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:  # 如果inchannel和outchannel不同需要把輸入值channel也調整成一樣，用上conv2D，若inchannel和outchannel一樣就直接加\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x+h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU0J_egpzfp_"},"outputs":[],"source":["class AttnBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = nn.BatchNorm2d(in_channels)\n","        self.q = torch.nn.Conv2d(in_channels, # in_cha == out_cha and the kernel size = 1, and size unchanged\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.k = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.v = torch.nn.Conv2d(in_channels,\n","                                 in_channels,\n","                                 kernel_size=1,\n","                                 stride=1,\n","                                 padding=0)\n","        self.proj_out = torch.nn.Conv2d(in_channels,\n","                                        in_channels,\n","                                        kernel_size=1,\n","                                        stride=1,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape # (batch, channel, height, width)\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)   # b,hw,c.  # 此兩變換(reshape + permute)詳細情形如下格，簡單來說最外圈還是每一個data(一張照片)，\n","                                # 往內一圈則是把該資料的所有channel(rgb)的同個位置放在一起\n","        k = k.reshape(b, c, h*w)  # b,c,hw # 單一此變換則是外圈是data，向內一圈則是該data一個channel內所有的值\n","        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        # 注意這邊是q * k，是把不同channels的同個位置變成vector然後內積，這樣跟cnn最大的差別是cnn只會在同一個區塊做相關性，attention卻在channels中的每個位置會相互做相關性\n","        w_ = w_ * (int(c)**(-0.5)) # 為何不是 * (int(h * w) ** (-0.5))?\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h*w)\n","        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n","        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = torch.bmm(v, w_)\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x+h_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WN78MuC9zi4f"},"outputs":[],"source":["class DownsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(in_channels,  # halves the size\n","                                        out_channels,\n","                                        kernel_size=3,\n","                                        stride=2,\n","                                        padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) # 此動作相當於在每個圖片的channel的右邊下面pad 0\n","\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPoqOAEmzk4C"},"outputs":[],"source":["class UpsampleFPN(nn.Module):\n","    def __init__(self, in_channels, out_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(int(in_channels),  # this conv let the size unchanged\n","                                        int(out_channels),\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\") # double the size\n","        if self.with_conv:\n","            x = self.conv(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9FaEA3kzoo8"},"outputs":[],"source":["class FPN(nn.Module):  # 此處預設每次的resolutions都是上一次的一半 第一次的resolution是原圖的 1/4\n","    def __init__(self, config):\n","        super().__init__()\n","        resolutions = config['model']['FPN_conv_res'].copy()\n","\n","        # resolutions = [64, 128, 256, 512]\n","        self.resolutions = resolutions.copy()\n","\n","        # self.target_channel = int(resolutions[0] / 2)\n","        self.target_channel = config['model']['FPN_target_C']\n","\n","        resolutions.insert(0, 3)\n","        # self.resolutions = resolutions # which is list\n","        self.ConvList = nn.ModuleList()\n","        self.tuneChannels = nn.ModuleList()\n","        self.Upsampple = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.Lrelu = nn.ELU()\n","        # self.Lrelu = nonlinearity\n","        # self.bn0 = nn.BatchNorm2d(self.resolutions[0])\n","        self.bn0 = nn.BatchNorm2d(resolutions[0])\n","        for idx in range(len(resolutions) - 1):\n","          self.ConvList.append(DownsampleFPN(resolutions[idx],\n","                                          resolutions[idx + 1],\n","                                          True))\n","\n","          self.tuneChannels.append(torch.nn.Conv2d(resolutions[idx + 1],\n","                                                   self.target_channel,\n","                                                   kernel_size = 3,\n","                                                   stride = 1,\n","                                                   padding = 1))\n","          if idx != len(resolutions) - 2:\n","            self.Upsampple.append(Upsample(self.target_channel, True))\n","\n","        self.convOut = torch.nn.Conv2d(self.target_channel,\n","                                      self.target_channel,\n","                                      kernel_size = 1,\n","                                      stride = 1,\n","                                      padding = 0)\n","        self.norm_seq = nn.ModuleList()\n","        for idx in range((len(self.resolutions) - 2) * 2 + 2):\n","\n","          self.norm_seq.append(nn.BatchNorm2d(self.target_channel))\n","\n","\n","\n","\n","    def forward(self, x):\n","        print(\"before FPN image.shape = {}\".format(x.shape))\n","        h = self.bn0(x)\n","        print(\"FPN bn0 shape = {}\".format(h.shape))\n","        FPN_list = []\n","\n","        for idx in range(len(self.resolutions)):\n","\n","\n","          if idx == 0:\n","            h = self.ConvList[idx](h)\n","            print(\"self.ConvList[{}](h) shape = {}\".format(idx, h.shape))\n","            h = self.pool(self.Lrelu(h))\n","            print(\"self.pool(self.Lrelu(h)) shape = {}\".format(h.shape))\n","          else:\n","            h = self.Lrelu(self.ConvList[idx](temp))\n","\n","          temp = h\n","          print(\"temp for idx = {} has shape : {}\".format(idx, temp.shape))\n","          h = self.Lrelu(self.tuneChannels[idx](h))\n","          print('self.Lrelu(self.tuneChannels[{}](h)) has shape : {}'.format(idx, h.shape))\n","          FPN_list.append(h)\n","        count = 0\n","        for idx in reversed(range(len(self.resolutions))):\n","          if idx == 0:\n","            print(\"for idx = {}, hold before self.norm_seq[count] : {}\".format(idx, hold.shape))\n","            hold = self.norm_seq[count](hold)\n","            print(\"for idx = {}, hold after self.norm_seq[count] : {}\".format(idx, hold.shape))\n","            count += 1\n","            print(\"----------------\")\n","            hold = self.convOut(hold + self.norm_seq[count](FPN_list[idx]))\n","            print(\"FPN_list[{}].shape = {}\".format(idx, FPN_list[idx].shape))\n","            print(\"self.norm_seq[{}](FPN_list[{}]).shape = {}\".format(count, idx, self.norm_seq[count](FPN_list[idx]).shape))\n","            print(\"for idx = {}, hold after self.convOut(hold + self.norm_seq[count](FPN_list[idx])) : {}\".format(idx, hold.shape))\n","            print(\"----------------\")\n","            break\n","          if idx == len(self.resolutions) - 1:\n","            hold = self.Upsampple[idx - 1](FPN_list[idx])\n","            print(\"----------------\")\n","            print(\"FPN_LIST[{}] has shape : {}\".format(idx, FPN_list[idx].shape))\n","            print(\"for idx = {} hold.shape : {}\".format(idx, hold.shape))\n","            print(\"----------------\")\n","          else:\n","            print(\"idx = {}, hold before self.norm_seq[count](hold) = {}\".format(idx, hold.shape))\n","            hold = self.norm_seq[count](hold)\n","            print(\"idx = {}, hold after self.norm_seq[count](hold) = {}\".format(idx, hold.shape))\n","            count += 1\n","\n","            hold = hold + self.norm_seq[count](FPN_list[idx])\n","            print(\"idx = {}, hold after hold + self.norm_seq[count](FPN_list[idx]) = {}\".format(idx, hold.shape))\n","            count = count + 1\n","            hold = self.Upsampple[idx - 1](hold)\n","            print(\"idx = {}, hold after self.Upsampple[idx - 1](hold) = {}\".format(idx, hold.shape))\n","\n","        return hold\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2F97mg67zpFx"},"outputs":[],"source":["class depth_phase1_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = DownsampleFPN(in_cha, out_cha, True)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.ELU()\n","  def forward(self, depth):\n","\n","    return self.Lrelu(self.bn(self.conv(depth)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eum5sN3lzq7O"},"outputs":[],"source":["class depth_phase2_block(nn.Module):\n","  def __init__(self, in_cha, out_cha):\n","    super().__init__()\n","    self.conv = torch.nn.Conv2d(in_cha,\n","                                out_cha,\n","                                kernel_size = 3,\n","                                stride = 1,\n","                                padding = 1)\n","    # self.Lrelu = nonlinearity\n","    self.bn = nn.BatchNorm2d(out_cha)\n","    self.Lrelu = nn.ELU()\n","  def forward(self, depth):\n","    return self.Lrelu(self.bn(self.conv(depth)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WeVd3M9zuNs"},"outputs":[],"source":["class depth_encode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        resolution = config['data']['image_size']\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        self.enc_channels = self.config['model']['depth_enc_channels'].copy()\n","        # targeted_size = 64\n","        # enc_channels = [4, 16, 64, 256]\n","        enc_channels = self.enc_channels.copy()\n","        enc_channels.insert(0, 1)\n","\n","        phase1 = 0\n","        while True:\n","          phase1 = phase1 + 1\n","          resolution = resolution / 2\n","          if resolution == self.targeted_size:\n","            break\n","        phase2 = len(self.enc_channels) - phase1\n","        self.phase1_model = nn.ModuleList()\n","        self.phase2_model = nn.ModuleList()\n","\n","        for idx in range(phase1):\n","          self.phase1_model.append(depth_phase1_block(enc_channels[idx],\n","                                                      enc_channels[idx + 1]))\n","        for idx in range(phase2):\n","          self.phase2_model.append(depth_phase2_block(enc_channels[phase1 + idx],\n","                                                      enc_channels[phase1 + idx + 1]))\n","\n","    def forward(self, depth):\n","        h = depth.unsqueeze(1)\n","        print(\"depth.unsqueeze(1).shape = {}\".format(h.shape))\n","        # print(depth.dtype)\n","        for idx in range(len(self.phase1_model)):\n","          h = self.phase1_model[idx](h)\n","          print(\"self.phase1_model[{}](h).shape = {}\".format(idx, h.shape))\n","\n","        for idx in range(len(self.phase2_model)):\n","          h = self.phase2_model[idx](h)\n","          print(\"self.phase2_model[{}](h).shape = {}\".format(idx, h.shape))\n","\n","        return h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uXsGuZ7zwiy"},"outputs":[],"source":["\n","class depth_decode(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config.copy()\n","        self.ch = config['model']['ch']\n","        # self.ch = 128\n","        self.resolution = config['data']['image_size']\n","        # self.resolution = 256\n","        self.targeted_size = self.config['model']['depth_enc_targeted_size']\n","        # self.targeted_size = 64\n","        count = 0\n","        tmp = self.targeted_size\n","        while True:\n","          if tmp == self.resolution:\n","            break\n","          count = count + 1\n","          tmp = tmp * 2\n","\n","        self.decode = nn.ModuleList()\n","        in_cha = self.ch\n","        self.Lrelu = nn.ELU()\n","        # self.Lrelu = nonlinearity\n","        for idx in range(count):\n","          out_cha = in_cha / 4\n","          self.decode.append(UpsampleFPN(in_cha, out_cha, True))\n","          in_cha = out_cha\n","        self.final_conv = torch.nn.Conv2d(int(out_cha),  # this conv let the size unchanged\n","                                        1,\n","                                        kernel_size=3,\n","                                        stride=1,\n","                                        padding=1)\n","\n","\n","\n","\n","\n","\n","\n","\n","    def forward(self, pred):\n","        for idx in range(len(self.decode)):\n","          pred = self.decode[idx](pred)\n","          print(\"pred = self.decode[{}](pred).shape = {}\".format(idx, pred.shape))\n","        pred = self.final_conv(pred)\n","        print(\"self.final_conv(pred).shape = {}\".format(pred.shape))\n","        pred = pred.squeeze(1)\n","        print(\"pred.squeeze(1).shape = {}\".format(pred.shape))\n","\n","\n","\n","\n","\n","        return pred\n"]},{"cell_type":"code","source":["# class depth_decode(nn.Module):\n","#     def __init__(self, config):\n","#         super(depth_decode, self).__init__()\n","#         self.conv1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n","#         self.bn1 = nn.BatchNorm2d(64)\n","#         self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n","#         self.bn2 = nn.BatchNorm2d(32)\n","#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n","#         self.bn3 = nn.BatchNorm2d(16)\n","#         self.conv4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n","#         self.bn4 = nn.BatchNorm2d(8)\n","#         self.conv5 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n","#         self.bn5 = nn.BatchNorm2d(4)\n","#         self.conv6 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=3, stride=1, padding=1)\n","#         self.bn6 = nn.BatchNorm2d(2)\n","#         self.conv7 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)\n","#         self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","\n","\n","#     def forward(self, x):\n","#         x = self.conv1(x)\n","#         x = self.bn1(x)\n","#         x = self.Lrelu(x)\n","#         x = self.conv2(x)\n","#         x = self.bn2(x)\n","#         x = self.Lrelu(x)\n","#         x = self.conv3(x)\n","#         x = self.bn3(x)\n","#         x = self.Lrelu(x)\n","#         x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n","#         x = self.conv4(x)\n","#         x = self.bn4(x)\n","#         x = self.Lrelu(x)\n","#         x = self.conv5(x)\n","#         x = self.bn5(x)\n","#         x = self.Lrelu(x)\n","#         x = self.conv6(x)\n","#         x = self.bn6(x)\n","#         x = self.Lrelu(x)\n","#         x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n","#         x = self.conv7(x)\n","#         x = x.squeeze(1)\n","\n","\n","#         return x"],"metadata":{"id":"7NgHEb1mx6Rg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lFxnZ9gz01z"},"outputs":[],"source":["class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod =      [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","            alphas_cumprod_prev = [1,   0.9, 0.9 * 0.8, 0.9 * 0.8 * 0.7]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0).to(device)\n","        self.alphas_cumprod_prev = torch.cat(\n","            [torch.ones(1).to(device), self.alphas_cumprod[:-1]], dim=0\n","        )\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n","\n","    def backward_DDIM(self, model, image, weight_path, skip, eta = 0.1):\n","        checkpoint = torch.load(weight_path, map_location = torch.device(device))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        with torch.inference_mode():\n","\n","\n","            # seq = range(0, timesteps, skip)   # 這是原版\n","            # seq_next = [-1] + list(seq[:-1])\n","\n","            seq = range(1, timesteps, skip)\n","            seq_next = [0] + list(seq[:-1])\n","\n","\n","\n","            # seq =      [1, 2, 3]\n","            # seq_next = [0, 1, 2]\n","            x0_preds = []\n","            depth = torch.randn([image.shape[0], image.shape[-1], image.shape[-1]]).to(torch.float32)\n","            print(depth.dtype)\n","            xs = [depth]\n","            n = image.shape[0]\n","\n","\n","\n","            for i, j in zip(reversed(seq), reversed(seq_next)):\n","                t = (torch.ones(n) * i).to(image.device)\n","                next_t = (torch.ones(n) * j).to(image.device)\n","\n","                at = self.alphas_cumprod.gather(-1, t.to(torch.int64))\n","\n","                at_next = self.alphas_cumprod.gather(-1, next_t.to(torch.int64))\n","\n","                xt = xs[-1].to(device) # x_t\n","                x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\n","\n","                x0_preds.append(x0_t.to(device))\n","                et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\n","                c1 = (\n","                    eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\n","                                                                                                # can make the sampling process of x_{t - 1} become identical\n","                                                                                                # as ddpm\n","                )\n","                c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","                xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\n","                xs.append(xt_next.to(device))\n","            return xs, x0_preds\n","\n","\n","\n","\n","    @staticmethod\n","    def get_index_from_list(values, t, x_shape):\n","        batch_size = t.shape[0]\n","        \"\"\"\n","        pick the values from vals\n","        according to the indices stored in `t`\n","        \"\"\"\n","        result = values.gather(-1, t.cpu())\n","        \"\"\"\n","        if\n","        x_shape = (5, 3, 64, 64)\n","            -> len(x_shape) = 4\n","            -> len(x_shape) - 1 = 3\n","\n","        and thus we reshape `out` to dims\n","        (batch_size, 1, 1, 1)\n","\n","        \"\"\"\n","        return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLPO2vjz0pDa"},"outputs":[],"source":["# # original model with deform attn\n","\n","class Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","        num_res_blocks = config['model']['num_res_blocks']\n","        attn_resolutions = config['model']['attn_resolutions']\n","        dropout = config['model']['dropout']\n","        in_channels = config['model']['in_channels']\n","        resolution = config['data']['image_size']\n","        resamp_with_conv = config['model']['resamp_with_conv']\n","        num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","        depth_enc_channels = config['model']['depth_enc_channels']\n","        if config['model']['type'] == 'bayesian':\n","            self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","        self.fpn = FPN(config)\n","        self.ch = ch\n","        # ch_mult = (1, 1, 2, 2)\n","        ch_mult = (1, 2)\n","        self.temb_ch = self.ch*4\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # timestep embedding\n","        self.temb = nn.Module()\n","        self.temb.dense = nn.ModuleList([\n","            torch.nn.Linear(self.ch,\n","                            self.temb_ch),\n","            torch.nn.Linear(self.temb_ch,\n","                            self.temb_ch),\n","        ])\n","\n","        '''\n","        # timestep embedding for diffusion---vvv\n","        self.temb.diff1 = nn.Linear(1, 1)\n","        self.temb.diff2 = nn.Linear(1, 1)\n","        # timestep embedding for diffusion---^^^\n","        '''\n","\n","\n","        self.depth_encode = depth_encode(config)\n","\n","        # diffusion process ---vvv\n","        self.beta_schedule = config['diffusion']['beta_schedule']\n","        self.start_schedule = config['diffusion']['beta_start']\n","        self.end_schedule = config['diffusion']['beta_end']\n","        self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","        self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","        # diffusion process ---^^^\n","\n","\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","                                       self.ch,\n","                                       kernel_size=3,\n","                                       stride=1,\n","                                       padding=1)\n","\n","\n","\n","        self.norm_out = nn.BatchNorm2d(block_in)\n","\n","        self.depth_decode = depth_decode(config)\n","        self.Lrelu = nn.ELU()\n","    def forward(self, image, depth, t, sampling = False):\n","        # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","        # timestep embedding\n","        temb = get_timestep_embedding(t, self.ch).to(device)\n","        temb = self.temb.dense[0](temb)\n","        temb = self.Lrelu(temb)\n","        temb = self.temb.dense[1](temb)\n","\n","\n","        print(\"image before fpn : {}\".format(image.shape))\n","        print(\"-----------here is fpn----------vvv\")\n","        img_enc = self.fpn(image)\n","        print(\"-----------here is fpn----------^^^\")\n","        print(\"img_enc.shape = {}\".format(img_enc.shape))\n","        depth = depth.to(torch.float32)\n","        # if sampling == False:\n","        print(\"depth before depth_encode : {}\".format(depth.shape))\n","        print(\"-----------here is depth_encode----------vvv\")\n","        depth = self.depth_encode(depth)\n","        print(\"-----------here is depth_encode----------^^^\")\n","        print(\"depth after depth_encode : {}\".format(depth.shape))\n","\n","\n","        # depth = depth.unsqueeze(1)\n","        # return img_enc, depth\n","\n","        # diffusion process ---vvv\n","        if sampling == False:\n","            noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","            noisy_map = noisy_map.to(torch.float32)\n","            noise = noise.to(torch.float32)\n","        else:\n","            noisy_map = depth\n","\n","\n","\n","        # diffusion process ---^^^\n","\n","\n","        # concat img_enc and noisy_map\n","        print(\"noisy_map shape : {}\".format(noisy_map.shape))\n","        backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","        print(\"backbone_input.shape = {}\".format(backbone_input.shape))\n","        # return backbone_input\n","\n","\n","\n","        # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","        # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","        # hs = [self.conv_in(image)]\n","        print(\"-----------here is main----------vvv\")\n","        hs = [self.conv_in(backbone_input)]\n","        print(\"self.conv_in(backbone_input).shape = {}\".format(hs[0].shape))\n","\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","\n","                h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","                print(\"self.down[{}].block[{}](hs[-1], temb).shape = {}\".format(i_level, i_block, h.shape))\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                    print(\"self.down[{}}].attn[{}](h).shape = {}\".format(i_level, i_block, h.shape))\n","                hs.append(h)\n","            if i_level != self.num_resolutions-1:\n","                h_temp = self.down[i_level].downsample(hs[-1])\n","                hs.append(h_temp)\n","                print(\"self.down[{}].downsample(hs[-1]).shape\".format(i_level, h_temp.shape))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h, temb)\n","        print(\"self.mid.block_1(h, temb).shape = {}\".format(h.shape))\n","        h = self.mid.attn_1(h)\n","        print(\"self.mid.attn_1(h).shape = {}\".format(h.shape))\n","        h = self.mid.block_2(h, temb)\n","        print(\"self.mid.block_2(h, temb).shape = {}\".format(h.shape))\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks+1):\n","                print(\"i_level = {}, i_block = {}, h before cat = {}\".format(i_level, i_block, h.shape))\n","                print(\"i_level = {}, i_block = {}, hs.pop() before cat = {}\".format(i_level, i_block, hs[-1].shape))\n","                h_cat = torch.cat([h, hs.pop()], dim=1)\n","                print(\"h_cat.shape : {}\".format(h_cat.shape))\n","                h = self.up[i_level].block[i_block](h_cat, temb) # u-net的cat down\n","                print(\"self.up[{}].block[{}](h_cat, temb).shape = {}\".format(i_level, i_block, h.shape))\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","                    print(\"self.up[{}].attn[{}](h).shape = {}\".format(i_level, i_block, h.shape))\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","                print(\"self.up[{}].upsample(h).shape = {}\".format(i_level, h.shape))\n","\n","        # end\n","        print(\"-----------here is main----------^^^\")\n","\n","        # ---take out in this exp---vvv\n","        h = self.norm_out(h)\n","        print(\"self.norm_out(h).shape = {}\".format(h.shape))\n","        # ---take out in this exp---^^^\n","\n","        h = self.Lrelu(h)\n","        print(\"-----------here is depth_decode----------vvv\")\n","        h = self.depth_decode(h)\n","        print(\"-----------here is depth_decode----------^^^\")\n","        print(\"self.depth_decode(h).shape = {}\".format(h.shape))\n","        return h\n"]},{"cell_type":"code","source":["# class Model(nn.Module): # develop : last normalization change to group norm\n","#     def __init__(self, config):\n","#         super().__init__()\n","#         self.config = config\n","#         ch, out_ch, ch_mult = config['model']['ch'], config['model']['out_ch'], tuple(config['model']['ch_mult'])\n","#         num_res_blocks = config['model']['num_res_blocks']\n","#         attn_resolutions = config['model']['attn_resolutions']\n","#         dropout = config['model']['dropout']\n","#         in_channels = config['model']['in_channels']\n","#         resolution = config['data']['image_size']\n","#         resamp_with_conv = config['model']['resamp_with_conv']\n","#         num_timesteps = config['diffusion']['num_diffusion_timesteps']\n","#         depth_enc_channels = config['model']['depth_enc_channels']\n","#         if config['model']['type'] == 'bayesian':\n","#             self.logvar = nn.Parameter(torch.zeros(num_timesteps))\n","#         self.fpn = FPN(config)\n","#         self.ch = ch\n","\n","#         self.temb_ch = self.ch*4\n","#         self.num_resolutions = len(ch_mult)\n","#         self.num_res_blocks = num_res_blocks\n","#         self.resolution = resolution\n","#         self.in_channels = in_channels\n","\n","#         # timestep embedding\n","#         self.temb = nn.Module()\n","#         self.temb.dense = nn.ModuleList([\n","#             torch.nn.Linear(self.ch,\n","#                             self.temb_ch),\n","#             torch.nn.Linear(self.temb_ch,\n","#                             self.temb_ch),\n","#         ])\n","\n","#         '''\n","#         # timestep embedding for diffusion---vvv\n","#         self.temb.diff1 = nn.Linear(1, 1)\n","#         self.temb.diff2 = nn.Linear(1, 1)\n","#         # timestep embedding for diffusion---^^^\n","#         '''\n","\n","\n","#         self.depth_encode = depth_encode(config)\n","\n","#         # diffusion process ---vvv\n","#         self.beta_schedule = config['diffusion']['beta_schedule']\n","#         self.start_schedule = config['diffusion']['beta_start']\n","#         self.end_schedule = config['diffusion']['beta_end']\n","#         self.timesteps = config['diffusion']['num_diffusion_timesteps']\n","#         self.diffusion_process = DiffusionModel(self.beta_schedule, self.start_schedule, self.end_schedule, self.timesteps)\n","#         # diffusion process ---^^^\n","\n","\n","\n","#         # downsampling\n","#         self.conv_in = torch.nn.Conv2d(depth_enc_channels[-1] * 2,\n","#                                        self.ch,\n","#                                        kernel_size=3,\n","#                                        stride=1,\n","#                                        padding=1)\n","# # nonlinear\n","#         curr_res = resolution\n","#         in_ch_mult = (1,)+ch_mult\n","#         self.down = nn.ModuleList()\n","#         block_in = None\n","#         for i_level in range(self.num_resolutions):\n","#             block = nn.ModuleList()\n","#             attn = nn.ModuleList()\n","#             block_in = ch*in_ch_mult[i_level]\n","#             block_out = ch*ch_mult[i_level]\n","#             for i_block in range(self.num_res_blocks):\n","\n","#                 block.append(ResnetBlock(in_channels=block_in,\n","#                                          out_channels=block_out,\n","#                                          temb_channels=self.temb_ch,\n","#                                          dropout=dropout))\n","#                 block_in = block_out\n","#                 if curr_res in attn_resolutions:\n","#                     attn.append(AttnBlock(block_in))\n","#             down = nn.Module()\n","#             down.block = block\n","#             down.attn = attn\n","#             if i_level != self.num_resolutions-1:\n","#                 down.downsample = Downsample(block_in, resamp_with_conv)\n","#                 curr_res = curr_res // 2\n","#             self.down.append(down)\n","\n","#         # middle\n","#         self.mid = nn.Module()\n","#         self.mid.block_1 = ResnetBlock(in_channels=block_in,\n","#                                        out_channels=block_in,\n","#                                        temb_channels=self.temb_ch,\n","#                                        dropout=dropout)\n","#         self.mid.attn_1 = AttnBlock(block_in)\n","#         self.mid.block_2 = ResnetBlock(in_channels=block_in,\n","#                                        out_channels=block_in,\n","#                                        temb_channels=self.temb_ch,\n","#                                        dropout=dropout)\n","\n","#         # upsampling\n","#         self.up = nn.ModuleList()\n","#         for i_level in reversed(range(self.num_resolutions)):\n","#             block = nn.ModuleList()\n","#             attn = nn.ModuleList()\n","#             block_out = ch*ch_mult[i_level]\n","#             skip_in = ch*ch_mult[i_level]\n","#             for i_block in range(self.num_res_blocks+1):\n","#                 if i_block == self.num_res_blocks:\n","#                     skip_in = ch*in_ch_mult[i_level] # 最後一個block時inchannel數可能變少\n","#                 block.append(ResnetBlock(in_channels=block_in+skip_in,\n","#                                          out_channels=block_out,\n","#                                          temb_channels=self.temb_ch,\n","#                                          dropout=dropout))\n","#                 block_in = block_out\n","#                 if curr_res in attn_resolutions:\n","#                     attn.append(AttnBlock(block_in))\n","#             up = nn.Module()\n","#             up.block = block\n","#             up.attn = attn\n","#             if i_level != 0:\n","#                 up.upsample = Upsample(block_in, resamp_with_conv)\n","#                 curr_res = curr_res * 2\n","#             self.up.insert(0, up)  # prepend to get consistent order, (0, up)代表在ModuleList()的0位置插入up這個Module\n","\n","#         # end\n","#         self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=False)\n","\n","#         self.depth_decode = depth_decode(config)\n","#         self.Lrelu = nn.LeakyReLU(negative_slope=0.1)\n","#     def forward(self, image, depth, t, sampling = False):\n","#         # assert x.shape[2] == x.shape[3] == self.resolution # to check if the height and width are the same with the resolution\n","\n","#         # timestep embedding\n","#         temb = get_timestep_embedding(t, self.ch).to(device)\n","#         temb = self.temb.dense[0](temb)\n","#         temb = self.Lrelu(temb)\n","#         temb = self.temb.dense[1](temb)\n","\n","\n","\n","#         img_enc = self.fpn(image)\n","#         depth = depth.to(torch.float32)\n","#         # if sampling == False:\n","#         depth = self.depth_encode(depth)\n","\n","\n","#         # depth = depth.unsqueeze(1)\n","#         # return img_enc, depth\n","\n","#         # diffusion process ---vvv\n","#         if sampling == False:\n","#             noisy_map, noise = self.diffusion_process.forward(depth, t, device = t.device)\n","#             noisy_map = noisy_map.to(torch.float32)\n","#             noise = noise.to(torch.float32)\n","#         else:\n","#             noisy_map = depth\n","\n","\n","\n","#         # diffusion process ---^^^\n","\n","\n","#         # concat img_enc and noisy_map\n","#         backbone_input = torch.cat([noisy_map, img_enc], dim = 1)\n","#         # return backbone_input\n","\n","\n","\n","#         # downsampling down 裡面有好幾個元素，每個元素包含block(resblock), attn, downsample，其中attn只有幾個元素會有，downsample除了最後一個元素以外都有\n","#         # 整個流程就是把x送進conv2D 然後送進down裡面經過resblock和部份attn downsample(conv2D)\n","#         # hs = [self.conv_in(image)]\n","\n","#         hs = [self.conv_in(backbone_input)]\n","\n","#         for i_level in range(self.num_resolutions):\n","#             for i_block in range(self.num_res_blocks):\n","\n","#                 h = self.down[i_level].block[i_block](hs[-1], temb) # h 如果可以是attn就是attn 不然就是res, note that h是把值喂進去模塊後的值，要把hs的最後跟time embedded送進去\n","#                 if len(self.down[i_level].attn) > 0:\n","#                     h = self.down[i_level].attn[i_block](h)\n","#                 hs.append(h)\n","#             if i_level != self.num_resolutions-1:\n","#                 hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","#         # middle\n","#         h = hs[-1]\n","#         h = self.mid.block_1(h, temb)\n","#         h = self.mid.attn_1(h)\n","#         h = self.mid.block_2(h, temb)\n","\n","#         # upsampling\n","#         for i_level in reversed(range(self.num_resolutions)):\n","#             for i_block in range(self.num_res_blocks+1):\n","#                 h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb) # u-net的cat down\n","#                 if len(self.up[i_level].attn) > 0:\n","#                     h = self.up[i_level].attn[i_block](h)\n","#             if i_level != 0:\n","#                 h = self.up[i_level].upsample(h)\n","\n","#         # end\n","#         temp_h = h\n","#         # ---take out in this exp---vvv\n","#         h = self.norm_out(h)\n","#         # ---take out in this exp---^^^\n","#         # h = h + temp_h\n","#         h = self.Lrelu(h)\n","#         h = self.depth_decode(h)\n","#         return h"],"metadata":{"id":"Gr87T0_6jje0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2PqVLemzVa6"},"outputs":[],"source":["model = Model(config)\n","model = model.to(device)"]},{"cell_type":"code","source":["'''\n","# developing\n","class DiffusionModel:\n","    def __init__(self, beta_schedule = 'linear', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n","        self.start_schedule = start_schedule\n","        self.end_schedule = end_schedule\n","        self.timesteps = timesteps\n","\n","        \"\"\"\n","        if\n","            betas = [0.1, 0.2, 0.3, ...]\n","        then\n","            alphas = [0.9, 0.8, 0.7, ...]\n","            alphas_cumprod =      [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n","            alphas_cumprod_prev = [1,   0.9, 0.9 * 0.8, 0.9 * 0.8 * 0.7]\n","\n","\n","        \"\"\"\n","        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\n","        self.betas = torch.tensor(betas)\n","\n","\n","        self.alphas = 1 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0).to(device)\n","        self.alphas_cumprod_prev = torch.cat(\n","            [torch.ones(1).to(device), self.alphas_cumprod[:-1]], dim=0\n","        )\n","    def forward(self, x_0, t, device):\n","        \"\"\"\n","        x_0: (B, C, H, W)\n","        t: (B,)\n","        \"\"\"\n","\n","\n","        noise = torch.randn_like(x_0)\n","\n","        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\n","\n","        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\n","\n","        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n","        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n","\n","        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\n","\n","    def backward_DDIM(self, model, image, weight_path, skip, eta = 0.1):\n","        checkpoint = torch.load(weight_path, map_location = torch.device(device))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        with torch.inference_mode():\n","\n","\n","            # seq = range(0, timesteps, skip)   # 這是原版\n","            # seq_next = [-1] + list(seq[:-1])\n","\n","            seq = range(1, timesteps, skip)\n","            seq_next = [0] + list(seq[:-1])\n","\n","\n","\n","            # seq =      [1, 2, 3]\n","            # seq_next = [0, 1, 2]\n","            x0_preds = []\n","            depth = torch.randn([image.shape[0], image.shape[-1], image.shape[-1]]).to(torch.float32)\n","            print(depth.dtype)\n","            xs = [depth]\n","            n = image.shape[0]\n","\n","\n","\n","            for i, j in zip(reversed(seq), reversed(seq_next)):\n","                t = (torch.ones(n) * i).to(image.device)\n","                next_t = (torch.ones(n) * j).to(image.device)\n","\n","                at = self.alphas_cumprod.gather(-1, t.to(torch.int64))\n","\n","                at_next = self.alphas_cumprod.gather(-1, next_t.to(torch.int64))\n","\n","                xt = xs[-1].to(device) # x_t\n","                x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\n","\n","                x0_preds.append(x0_t.to(device))\n","                et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\n","                c1 = (\n","                    eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\n","                                                                                                # can make the sampling process of x_{t - 1} become identical\n","                                                                                                # as ddpm\n","                )\n","                c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","                xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\n","                xs.append(xt_next.to(device))\n","            return xs, x0_preds\n","    def backward_DDPM(self, model, image, weight_path): # 代入的圖片希望是(batch, 256, 256, 3) batch也可以是1 只是希望是四維的 且是tensor\n","        image = image.to(torch.float32).to(device)\n","        image = image_loader_to_tensor(image)\n","        checkpoint = torch.load(weight_path, map_location = torch.device(device))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        n = image.shape[0]\n","        pure_noise = torch.randn((image.shape[0], image.shape[-1], image.shape[-1]))\n","        xt = [pure_noise]\n","        with torch.inference_mode():\n","            for i, j in zip(reversed(range(1, self.timesteps + 1)), range(0, self.timesteps)):\n","                noise = torch.randn((image.shape[0], image.shape[-1], image.shape[-1]))\n","                t = (torch.ones(n) * i).to(image.device)\n","                t_next = (torch.ones(n) * j).to(image.device)\n","                depth_tmp = xt[-1]\n","                ans = model(image, depth_tmp, t, sampling = True)\n","                betas_t = get_index_from_list(self.betas, t, depth_tmp.shape)\n","                sqrt_alphas = get_index_from_list(self.alphas, t, depth_tmp.shape)\n","\n","                sqrt_next_alphas_cumprod = get_index_from_list(torch.sqrt(self.alphas_cumprod), t_next, depth_tmp.shape)\n","                one_minus_next_alphas_prod = get_index_from_list(1 - self.alphas_cumprod, t_next, depth_tmp.shape)\n","                one_minus_alphas_prod = get_index_from_list(1 - self.alphas_cumprod, t, depth_tmp.shape)\n","                xt.append((sqrt_alphas * (one_minus_next_alphas_prod) * depth_tmp + sqrt_next_alphas_cumprod * betas_t * ans) / (one_minus_alphas_prod) + betas_t * noise)\n","        return xt\n","'''"],"metadata":{"id":"SoyWoB0UbYlS","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"38a53e72-4579-4594-aad8-d8fb3da5a1d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# developing\\nclass DiffusionModel:\\n    def __init__(self, beta_schedule = \\'linear\\', start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\\n        self.start_schedule = start_schedule\\n        self.end_schedule = end_schedule\\n        self.timesteps = timesteps\\n\\n        \"\"\"\\n        if\\n            betas = [0.1, 0.2, 0.3, ...]\\n        then\\n            alphas = [0.9, 0.8, 0.7, ...]\\n            alphas_cumprod =      [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\\n            alphas_cumprod_prev = [1,   0.9, 0.9 * 0.8, 0.9 * 0.8 * 0.7]\\n\\n\\n        \"\"\"\\n        betas = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = timesteps)\\n        self.betas = torch.tensor(betas)\\n\\n\\n        self.alphas = 1 - self.betas\\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0).to(device)\\n        self.alphas_cumprod_prev = torch.cat(\\n            [torch.ones(1).to(device), self.alphas_cumprod[:-1]], dim=0\\n        )\\n    def forward(self, x_0, t, device):\\n        \"\"\"\\n        x_0: (B, C, H, W)\\n        t: (B,)\\n        \"\"\"\\n\\n\\n        noise = torch.randn_like(x_0)\\n\\n        sqrt_alphas_cumprod_t = get_index_from_list(self.alphas_cumprod.sqrt(), t.to(torch.int64), x_0.shape)\\n\\n        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t.to(torch.int64), x_0.shape)\\n\\n        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\\n        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\\n\\n        return mean + variance, noise.to(device) # mean為x_0乘以alpha bar, variance 為 noise 乘以 1-alpha bar\\n\\n    def backward_DDIM(self, model, image, weight_path, skip, eta = 0.1):\\n        checkpoint = torch.load(weight_path, map_location = torch.device(device))\\n        model.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n        with torch.inference_mode():\\n\\n\\n            # seq = range(0, timesteps, skip)   # 這是原版\\n            # seq_next = [-1] + list(seq[:-1])\\n\\n            seq = range(1, timesteps, skip)\\n            seq_next = [0] + list(seq[:-1])\\n\\n\\n\\n            # seq =      [1, 2, 3]\\n            # seq_next = [0, 1, 2]\\n            x0_preds = []\\n            depth = torch.randn([image.shape[0], image.shape[-1], image.shape[-1]]).to(torch.float32)\\n            print(depth.dtype)\\n            xs = [depth]\\n            n = image.shape[0]\\n\\n\\n\\n            for i, j in zip(reversed(seq), reversed(seq_next)):\\n                t = (torch.ones(n) * i).to(image.device)\\n                next_t = (torch.ones(n) * j).to(image.device)\\n\\n                at = self.alphas_cumprod.gather(-1, t.to(torch.int64))\\n\\n                at_next = self.alphas_cumprod.gather(-1, next_t.to(torch.int64))\\n\\n                xt = xs[-1].to(device) # x_t\\n                x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\\n\\n                x0_preds.append(x0_t.to(device))\\n                et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\\n                c1 = (\\n                    eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\\n                                                                                                # can make the sampling process of x_{t - 1} become identical\\n                                                                                                # as ddpm\\n                )\\n                c2 = ((1 - at_next) - c1 ** 2).sqrt()\\n                xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\\n                xs.append(xt_next.to(device))\\n            return xs, x0_preds\\n    def backward_DDPM(self, model, image, weight_path): # 代入的圖片希望是(batch, 256, 256, 3) batch也可以是1 只是希望是四維的 且是tensor\\n        image = image.to(torch.float32).to(device)\\n        image = image_loader_to_tensor(image)\\n        checkpoint = torch.load(weight_path, map_location = torch.device(device))\\n        model.load_state_dict(checkpoint[\\'model_state_dict\\'])\\n        n = image.shape[0]\\n        pure_noise = torch.randn((image.shape[0], image.shape[-1], image.shape[-1]))\\n        xt = [pure_noise]\\n        with torch.inference_mode():\\n            for i, j in zip(reversed(range(1, self.timesteps + 1)), range(0, self.timesteps)):\\n                noise = torch.randn((image.shape[0], image.shape[-1], image.shape[-1]))\\n                t = (torch.ones(n) * i).to(image.device)\\n                t_next = (torch.ones(n) * j).to(image.device)\\n                depth_tmp = xt[-1]\\n                ans = model(image, depth_tmp, t, sampling = True)\\n                betas_t = get_index_from_list(self.betas, t, depth_tmp.shape)\\n                sqrt_alphas = get_index_from_list(self.alphas, t, depth_tmp.shape)\\n\\n                sqrt_next_alphas_cumprod = get_index_from_list(torch.sqrt(self.alphas_cumprod), t_next, depth_tmp.shape)\\n                one_minus_next_alphas_prod = get_index_from_list(1 - self.alphas_cumprod, t_next, depth_tmp.shape)\\n                one_minus_alphas_prod = get_index_from_list(1 - self.alphas_cumprod, t, depth_tmp.shape)\\n                xt.append((sqrt_alphas * (one_minus_next_alphas_prod) * depth_tmp + sqrt_next_alphas_cumprod * betas_t * ans) / (one_minus_alphas_prod) + betas_t * noise)\\n        return xt\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["'''\n","input_img = batch['img'].to(torch.float32).to(device)\n","input_img = image_loader_to_tensor(input_img)\n","target_depth = batch['depth'].to(torch.float32).to(device)\n","target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","pred_depth = model(input_img, target_depth, t)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"aPUF1fBnikTx","outputId":"eadc6140-2d95-4df3-9a49-adc33e0a9ace"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ninput_img = batch['img'].to(torch.float32).to(device)\\ninput_img = image_loader_to_tensor(input_img)\\ntarget_depth = batch['depth'].to(torch.float32).to(device)\\ntarget_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\\n\\npred_depth = model(input_img, target_depth, t)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["'''\n","data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle/dataset_1.pth'\n","data_check = torch.load(data_path, map_location = torch.device(device))\n","'''"],"metadata":{"id":"-TrVeE1FbmG2","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"5608ff89-7122-4111-885d-a2c0665f384c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle/dataset_1.pth'\\ndata_check = torch.load(data_path, map_location = torch.device(device))\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQHKn7pO1rWx"},"outputs":[],"source":["# BATCH_SIZE = 256\n","\n","NO_LARGE_EPOCHS = 5\n","save_frequency = 5\n","LR = 0.001\n","VERBOSE = False\n","clip_value = 1e-2\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/name_zip_shuffle'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip_shuffle'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip_shuffle'\n","batch_size = 32\n","train_val_rate = 0.99\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UCqxssL2aDl"},"outputs":[],"source":["def compute_depth_mean(path):\n","  file_list = sorted(os.listdir(path))\n","  total_sum = 0\n","\n","  total_nonzero = 0\n","  # count = 0\n","  for name in file_list:\n","    file_path = path + '/' + name\n","    check = torch.load(file_path)\n","\n","    target = torch.tensor(check['depth_list']).to(torch.float64)\n","    print(target.dtype)\n","    total_sum += torch.sum(target)\n","\n","    total_nonzero += torch.nonzero(target).size(0)\n","    # count += 1\n","    # if count == 2:\n","    #   break\n","  total_mean = total_sum / total_nonzero\n","  return total_sum, total_nonzero, total_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDuwfADL2brG"},"outputs":[],"source":["def create_dataset_large_epoch(random_list, now, data_path, name_path): #data_per_epoch / amount_from_file要是整數\n","\n","  data_list = sorted(os.listdir(data_path))\n","  name_list = sorted(os.listdir(name_path))\n","\n","  # file_idx = data_per_epoch // amount_from_file\n","  output_depth_path = []\n","  output_image_path = []\n","  output_depth = []\n","  output_image = []\n","\n","  now_number = random_list[now]\n","  data_path = data_path + '/' + data_list[now_number]\n","  name_path = name_path + '/' + name_list[now_number]\n","  data_checkpoint = torch.load(data_path)\n","  name_checkpoint = torch.load(name_path)\n","  output_image_path = name_checkpoint['image_name']\n","  output_depth_path = name_checkpoint['depth_name']\n","  output_depth = data_checkpoint['depth_list']\n","  output_image = data_checkpoint['image_list']\n","  return output_image_path, output_depth_path, output_depth, output_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqygFNRK2e-K"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img, depth):\n","        self.img = img\n","        self.depth = depth\n","\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        sample = {'img': self.img[idx], 'depth': self.depth[idx]}\n","        return sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1gcbJdJ2gfx"},"outputs":[],"source":["def image_loader_to_tensor(tensor):\n","  tensor = tensor.to(torch.float32)\n","  tensor = tensor / 255.0\n","  tensor = tensor * 2.0\n","  tensor = tensor - 1.0\n","  tensor = tensor.permute(0, 3, 1, 2)\n","  return tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63y4Ey5I2h_M"},"outputs":[],"source":["def depth_loader_to_tensor(tensor, DEPTH_MEAN, DEPTH_STD):\n","\n","  nonzero_mask = tensor != 0\n","  zero_mask = tensor == 0\n","  # mean = tensor[nonzero_mask].mean()\n","  # std = tensor[nonzero_mask].std()\n","  result = (tensor[nonzero_mask] - DEPTH_MEAN) / DEPTH_STD\n","  tensor[nonzero_mask] = result\n","  tensor[zero_mask] = -1\n","\n","  return tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7qIRIZK2jW-"},"outputs":[],"source":["def tensor_to_depth(tensor, DEPTH_MEAN, DEPTH_STD):\n","    minus_mask = tensor == -1\n","    non_minus = tensor != -1\n","    result = tensor[non_minus] * DEPTH_STD + DEPTH_MEAN\n","    tensor[non_minus] = result\n","    tensor[minus_mask] = 0\n","    return tensor"]},{"cell_type":"code","source":["'''\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","'''"],"metadata":{"id":"wtHzNRgUfbGu","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"2e78d680-3113-425f-e976-1bb0ab11e497"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ncheck_length = len(sorted(os.listdir(data_path)))\\nrandom_list = []\\nfor idx in range(check_length):\\n    random_list.append(idx)\\nnow = 15\\noutput_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["'''\n","print(type(output_image))\n","print(len(output_image))\n","print(type(output_image[0]))\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"v6eMcy59fvog","outputId":"091ddd5a-1e60-4e0d-f0c9-a119df253c67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nprint(type(output_image))\\nprint(len(output_image))\\nprint(type(output_image[0]))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["'''\n","custom_dataset = CustomDataset(output_image, output_depth)\n","print(type(custom_dataset[0]['img']))\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","print(custom_dataset[0]['img'].shape)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"e3DN1j95frh-","outputId":"3e1d95da-0e05-4045-b6af-4b70fd49da01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ncustom_dataset = CustomDataset(output_image, output_depth)\\nprint(type(custom_dataset[0]['img']))\\ntrain_size = int(train_val_rate * len(custom_dataset))\\nval_size = len(custom_dataset) - train_size\\nprint(custom_dataset[0]['img'].shape)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["'''\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","'''"],"metadata":{"id":"obp41rIwfTpj","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"1957d470-ad23-4eaf-e9d3-84e2d9e3936c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nvalidloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["'''\n","for batch in trainloader:\n","    print(type(batch))\n","    print(batch.keys())\n","    print(type(batch['img']))\n","    print(batch['img'].shape)\n","    print(batch['depth'].shape)\n","    print(image_loader_to_tensor(batch['img']).shape)\n","    print(depth_loader_to_tensor(batch['depth'], DEPTH_MEAN, DEPTH_STD).shape)\n","    # target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","    break\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"LT-CjIEWhR4X","outputId":"ade01e64-d712-4ea9-ddcc-f3175e015094"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor batch in trainloader:\\n    print(type(batch))\\n    print(batch.keys())\\n    print(type(batch['img']))\\n    print(batch['img'].shape)\\n    print(batch['depth'].shape)\\n    print(image_loader_to_tensor(batch['img']).shape)\\n    print(depth_loader_to_tensor(batch['depth'], DEPTH_MEAN, DEPTH_STD).shape)\\n    # target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\\n    break\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["'''\n","target_depth = batch['depth'].to(torch.float32).to(device)\n","print(target_depth.shape)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nRBxYSz08PRO","outputId":"86420f7a-8a99-4fdb-aff3-7c09c361d19a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntarget_depth = batch['depth'].to(torch.float32).to(device)\\nprint(target_depth.shape)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["'''\n","for i, j in zip(range(1, 5), range(2, 6)):\n","    print(i, j)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MOFsOsnZEBKC","outputId":"b2c26427-69a4-470b-d5be-bf5eb916a9f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfor i, j in zip(range(1, 5), range(2, 6)):\\n    print(i, j)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIIfccqN2j_H"},"outputs":[],"source":["class Model_optimize:\n","    def __init__(self, model, weight_path, loaded = False):\n","        if loaded == False:\n","            checkpoint = torch.load(weight_path, map_location=torch.device(device))\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","        self.model = model\n","        count = 0\n","        for name, param in model.named_parameters():\n","            count += 1\n","        self.amount_of_param = count\n","    def find_a_parameter(self, target):\n","        list = []\n","\n","        for name, param in model.named_parameters():\n","            par_shape = len(param.shape)\n","            if par_shape == 1:\n","                if param[0] == target:\n","                    list.append(name)\n","            elif par_shape == 2:\n","                if param[0][0] == target:\n","                    list.append(name)\n","            elif par_shape == 3:\n","                if param[0][0][0] == target:\n","                    list.append(name)\n","            elif par_shape == 4:\n","                if param[0][0][0][0] == target:\n","                    list.append(name)\n","        return list\n","    def analyst_param(self):\n","        for name, param in model.named_parameters():\n","            print('{} | {}'.format(name, param))\n","            print('--------------------------')\n","    def list_name(self):\n","        for name, param in model.named_parameters():\n","            print(name)\n","            print('--------------------------')\n","    def statics(self):\n","        for name, param in model.named_parameters():\n","            print('{} | max = {} | min = {} | mean = {}'.format(name, torch.max(param), torch.min(param), torch.mean(param)))\n","    def plot_name(self, name):\n","        for names, param in model.named_parameters():\n","            if names == name:\n","                weights_list = param.data.cpu().numpy().flatten()\n","                plt.figure(figsize=(7, 5))\n","                plt.title('Weight Distribution')\n","                plt.hist(weights_list, bins=50, alpha=0.7)\n","                plt.xlabel('Weight Value')\n","                plt.ylabel('Frequency')\n","                plt.show()\n","\n","                break\n","    def plot_idx(self, idx):\n","        count = 0\n","        for names, param in model.named_parameters():\n","            if count == idx:\n","                weights_list = param.data.cpu().numpy().flatten()\n","                plt.figure(figsize=(7, 5))\n","                plt.title(names)\n","                plt.hist(weights_list, bins=50, alpha=0.7)\n","                plt.xlabel('Weight Value')\n","                plt.ylabel('Frequency')\n","                plt.show()\n","\n","                break\n","            count += 1\n","    def plot_whole_sep(self):\n","        for idx in range(self.amount_of_param):\n","            self.plot_idx(idx)\n","\n","    def plot_whole(self):\n","        weights_list = []\n","        for name, param in model.named_parameters():\n","            weights_list.append(param.data.cpu().numpy().flatten())\n","        plt.figure(figsize=(7, 5))\n","        plt.title('whole_plot')\n","        plt.hist(weights_list, bins=50, alpha=0.7)\n","        plt.xlabel('Weight Value')\n","        plt.ylabel('Frequency')\n","        plt.show()"]},{"cell_type":"code","source":["'''\n","weight_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/drawing_weight/weight_2_26.pth'\n","# checkpoint = torch.load(weight_path, map_location = torch.device(device))\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","'''"],"metadata":{"id":"0Mj-GK17BTPw","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"65096b5f-f394-405c-b6c3-49baf726a00e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nweight_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/drawing_weight/weight_2_26.pth'\\n# checkpoint = torch.load(weight_path, map_location = torch.device(device))\\n# model.load_state_dict(checkpoint['model_state_dict'])\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["'''\n","#reference mode\n","\n","beta_schedule = config['diffusion']['beta_schedule']\n","start_schedule = config['diffusion']['beta_start']\n","end_schedule = config['diffusion']['beta_end']\n","timesteps = config['diffusion']['num_diffusion_timesteps']\n","diff = DiffusionModel(beta_schedule, start_schedule, end_schedule, timesteps)\n","model = Model(config)\n","model = model.to(device)\n","ans1, ans2 = diff.backward(model, img, weight_path, 1) # img should have 4 dimension\n","'''"],"metadata":{"id":"I30lO59sBzFV","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"4fc45cd2-1a63-47fe-c693-45a4ed0b8f25"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n#reference mode\\n\\nbeta_schedule = config['diffusion']['beta_schedule']\\nstart_schedule = config['diffusion']['beta_start']\\nend_schedule = config['diffusion']['beta_end']\\ntimesteps = config['diffusion']['num_diffusion_timesteps']\\ndiff = DiffusionModel(beta_schedule, start_schedule, end_schedule, timesteps)\\nmodel = Model(config)\\nmodel = model.to(device)\\nans1, ans2 = diff.backward(model, img, weight_path, 1) # img should have 4 dimension\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["'''\n","# convert back into depth map\n","with torch.inference_mode():\n","    final_ans = tensor_to_depth(depth2, DEPTH_MEAN, DEPTH_STD)\n","    final_ans = torch.squeeze(final_ans, dim = 0).to('cpu').numpy()\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"bfpiTt3vL0UQ","outputId":"bd577e47-7ff8-4812-e8aa-f6fd434cb8b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# convert back into depth map\\nwith torch.inference_mode():\\n    final_ans = tensor_to_depth(depth2, DEPTH_MEAN, DEPTH_STD)\\n    final_ans = torch.squeeze(final_ans, dim = 0).to('cpu').numpy()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["\n","'''\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","load_path = ''\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","for batch in validloader:\n","    input_img = batch['img'].to(torch.float32).to(device)\n","    input_img = image_loader_to_tensor(input_img)\n","    break\n","\n","'''"],"metadata":{"id":"YzWPtaK2Cs2W","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"3a7f669a-887c-4460-cad3-b25c6989d420"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\\nname_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\\nload_path = ''\\ncheck_length = len(sorted(os.listdir(data_path)))\\nrandom_list = []\\nfor idx in range(check_length):\\n    random_list.append(idx)\\nnow = 15\\noutput_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\\ncustom_dataset = CustomDataset(output_image, output_depth)\\ntrain_size = int(train_val_rate * len(custom_dataset))\\nval_size = len(custom_dataset) - train_size\\ntrain_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\\ntrainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nvalidloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\\nfor batch in validloader:\\n    input_img = batch['img'].to(torch.float32).to(device)\\n    input_img = image_loader_to_tensor(input_img)\\n    break\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# # input_img = input_img[0]\n","# print(input_img.shape)\n","# img = input_img[0]\n","# img = torch.unsqueeze(img, dim = 0)\n","# print(img.shape)\n","# print(img.dtype)"],"metadata":{"id":"15AJEnZTEldS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","beta = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = 300)\n","betas = torch.tensor(beta)\n","alphas = 1 - betas\n","seq = range(0, 300, 5)\n","seq_next = [-1] + list(seq[:-1])\n","n = 1\n","image = img\n","alphas_cumprod = torch.cumprod(alphas, axis=0)\n","for i, j in zip(reversed(seq), reversed(seq_next)):\n","    t = (torch.ones(n) * i).to(image.device)\n","    next_t = (torch.ones(n) * j).to(image.device)\n","    print(t)\n","    at = alphas_cumprod.gather(-1, t.to(torch.int64))\n","\n","    at_next = alphas_cumprod.gather(-1, next_t.to(torch.int64))\n","\n","    xt = xs[-1].to(device) # x_t\n","    x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\n","\n","    x0_preds.append(x0_t.to('cpu'))\n","    et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\n","    c1 = (\n","        eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\n","                                                                                    # can make the sampling process of x_{t - 1} become identical\n","                                                                                    # as ddpm\n","    )\n","    c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","    xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\n","    xs.append(xt_next.to('cpu'))\n","    '''"],"metadata":{"id":"4JsDOINaHKJH","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"54c4fde7-a6ed-4ed1-8138-771061640204"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nbeta = get_beta_schedule(beta_schedule, beta_start = start_schedule, beta_end = end_schedule, num_diffusion_timesteps = 300)\\nbetas = torch.tensor(beta)\\nalphas = 1 - betas\\nseq = range(0, 300, 5)\\nseq_next = [-1] + list(seq[:-1])\\nn = 1\\nimage = img\\nalphas_cumprod = torch.cumprod(alphas, axis=0)\\nfor i, j in zip(reversed(seq), reversed(seq_next)):\\n    t = (torch.ones(n) * i).to(image.device)\\n    next_t = (torch.ones(n) * j).to(image.device)\\n    print(t)\\n    at = alphas_cumprod.gather(-1, t.to(torch.int64))\\n\\n    at_next = alphas_cumprod.gather(-1, next_t.to(torch.int64))\\n\\n    xt = xs[-1].to(device) # x_t\\n    x0_t = model(image, xt, t, sampling = True) # episolon t (predicted)\\n\\n    x0_preds.append(x0_t.to('cpu'))\\n    et = (-1 * x0_t * (at.sqrt()) - xt) / (at.sqrt())\\n    c1 = (\\n        eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt() # c1是var in the distribution, which is at ddim page 5\\n                                                                                    # can make the sampling process of x_{t - 1} become identical\\n                                                                                    # as ddpm\\n    )\\n    c2 = ((1 - at_next) - c1 ** 2).sqrt()\\n    xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x0_t) + c2 * et # 這個xt_next也是x_(t-1) 取法是ddim 裡面定義的q(x_(t-1)|x_t, x_0)\\n    xs.append(xt_next.to('cpu'))\\n    \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":53}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SOLV8Y8_2th4","outputId":"0f21c623-6cb7-4d92-904d-0cab774149fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["image before fpn : torch.Size([32, 3, 256, 256])\n","-----------here is fpn----------vvv\n","before FPN image.shape = torch.Size([32, 3, 256, 256])\n","FPN bn0 shape = torch.Size([32, 3, 256, 256])\n","self.ConvList[0](h) shape = torch.Size([32, 64, 128, 128])\n","self.pool(self.Lrelu(h)) shape = torch.Size([32, 64, 64, 64])\n","temp for idx = 0 has shape : torch.Size([32, 64, 64, 64])\n","self.Lrelu(self.tuneChannels[0](h)) has shape : torch.Size([32, 256, 64, 64])\n","temp for idx = 1 has shape : torch.Size([32, 128, 32, 32])\n","self.Lrelu(self.tuneChannels[1](h)) has shape : torch.Size([32, 256, 32, 32])\n","temp for idx = 2 has shape : torch.Size([32, 256, 16, 16])\n","self.Lrelu(self.tuneChannels[2](h)) has shape : torch.Size([32, 256, 16, 16])\n","temp for idx = 3 has shape : torch.Size([32, 512, 8, 8])\n","self.Lrelu(self.tuneChannels[3](h)) has shape : torch.Size([32, 256, 8, 8])\n","----------------\n","FPN_LIST[3] has shape : torch.Size([32, 256, 8, 8])\n","for idx = 3 hold.shape : torch.Size([32, 256, 16, 16])\n","----------------\n","idx = 2, hold before self.norm_seq[count](hold) = torch.Size([32, 256, 16, 16])\n","idx = 2, hold after self.norm_seq[count](hold) = torch.Size([32, 256, 16, 16])\n","idx = 2, hold after hold + self.norm_seq[count](FPN_list[idx]) = torch.Size([32, 256, 16, 16])\n","idx = 2, hold after self.Upsampple[idx - 1](hold) = torch.Size([32, 256, 32, 32])\n","idx = 1, hold before self.norm_seq[count](hold) = torch.Size([32, 256, 32, 32])\n","idx = 1, hold after self.norm_seq[count](hold) = torch.Size([32, 256, 32, 32])\n","idx = 1, hold after hold + self.norm_seq[count](FPN_list[idx]) = torch.Size([32, 256, 32, 32])\n","idx = 1, hold after self.Upsampple[idx - 1](hold) = torch.Size([32, 256, 64, 64])\n","for idx = 0, hold before self.norm_seq[count] : torch.Size([32, 256, 64, 64])\n","for idx = 0, hold after self.norm_seq[count] : torch.Size([32, 256, 64, 64])\n","----------------\n","FPN_list[0].shape = torch.Size([32, 256, 64, 64])\n","self.norm_seq[5](FPN_list[0]).shape = torch.Size([32, 256, 64, 64])\n","for idx = 0, hold after self.convOut(hold + self.norm_seq[count](FPN_list[idx])) : torch.Size([32, 256, 64, 64])\n","----------------\n","-----------here is fpn----------^^^\n","img_enc.shape = torch.Size([32, 256, 64, 64])\n","depth before depth_encode : torch.Size([32, 256, 256])\n","-----------here is depth_encode----------vvv\n","depth.unsqueeze(1).shape = torch.Size([32, 1, 256, 256])\n","self.phase1_model[0](h).shape = torch.Size([32, 4, 128, 128])\n","self.phase1_model[1](h).shape = torch.Size([32, 16, 64, 64])\n","self.phase2_model[0](h).shape = torch.Size([32, 64, 64, 64])\n","self.phase2_model[1](h).shape = torch.Size([32, 256, 64, 64])\n","-----------here is depth_encode----------^^^\n","depth after depth_encode : torch.Size([32, 256, 64, 64])\n","noisy_map shape : torch.Size([32, 256, 64, 64])\n","backbone_input.shape = torch.Size([32, 512, 64, 64])\n","-----------here is main----------vvv\n","self.conv_in(backbone_input).shape = torch.Size([32, 128, 64, 64])\n","self.down[0].block[0](hs[-1], temb).shape = torch.Size([32, 128, 64, 64])\n","self.down[0].block[1](hs[-1], temb).shape = torch.Size([32, 128, 64, 64])\n","self.down[0].downsample(hs[-1]).shape\n","self.down[1].block[0](hs[-1], temb).shape = torch.Size([32, 256, 32, 32])\n","self.down[1].block[1](hs[-1], temb).shape = torch.Size([32, 256, 32, 32])\n","self.mid.block_1(h, temb).shape = torch.Size([32, 256, 32, 32])\n","self.mid.attn_1(h).shape = torch.Size([32, 256, 32, 32])\n","self.mid.block_2(h, temb).shape = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 0, h before cat = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 0, hs.pop() before cat = torch.Size([32, 256, 32, 32])\n","h_cat.shape : torch.Size([32, 512, 32, 32])\n","self.up[1].block[0](h_cat, temb).shape = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 1, h before cat = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 1, hs.pop() before cat = torch.Size([32, 256, 32, 32])\n","h_cat.shape : torch.Size([32, 512, 32, 32])\n","self.up[1].block[1](h_cat, temb).shape = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 2, h before cat = torch.Size([32, 256, 32, 32])\n","i_level = 1, i_block = 2, hs.pop() before cat = torch.Size([32, 128, 32, 32])\n","h_cat.shape : torch.Size([32, 384, 32, 32])\n","self.up[1].block[2](h_cat, temb).shape = torch.Size([32, 256, 32, 32])\n","self.up[1].upsample(h).shape = torch.Size([32, 256, 64, 64])\n","i_level = 0, i_block = 0, h before cat = torch.Size([32, 256, 64, 64])\n","i_level = 0, i_block = 0, hs.pop() before cat = torch.Size([32, 128, 64, 64])\n","h_cat.shape : torch.Size([32, 384, 64, 64])\n","self.up[0].block[0](h_cat, temb).shape = torch.Size([32, 128, 64, 64])\n","i_level = 0, i_block = 1, h before cat = torch.Size([32, 128, 64, 64])\n","i_level = 0, i_block = 1, hs.pop() before cat = torch.Size([32, 128, 64, 64])\n","h_cat.shape : torch.Size([32, 256, 64, 64])\n","self.up[0].block[1](h_cat, temb).shape = torch.Size([32, 128, 64, 64])\n","i_level = 0, i_block = 2, h before cat = torch.Size([32, 128, 64, 64])\n","i_level = 0, i_block = 2, hs.pop() before cat = torch.Size([32, 128, 64, 64])\n","h_cat.shape : torch.Size([32, 256, 64, 64])\n","self.up[0].block[2](h_cat, temb).shape = torch.Size([32, 128, 64, 64])\n","-----------here is main----------^^^\n","self.norm_out(h).shape = torch.Size([32, 128, 64, 64])\n","-----------here is depth_decode----------vvv\n","pred = self.decode[0](pred).shape = torch.Size([32, 32, 128, 128])\n","pred = self.decode[1](pred).shape = torch.Size([32, 8, 256, 256])\n","self.final_conv(pred).shape = torch.Size([32, 1, 256, 256])\n","pred.squeeze(1).shape = torch.Size([32, 256, 256])\n","-----------here is depth_decode----------^^^\n","self.depth_decode(h).shape = torch.Size([32, 256, 256])\n"]},{"output_type":"error","ename":"SystemExit","evalue":"","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["\n","#debug epoch\n","\n","epoch = 0\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","# data_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/data_zip_shuffle'\n","# name_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Data/name_zip_shuffle'\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","for large_epoch in range(1, NO_LARGE_EPOCHS + 1):\n","\n","\n","\n","    for now in range(check_length): # 一個小epoch是一個checkpoint檔，紀錄一次\n","\n","        epoch += 1\n","\n","\n","\n","        start_time = time.time()\n","        mean_epoch_loss = []\n","        mean_epoch_loss_val = []\n","        epoch_gradient = {}\n","        for batch in trainloader:\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","            pred_depth = model(input_img, target_depth, t)\n","            # --------------------exit--------\n","            sys.exit()\n","            # --------------------exit--------\n","\n","            optimizer.zero_grad()\n","            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            # params = list(model.parameters())\n","            # weight_tensor = params[490]\n","            # chains = torch.autograd.grad(loss, weight_tensor, retain_graph=True)\n","            # for chain in chains:\n","            #     print(chain)\n","            mean_epoch_loss.append(loss.item())\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","            # for param in model.parameters():\n","            #     if param.grad is not None:\n","            #         param.grad.data.clamp_(-clip_value, clip_value)\n","            optimizer.step()\n","            #---gradient---vvv\n","            for name, param in model.named_parameters():\n","                if param.grad == None:\n","                    epoch_gradient[name + 'zero'] = 1\n","                elif name not in epoch_gradient:\n","                    epoch_gradient[name] = param.grad.clone()\n","                else:\n","                    epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","        with torch.inference_mode():\n","            for batch in validloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                input_img = batch['img'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss_val.append(val_loss.item())\n","\n","        if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","            checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'now' : now,\n","                'random_list' : random_list,\n","                'gradients' : epoch_gradient\n","            }\n","\n","            torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/take_out_normal_weight6'\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/take_out_normal_weight'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(exe_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/take_out_normal_loss6'\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/drawing_loss'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n","\n"]},{"cell_type":"code","source":["\n","#debug epoch continue\n","large_epoch = 3\n","epoch = 50\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/data_zip'\n","name_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Data/name_zip'\n","load_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/take_out_normal_weight5/weight_{}_{}.pth'.format(large_epoch, epoch)\n","# load_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/drawing_weight2/weight_{}_{}.pth'.format(large_epoch, epoch)\n","check_length = len(sorted(os.listdir(data_path)))\n","random_list = []\n","for idx in range(check_length):\n","    random_list.append(idx)\n","now = 15\n","output_image_path, output_depth_path, output_depth, output_image = create_dataset_large_epoch(random_list, now, data_path, name_path)\n","custom_dataset = CustomDataset(output_image, output_depth)\n","train_size = int(train_val_rate * len(custom_dataset))\n","val_size = len(custom_dataset) - train_size\n","train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n","trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n","\n","checkpoint = torch.load(load_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","\n","\n","for large_epoch in range(large_epoch, NO_LARGE_EPOCHS + 1):\n","\n","\n","\n","    while True: # 一個小epoch是一個checkpoint檔，紀錄一次\n","        current_lr = optimizer.param_groups[0]['lr']\n","        print('lr : {}'.format(current_lr))\n","        sym = 0\n","        epoch += 1\n","\n","        start_time = time.time()\n","        mean_epoch_loss = []\n","        mean_epoch_loss_val = []\n","        epoch_gradient = {}\n","\n","        if sym == 1:\n","            sym = 1\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = 0.0001\n","\n","\n","        for batch in trainloader:\n","            # current_lr = optimizer.param_groups[0]['lr']\n","            # print('lr : {}'.format(current_lr))\n","            t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","\n","            input_img = batch['img'].to(torch.float32).to(device)\n","            input_img = image_loader_to_tensor(input_img)\n","            target_depth = batch['depth'].to(torch.float32).to(device)\n","            target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","\n","            pred_depth = model(input_img, target_depth, t)\n","\n","            optimizer.zero_grad()\n","            loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","            mean_epoch_loss.append(loss.item())\n","            loss.backward()\n","            for param in model.parameters():\n","                if param.grad is not None:\n","                    param.grad.data.clamp_(-clip_value, clip_value)\n","            optimizer.step()\n","\n","\n","\n","\n","            #---gradient---vvv\n","            for name, param in model.named_parameters():\n","                if param.grad == None:\n","                    epoch_gradient[name + 'zero'] = 1\n","                elif name not in epoch_gradient:\n","                    epoch_gradient[name] = param.grad.clone()\n","                else:\n","                    epoch_gradient[name] += param.grad\n","            #---gradient---^^^\n","        with torch.inference_mode():\n","            for batch in validloader:\n","                t = torch.randint(0, config['diffusion']['num_diffusion_timesteps'], (batch_size,)).long().to(device)\n","                input_img = batch['img'].to(torch.float32).to(device)\n","\n","                input_img = image_loader_to_tensor(input_img)\n","                target_depth = batch['depth'].to(torch.float32).to(device)\n","                target_depth = depth_loader_to_tensor(target_depth, DEPTH_MEAN, DEPTH_STD)\n","                pred_depth = model(input_img, target_depth, t)\n","\n","                val_loss = torch.nn.functional.mse_loss(target_depth, pred_depth)\n","                mean_epoch_loss_val.append(val_loss.item())\n","\n","        if epoch % save_frequency == 0 or epoch == check_length * NO_LARGE_EPOCHS:\n","            checkpoint = {\n","                'large_epoch' : large_epoch,\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(), # model.state_dict()是存下param的的值和形狀\n","                'optimizer_state_dict': optimizer.state_dict(), # optimizer.state_dict()則是存下優化器的param如momentum等等 不包含當下梯度\n","                'valid_loss' : np.mean(mean_epoch_loss_val),\n","                'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","                'now' : now,\n","                'random_list' : random_list,\n","                'gradients' : epoch_gradient\n","            }\n","\n","            torch.save(checkpoint, 'weight_{}_{}.pth'.format(large_epoch, epoch))\n","            source_path = 'weight_{}_{}.pth'.format(large_epoch, epoch)\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/drawing_weight'\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_weight'\n","            # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/drawing_weight2'\n","            destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/take_out_normal_weight5'\n","\n","\n","            # save them to the google drive\n","            shutil.copy(source_path, destination_path)\n","\n","        #---計算時間---vvv\n","        end_time = time.time()\n","        exe_time = end_time - start_time\n","        hours, remainder = divmod(exe_time, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        #---計算時間---^^^\n","\n","        #-----以下是存loss的---vvv\n","        checkpoint = {\n","        'large_epoch' : large_epoch,\n","        'epoch': epoch,\n","        'valid_loss' : np.mean(mean_epoch_loss_val),\n","        'loss' : np.mean(mean_epoch_loss), # 記得不能存tensor\n","        'time' : exe_time\n","        }\n","\n","        torch.save(checkpoint, 'loss_{}_{}.pth'.format(large_epoch, epoch))\n","        source_path = 'loss_{}_{}.pth'.format(large_epoch, epoch)\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/model1_loss'\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/drawing_loss'\n","        # destination_path = '/content/drive/MyDrive/Colab Notebooks/共用區/Simple_DE/Checkpoint/drawing_loss2'\n","        destination_path = '/content/drive/MyDrive/Colab Notebooks/Simple_DE/Checkpoint/take_out_normal_loss5'\n","\n","\n","        # save them to the google drive\n","        shutil.copy(source_path, destination_path)\n","        #-----以下是存loss的---^^^\n","\n","        print('---')\n","        print(f\"Large Epoch: {large_epoch}, Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n","        print(\"time = {}:{}:{}\".format(int(hours), int(minutes), int(seconds)))\n","\n","        if epoch % 18 == 0:\n","            break\n"],"metadata":{"id":"T1gBD05bwwrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NGxbBkmfvby"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"code","source":[],"metadata":{"id":"ir__aJEQTxb2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}